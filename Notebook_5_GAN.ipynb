{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook 5: GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_Zum99rg91H",
        "colab_type": "text"
      },
      "source": [
        "# Why GAN for stock market prediction?\n",
        "\n",
        "Generative Adversarial Networks (GAN) have been recently used mainly in creating realistic images, paintings, and video clips. There arenâ€™t many applications of GANs being used for predicting time-series data as in our case. The main idea, however, should be same â€” we want to predict future stock movements. In the future, the pattern and behavior of Amazonâ€™s stock should be more or less the same (unless it starts operating in a totally different way, or the economy drastically changes). Hence, we want to â€˜generateâ€™ data for the future that will have similar (not absolutely the same, of course) distribution as the one we already have â€” the historical trading data. So, in theory, it should work.\n",
        "\n",
        "**HOW DOES GAN WORK?**\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1094/1*hN0QKvuY4n07jxQCwRSmpg.jpeg)\n",
        "\n",
        "THIS BOOK IS A RESEARCH IDEA..CODE IS NOT COMPLETE...A GAN network consists of two models â€” a Generator (G) and Discriminator (D). The steps in training a GAN are:\n",
        "The Generator is, using random data (noise denoted z), trying to â€˜generateâ€™ data indistinguishable of, or extremely close to, the real data. Its purpose is to learn the distribution of the real data.\n",
        "\n",
        "Randomly, real or generated data is fitted into the Discriminator, which acts as a classifier and tries to understand whether the data is coming from the Generator or is the real data. D estimates the (distributions) probabilities of the incoming sample to the real dataset. (more info on comparing two distributions in section 3.2. below).\n",
        "\n",
        "\n",
        "Then, the losses from G and D are combined and propagated back through the generator. Ergo, the generatorâ€™s loss depends on both the generator and the discriminator. This is the step that helps the Generator learn about the real data distribution. If the generator doesnâ€™t do a good job at generating a realistic data (having the same distribution), the Discriminatorâ€™s work will be very easy to distinguish generated from real data sets. Hence, the Discriminatorâ€™s loss will be very small. Small discriminator loss will result in bigger generator loss (see the equation below for L(D,G)). This makes creating the discriminator a bit tricky, because too good of a discriminator will always result in a huge generator loss, making the generator unable to learn.\n",
        "The process goes on until the Discriminator can no longer distinguish generated from real data.\n",
        "\n",
        "\n",
        "When combined together, D and G as sort of playing a minmax game (the Generator is trying to fool the Discriminator making it increase the probability for on fake examples, i.e. minimize ð”¼zâˆ¼pz(z)[log(1âˆ’D(G(z)))]. The Discriminator wants to separate the data coming from the Generator, D(G(z)), by maximizing ð”¼xâˆ¼pr(x)[logD(x)]. Having separated loss functions, however, it is not clear how both can converge together (that is why we use some advancements over the plain GANs, such as Wasserstein GAN). Overall, the combined loss function looks like:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1094/1*TqzF5d6xFvo6IJMOdBBtnA.png)\n",
        "\n",
        "**IN OUR CASE WE WILL BE USING THE TIME SERIES GENERATED DATA IN  NOTEBOOK2 AS THE DATA AND A CNN AS THE DISCRIMINATOR**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGrrt16a7kR2",
        "colab_type": "code",
        "outputId": "cc8cc900-c9cb-49ee-9174-197fc760d13e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "#set up the google colab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SACvOFk574rz",
        "colab_type": "code",
        "outputId": "4bee18b7-8e45-49d2-dbb8-983578658cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "# drive.mount('/content/drive')\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzSA6-II8nEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "googlepath = \"/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/\"\n",
        "\n",
        "# Setting the Training Amount\n",
        "TRAINING_AMOUNT = 50000 # low to test for now\n",
        "SAVE_STEPS_AMOUNT = 10000 # testing for now\n",
        "PCT_CHANGE_AMOUNT = 5 # just want to see up down trends\n",
        "HISTORICAL_DAYS_AMOUNT = 20\n",
        "DAYS_AHEAD = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_HY8l1y8WvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting the Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1lkBfhEwc2c",
        "colab_type": "text"
      },
      "source": [
        "**ABOVE STEPS JUST KEEP THE DRIVE ON FOR GOOGLE COLAB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tCCJqT_9NVW",
        "colab_type": "code",
        "outputId": "ca60945e-591b-4f8b-e643-75d2bb212144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "'''\n",
        "Downloads stock data from alphavantage\n",
        "'''\n",
        "import pandas as pd \n",
        "import os\n",
        "import time\n",
        "import urllib\n",
        "import json\n",
        "import csv\n",
        "import requests\n",
        "import io\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "ALPHA_VANTAGE_KEY = \"\" //get your own key\n",
        "\n",
        "'''\n",
        "Note should have companylist.csv in the directory with this file.\n",
        "'''\n",
        "\n",
        "'''\n",
        "Saves data to a file\n",
        "'''\n",
        "def save(googlepath, stock_csv, output_dir, filename):\n",
        "    try:\n",
        "        #the output dir may not exist\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "    except Exception as ex:\n",
        "        print('Could not create output dir')\n",
        "        print(ex)\n",
        "        return\n",
        "    filepath = os.path.join(googlepath, output_dir, filename)\n",
        "    try:\n",
        "#         print(stock_csv)\n",
        "        df = stock_csv\n",
        "        df = df.sort_values(by='timestamp')  \n",
        "#         print(df)\n",
        "        df.to_csv(filepath, index=False)\n",
        "    except Exception as ex:\n",
        "        print('Could not open file {} to write data'.format(filepath))\n",
        "        print(ex)\n",
        "\n",
        "\n",
        "def try_download(symbol):\n",
        "    try:\n",
        "        # Keep call frequency below threshold \n",
        "        time.sleep(12)    \n",
        "        url = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={}&apikey={}&datatype=csv&outputsize=full'.format(symbol, ALPHA_VANTAGE_KEY)\n",
        "        c = pd.read_csv(url)\n",
        "        # getting rid of some columns won't look at for now\n",
        "        c = c.drop(['split_coefficient', 'dividend_amount', 'adjusted_close'], axis=1)\n",
        "        return c, True\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "        return None, None\n",
        "\n",
        "\n",
        "\n",
        "#Given a stock symbol (aka 'tsla') will download and save the data to the\n",
        "#output dir as a csv \n",
        "\n",
        "def download_symbol(symbol, output_dir, retry_count=4):\n",
        "\n",
        "    stock_csv, didPass = try_download(symbol)\n",
        "    if didPass:\n",
        "        save(googlepath, stock_csv, output_dir, '{}.csv'.format(symbol))\n",
        "    else:\n",
        "        print('Failed to download {}'.format(symbol))\n",
        "\n",
        "df = pd.read_csv(f\"{googlepath}companylist.csv\")\n",
        "# df = df.sort_values(by=['MarketCap'], ascending=False)\n",
        "# Top 30 Companies\n",
        "# df = df[:30]\n",
        "for symbol in df.Symbol:\n",
        "    my_file = Path(f\"{googlepath}stock_data/{symbol}.csv\")  # check if already downloaded\n",
        "#     print(my_file.exists())\n",
        "    if not my_file.exists():\n",
        "        print('Downloading {}'.format(symbol))\n",
        "        download_symbol(symbol, 'stock_data')\n",
        "    else:\n",
        "        print(f\"Already downloaded {symbol}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already downloaded AMZN\n",
            "Already downloaded AAPL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT047Jy4wmR0",
        "colab_type": "text"
      },
      "source": [
        "**GOT THE DATA FROM ALPHAADVANTAGE API FOR AMAZON**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIy6WWcg-M5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot confusion matrices\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh-UWwe-wyT4",
        "colab_type": "text"
      },
      "source": [
        "**THIS IS A CLASSIFICATION PROBLEM. I WILL TRY TO PREDICT WHETHER THE STOCK PRICES CAN GO UP OR DOWN. CONFUSION MATRIX HAS 2 LABELS UP AND DOWN. UP INDICATES STOCK PRICES ARE GOING UP AND DOWN INDICATES OTHERWISE. WILL USE GAN TO TRY AND PREDICT HOW MANY TIMES I GET STOCK PRICE MOVEMENTS RIGHT.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tetAWwGsiKuq",
        "colab_type": "text"
      },
      "source": [
        "**Confusion matrix just loads into the google colab folder for GAN. It helps to  know how good the predicted models from the GAN are**\n",
        "\n",
        "Ref: https://towardsdatascience.com/comprehensive-introduction-to-turing-learning-and-gans-part-1-81f6d02e644d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k070Rvz8Qirt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf #Define the GAN and data generator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "SEED = 42\n",
        "tf.set_random_seed(SEED)\n",
        "\n",
        "class GAN():\n",
        "\n",
        "    def sample_Z(self, batch_size, n):\n",
        "        return np.random.uniform(-1., 1., size=(batch_size, n))\n",
        "\n",
        "    def __init__(self, num_features, num_historical_days, generator_input_size=200, is_train=True):\n",
        "        def get_batch_norm_with_global_normalization_vars(size):\n",
        "            v = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            m = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            beta = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            gamma = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            return v, m, beta, gamma\n",
        "\n",
        "        self.X = tf.placeholder(tf.float32, shape=[None, num_historical_days, num_features])\n",
        "        X = tf.reshape(self.X, [-1, num_historical_days, 1, num_features])\n",
        "        self.Z = tf.placeholder(tf.float32, shape=[None, generator_input_size])\n",
        "\n",
        "        generator_output_size = num_features*num_historical_days\n",
        "        with tf.variable_scope(\"generator\"):\n",
        "            W1 = tf.Variable(tf.truncated_normal([generator_input_size, generator_output_size*10]))\n",
        "            b1 = tf.Variable(tf.truncated_normal([generator_output_size*10]))\n",
        "\n",
        "            h1 = tf.nn.sigmoid(tf.matmul(self.Z, W1) + b1)\n",
        "\n",
        "            # v1, m1, beta1, gamma1 = get_batch_norm_with_global_normalization_vars(generator_output_size*10)\n",
        "            # h1 = tf.nn.batch_norm_with_global_normalization(h1, v1, m1,\n",
        "            #         beta1, gamma1, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([generator_output_size*10, generator_output_size*5]))\n",
        "            b2 = tf.Variable(tf.truncated_normal([generator_output_size*5]))\n",
        "\n",
        "            h2 = tf.nn.sigmoid(tf.matmul(h1, W2) + b2)\n",
        "\n",
        "            # v2, m2, beta2, gamma2 = get_batch_norm_with_global_normalization_vars(generator_output_size*5)\n",
        "            # h2 = tf.nn.batch_norm_with_global_normalization(h2, v2, m2,\n",
        "            #         beta2, gamma2, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "\n",
        "\n",
        "            W3 = tf.Variable(tf.truncated_normal([generator_output_size*5, generator_output_size]))\n",
        "            b3 = tf.Variable(tf.truncated_normal([generator_output_size]))\n",
        "\n",
        "            g_log_prob = tf.matmul(h2, W3) + b3\n",
        "            g_log_prob = tf.reshape(g_log_prob, [-1, num_historical_days, 1, num_features])\n",
        "            self.gen_data = tf.reshape(g_log_prob, [-1, num_historical_days, num_features])\n",
        "            #g_log_prob = g_log_prob / tf.reshape(tf.reduce_max(g_log_prob, axis=1), [-1, 1, num_features, 1])\n",
        "            #g_prob = tf.nn.sigmoid(g_log_prob)\n",
        "\n",
        "            theta_G = [W1, b1, W2, b2, W3, b3]\n",
        "\n",
        "\n",
        "\n",
        "        with tf.variable_scope(\"discriminator\"):\n",
        "            #[filter_height, filter_width, in_channels, out_channels]\n",
        "            k1 = tf.Variable(tf.truncated_normal([3, 1, num_features, 32],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b1 = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
        "\n",
        "            v1, m1, beta1, gamma1 = get_batch_norm_with_global_normalization_vars(32)\n",
        "\n",
        "            k2 = tf.Variable(tf.truncated_normal([3, 1, 32, 64],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b2 = tf.Variable(tf.zeros([64], dtype=tf.float32))\n",
        "\n",
        "            v2, m2, beta2, gamma2 = get_batch_norm_with_global_normalization_vars(64)\n",
        "\n",
        "            k3 = tf.Variable(tf.truncated_normal([3, 1, 64, 128],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b3 = tf.Variable(tf.zeros([128], dtype=tf.float32))\n",
        "\n",
        "            v3, m3, beta3, gamma3 = get_batch_norm_with_global_normalization_vars(128)\n",
        "\n",
        "            W1 = tf.Variable(tf.truncated_normal([18*1*128, 128]))\n",
        "            b4 = tf.Variable(tf.truncated_normal([128]))\n",
        "\n",
        "            v4, m4, beta4, gamma4 = get_batch_norm_with_global_normalization_vars(128)\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([128, 1]))\n",
        "\n",
        "            theta_D = [k1, b1, k2, b2, k3, b3, W1, b4, W2]\n",
        "\n",
        "        def discriminator(X):\n",
        "            conv = tf.nn.conv2d(X,k1,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b1))\n",
        "            pool = relu\n",
        "            # pool = tf.nn.avg_pool(relu, ksize=[1, 2, 1, 1], strides=[1, 2, 1, 1], padding='SAME')\n",
        "            if is_train:\n",
        "                pool = tf.nn.dropout(pool, keep_prob = 0.8)\n",
        "            # pool = tf.nn.batch_norm_with_global_normalization(pool, v1, m1,\n",
        "            #         beta1, gamma1, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "            print(pool)\n",
        "\n",
        "            conv = tf.nn.conv2d(pool, k2,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b2))\n",
        "            pool = relu\n",
        "            #pool = tf.nn.avg_pool(relu, ksize=[1, 2, 1, 1], strides=[1, 2, 1, 1], padding='SAME')\n",
        "            if is_train:\n",
        "                pool = tf.nn.dropout(pool, keep_prob = 0.8)\n",
        "            # pool = tf.nn.batch_norm_with_global_normalization(pool, v2, m2,\n",
        "            #         beta2, gamma2, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "            print(pool)\n",
        "\n",
        "            conv = tf.nn.conv2d(pool, k3, strides=[1, 1, 1, 1], padding='VALID')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b3))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob=0.8)\n",
        "            # relu = tf.nn.batch_norm_with_global_normalization(relu, v3, m3,\n",
        "            #         beta3, gamma3, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "            print(relu)\n",
        "\n",
        "\n",
        "            flattened_convolution_size = int(relu.shape[1]) * int(relu.shape[2]) * int(relu.shape[3])\n",
        "            print(flattened_convolution_size)\n",
        "            flattened_convolution = features = tf.reshape(relu, [-1, flattened_convolution_size])\n",
        "\n",
        "            if is_train:\n",
        "                flattened_convolution =  tf.nn.dropout(flattened_convolution, keep_prob=0.8)\n",
        "\n",
        "            h1 = tf.nn.relu(tf.matmul(flattened_convolution, W1) + b4)\n",
        "\n",
        "            # h1 = tf.nn.batch_norm_with_global_normalization(h1, v4, m4,\n",
        "            #         beta4, gamma4, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "\n",
        "            D_logit = tf.matmul(h1, W2)\n",
        "            D_prob = tf.nn.sigmoid(D_logit)\n",
        "            return D_prob, D_logit, features\n",
        "\n",
        "        D_real, D_logit_real, self.features = discriminator(X)\n",
        "        D_fake, D_logit_fake, _ = discriminator(g_log_prob)\n",
        "\n",
        "\n",
        "        D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
        "        D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
        "        self.D_l2_loss = (0.0001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_D]) / len(theta_D))\n",
        "        self.D_loss = D_loss_real + D_loss_fake + self.D_l2_loss\n",
        "        self.G_l2_loss = (0.00001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_G]) / len(theta_G))\n",
        "        self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) + self.G_l2_loss\n",
        "\n",
        "\n",
        "        self.D_solver = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.D_loss, var_list=theta_D)\n",
        "        self.G_solver = tf.train.AdamOptimizer(learning_rate=0.000055).minimize(self.G_loss, var_list=theta_G)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag17WheKi7qV",
        "colab_type": "text"
      },
      "source": [
        "**Defined the discriminator and generator in above code.** \n",
        "**Code ref: https://www.tensorflow.org/beta/tutorials/generative/dcgan**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4-qKPvQQ4zD",
        "colab_type": "code",
        "outputId": "541c11d8-9aea-4a57-cf4e-4b7a22fac2ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#training the GAN\n",
        "import os\n",
        "import pandas as pd\n",
        "# from gan import GAN\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "random.seed(42)\n",
        "class TrainGan:\n",
        "\n",
        "    def __init__(self, num_historical_days, batch_size=128):\n",
        "        self.batch_size = batch_size\n",
        "        self.data = []\n",
        "#         files = [os.path.join('./stock_data', f) for f in os.listdir('./stock_data')]\n",
        "\n",
        "        # Google Drive Method\n",
        "        files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]\n",
        "#         print(files)\n",
        "      \n",
        "        for file in files:\n",
        "            print(file)\n",
        "            #Read in file -- note that parse_dates will be need later\n",
        "            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "            df = df[['open','high','low','close','volume']]\n",
        "            # #Create new index with missing days\n",
        "            # idx = pd.date_range(df.index[-1], df.index[0])\n",
        "            # #Reindex and fill the missing day with the value from the day before\n",
        "            # df = df.reindex(idx, method='bfill').sort_index(ascending=False)\n",
        "            #Normilize using a of size num_historical_days\n",
        "            df = ((df -\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "            #Drop the last 10 day that we don't have data for\n",
        "            df = df.dropna()\n",
        "            #Hold out the last year of trading for testing\n",
        "            #Padding to keep labels from bleeding\n",
        "            df = df[400:]\n",
        "            #This may not create good samples if num_historical_days is a\n",
        "            #mutliple of 7\n",
        "            for i in range(num_historical_days, len(df), num_historical_days):\n",
        "                self.data.append(df.values[i-num_historical_days:i])\n",
        "\n",
        "        self.gan = GAN(num_features=5, num_historical_days=num_historical_days,\n",
        "                        generator_input_size=200)\n",
        "\n",
        "    def random_batch(self, batch_size=128):\n",
        "        batch = []\n",
        "        while True:\n",
        "            batch.append(random.choice(self.data))\n",
        "            if (len(batch) == batch_size):\n",
        "                yield batch\n",
        "                batch = []\n",
        "\n",
        "    def train(self, print_steps=100, display_data=100, save_steps=SAVE_STEPS_AMOUNT):\n",
        "        if not os.path.exists(f'{googlepath}models'):\n",
        "            os.makedirs(f'{googlepath}models')\n",
        "        sess = tf.Session()\n",
        "        \n",
        "        G_loss = 0\n",
        "        D_loss = 0\n",
        "        G_l2_loss = 0\n",
        "        D_l2_loss = 0\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()\n",
        "        currentStep = \"0\"\n",
        "        \n",
        "        g_loss_array = []\n",
        "        d_loss_array = []\n",
        "        \n",
        "        if os.path.exists(f'{googlepath}models/checkpoint'):\n",
        "                with open(f'{googlepath}models/checkpoint', 'rb') as f:\n",
        "                    model_name = next(f).split('\"'.encode())[1]\n",
        "                filename = \"{}models/{}\".format(googlepath, model_name.decode())\n",
        "                currentStep = filename.split(\"-\")[1]\n",
        "                new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\n",
        "                new_saver.restore(sess, \"{}\".format(filename))\n",
        "\n",
        "        for i, X in enumerate(self.random_batch(self.batch_size)):\n",
        "\n",
        "            \n",
        "            \n",
        "            \n",
        "            if i % 1 == 0:\n",
        "                _, D_loss_curr, D_l2_loss_curr = sess.run([self.gan.D_solver, self.gan.D_loss, self.gan.D_l2_loss], feed_dict=\n",
        "                        {self.gan.X:X, self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\n",
        "                D_loss += D_loss_curr\n",
        "                D_l2_loss += D_l2_loss_curr\n",
        "            if i % 1 == 0:\n",
        "                _, G_loss_curr, G_l2_loss_curr = sess.run([self.gan.G_solver, self.gan.G_loss, self.gan.G_l2_loss],\n",
        "                        feed_dict={self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\n",
        "                G_loss += G_loss_curr\n",
        "                G_l2_loss += G_l2_loss_curr\n",
        "                \n",
        "            g_loss_array.append(G_loss_curr - G_l2_loss)\n",
        "            d_loss_array.append(D_loss_curr - D_l2_loss)\n",
        "            \n",
        "            \n",
        "            if (i+1) % print_steps == 0:\n",
        "                print('Step={} D_loss={}, G_loss={}'.format(i + int(currentStep), D_loss/print_steps - D_l2_loss/print_steps, G_loss/print_steps - G_l2_loss/print_steps))\n",
        "                #print('D_l2_loss = {} G_l2_loss={}'.format(D_l2_loss/print_steps, G_l2_loss/print_steps))\n",
        "                G_loss = 0\n",
        "                D_loss = 0\n",
        "                G_l2_loss = 0\n",
        "                D_l2_loss = 0\n",
        "            if (i+1) % save_steps == 0:\n",
        "                saver.save(sess, f'{googlepath}/models/gan.ckpt', i + int(currentStep))\n",
        "            \n",
        "            # end training at training_amount epochs\n",
        "            if ((i + int(currentStep)) > TRAINING_AMOUNT):\n",
        "                \n",
        "                print(\"Reached {} epochs for GAN\".format(i + int(currentStep)))\n",
        "                sess.close()\n",
        "                \n",
        "                axisX = np.arange(0,len(g_loss_array),1)\n",
        "                plt.plot(axisX, g_loss_array, label='generator loss')\n",
        "                plt.plot(axisX, d_loss_array, label='discriminator loss')\n",
        "                plt.legend()\n",
        "                plt.title('generator and discriminator loss')\n",
        "                plt.show()\n",
        "                \n",
        "                break\n",
        "\n",
        "            # if (i+1) % display_data == 0:\n",
        "            #     print('Generated Data')\n",
        "            #     print(sess.run(self.gan.gen_data, feed_dict={self.gan.Z:self.gan.sample_Z(1, 200)}))\n",
        "            #     print('Real Data')\n",
        "            #     print(X[0])\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "tf.reset_default_graph()\n",
        "gan = TrainGan(HISTORICAL_DAYS_AMOUNT, 128)\n",
        "gan.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AMZN.csv\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AAPL.csv\n",
            "Tensor(\"dropout/mul_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"dropout_1/mul_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"dropout_2/mul_1:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"dropout_4/mul_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"dropout_5/mul_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"dropout_6/mul_1:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Step=99 D_loss=21.482124568223952, G_loss=1003.659901984036\n",
            "Step=199 D_loss=4.823235703706741, G_loss=972.0354927682877\n",
            "Step=299 D_loss=2.394378743171692, G_loss=884.8510018214583\n",
            "Step=399 D_loss=2.005262258052826, G_loss=773.7338873827457\n",
            "Step=499 D_loss=1.5702294325828554, G_loss=709.1177568811179\n",
            "Step=599 D_loss=1.1594652080535888, G_loss=689.5925608286261\n",
            "Step=699 D_loss=0.7877929711341858, G_loss=661.0052268096804\n",
            "Step=799 D_loss=0.7211698257923125, G_loss=650.790792374909\n",
            "Step=899 D_loss=0.4893750464916229, G_loss=663.5066134673358\n",
            "Step=999 D_loss=0.5363713884353636, G_loss=683.0153897365927\n",
            "Step=1099 D_loss=0.2690894949436189, G_loss=679.9851201674342\n",
            "Step=1199 D_loss=0.3260520005226135, G_loss=678.0498606917263\n",
            "Step=1299 D_loss=0.2621238470077514, G_loss=699.3449530783296\n",
            "Step=1399 D_loss=0.2678714823722841, G_loss=682.6424646419287\n",
            "Step=1499 D_loss=0.15121254682540908, G_loss=696.956688734889\n",
            "Step=1599 D_loss=0.18792173624038688, G_loss=723.1149037784338\n",
            "Step=1699 D_loss=0.097623188495636, G_loss=757.4127888515592\n",
            "Step=1799 D_loss=0.14390256762504583, G_loss=735.9834107753635\n",
            "Step=1899 D_loss=0.14697734951972974, G_loss=760.297486602366\n",
            "Step=1999 D_loss=0.07502165079116829, G_loss=767.6319904288649\n",
            "Step=2099 D_loss=0.07600696086883563, G_loss=725.0871964722871\n",
            "Step=2199 D_loss=0.11817311286926269, G_loss=722.7946602016688\n",
            "Step=2299 D_loss=0.0677603745460511, G_loss=717.4249978345632\n",
            "Step=2399 D_loss=0.12118443131446832, G_loss=691.7012282684445\n",
            "Step=2499 D_loss=0.13256907939910878, G_loss=673.8006085717678\n",
            "Step=2599 D_loss=0.10701594710350038, G_loss=676.2088297140598\n",
            "Step=2699 D_loss=0.047173702716827304, G_loss=686.3694990098477\n",
            "Step=2799 D_loss=0.09108238577842709, G_loss=667.2923990491033\n",
            "Step=2899 D_loss=0.13180714607238775, G_loss=651.6191401460767\n",
            "Step=2999 D_loss=0.05202345252037044, G_loss=585.1303417423368\n",
            "Step=3099 D_loss=0.1384107816219331, G_loss=594.0517138567567\n",
            "Step=3199 D_loss=0.13463665843009953, G_loss=573.408214686811\n",
            "Step=3299 D_loss=0.11584906816482543, G_loss=527.8816125610471\n",
            "Step=3399 D_loss=0.19725380539894122, G_loss=508.3439668381214\n",
            "Step=3499 D_loss=0.10859749674797059, G_loss=472.0866439092159\n",
            "Step=3599 D_loss=0.1671544313430786, G_loss=430.76201215952636\n",
            "Step=3699 D_loss=0.2574426710605622, G_loss=415.41488360762594\n",
            "Step=3799 D_loss=0.48880385398864745, G_loss=380.279152494967\n",
            "Step=3899 D_loss=0.6927109777927398, G_loss=385.3662822690606\n",
            "Step=3999 D_loss=0.8850274729728698, G_loss=346.3308219978214\n",
            "Step=4099 D_loss=0.9642401552200317, G_loss=323.1309340664744\n",
            "Step=4199 D_loss=0.7479719495773316, G_loss=274.8779580026865\n",
            "Step=4299 D_loss=1.0202482688426973, G_loss=230.7633512353897\n",
            "Step=4399 D_loss=0.7474058723449708, G_loss=202.41531512349843\n",
            "Step=4499 D_loss=0.9694397163391113, G_loss=163.71412862479684\n",
            "Step=4599 D_loss=0.6235198473930359, G_loss=146.71389400988815\n",
            "Step=4699 D_loss=0.44648866415023813, G_loss=132.76276130199432\n",
            "Step=4799 D_loss=0.4305442130565642, G_loss=117.48234938383102\n",
            "Step=4899 D_loss=0.6001283395290375, G_loss=100.24636439234018\n",
            "Step=4999 D_loss=0.5287986981868742, G_loss=92.07341179400683\n",
            "Step=5099 D_loss=0.5286005091667174, G_loss=80.07631469517946\n",
            "Step=5199 D_loss=0.42115316510200507, G_loss=73.41110347658396\n",
            "Step=5299 D_loss=0.29895287871360776, G_loss=65.67180239707231\n",
            "Step=5399 D_loss=0.2839674150943756, G_loss=62.85736809045076\n",
            "Step=5499 D_loss=0.2479951035976411, G_loss=59.456213299632076\n",
            "Step=5599 D_loss=0.193868237733841, G_loss=56.04013276696205\n",
            "Step=5699 D_loss=0.17896413326263438, G_loss=53.924531018137934\n",
            "Step=5799 D_loss=0.08617854356765742, G_loss=52.71308687776327\n",
            "Step=5899 D_loss=0.16655039668083194, G_loss=49.46681874930859\n",
            "Step=5999 D_loss=0.0940324151515961, G_loss=46.820494781732556\n",
            "Step=6099 D_loss=0.1309439909458161, G_loss=45.34221539020539\n",
            "Step=6199 D_loss=0.06462140917778014, G_loss=45.417413767874244\n",
            "Step=6299 D_loss=0.10111437916755683, G_loss=44.599119243919844\n",
            "Step=6399 D_loss=0.062404766082763574, G_loss=43.991257545351985\n",
            "Step=6499 D_loss=0.07087684869766231, G_loss=43.085535711050035\n",
            "Step=6599 D_loss=0.0344735157489775, G_loss=41.50346302986145\n",
            "Step=6699 D_loss=0.03172168970108036, G_loss=40.96745768070221\n",
            "Step=6799 D_loss=0.03021258711814867, G_loss=41.035902818739416\n",
            "Step=6899 D_loss=0.03840612173080449, G_loss=41.99581870734692\n",
            "Step=6999 D_loss=0.029005599021911577, G_loss=40.853871484398844\n",
            "Step=7099 D_loss=0.043684006929397556, G_loss=43.30623542636633\n",
            "Step=7199 D_loss=0.03143383741378791, G_loss=41.74386103481054\n",
            "Step=7299 D_loss=0.027016048431396378, G_loss=42.804553261995316\n",
            "Step=7399 D_loss=0.04326787233352669, G_loss=46.52576222628355\n",
            "Step=7499 D_loss=0.023229632377624432, G_loss=44.754980816841126\n",
            "Step=7599 D_loss=0.015270376205444247, G_loss=45.61937651306391\n",
            "Step=7699 D_loss=0.017479885816574114, G_loss=43.04947779178619\n",
            "Step=7799 D_loss=0.01067161083221424, G_loss=41.764116862416266\n",
            "Step=7899 D_loss=0.006972281932830837, G_loss=41.937511722743515\n",
            "Step=7999 D_loss=0.012985860109329339, G_loss=43.26608218163252\n",
            "Step=8099 D_loss=0.028124758005142247, G_loss=44.74328700572252\n",
            "Step=8199 D_loss=0.0166414856910706, G_loss=49.383515779674056\n",
            "Step=8299 D_loss=0.010055571794509888, G_loss=51.91720933407545\n",
            "Step=8399 D_loss=0.0009281861782073353, G_loss=47.493417184650895\n",
            "Step=8499 D_loss=0.004521496295929017, G_loss=45.85324155658484\n",
            "Step=8599 D_loss=0.004829933643341056, G_loss=44.31548895686865\n",
            "Step=8699 D_loss=0.0038468420505524126, G_loss=42.93443937689066\n",
            "Step=8799 D_loss=0.007531316280364964, G_loss=41.33050384670496\n",
            "Step=8899 D_loss=0.005857591629028214, G_loss=43.26003482431173\n",
            "Step=8999 D_loss=0.0031549775600432373, G_loss=44.043631010353565\n",
            "Step=9099 D_loss=0.003846890926361146, G_loss=43.75247846633196\n",
            "Step=9199 D_loss=0.01607775449752813, G_loss=44.88942289680243\n",
            "Step=9299 D_loss=0.013570747375488246, G_loss=46.664570089578625\n",
            "Step=9399 D_loss=0.0038370561599729847, G_loss=46.758994833528995\n",
            "Step=9499 D_loss=0.006801439523696828, G_loss=46.8192294126749\n",
            "Step=9599 D_loss=0.010602716207504193, G_loss=47.2521297737956\n",
            "Step=9699 D_loss=0.009301170110702639, G_loss=46.39478504508734\n",
            "Step=9799 D_loss=0.014024473428726036, G_loss=47.331220595240595\n",
            "Step=9899 D_loss=0.0031042385101318626, G_loss=51.27050706207752\n",
            "Step=9999 D_loss=0.010947915315627998, G_loss=49.08213692754507\n",
            "Step=10099 D_loss=0.0018546140193937966, G_loss=49.725262292325496\n",
            "Step=10199 D_loss=0.0002546358108519442, G_loss=46.609252310693265\n",
            "Step=10299 D_loss=0.015024766921996946, G_loss=46.35052593916655\n",
            "Step=10399 D_loss=0.004245952367782468, G_loss=44.16127638369799\n",
            "Step=10499 D_loss=0.01064187049865728, G_loss=43.24815298378467\n",
            "Step=10599 D_loss=0.002700401544570852, G_loss=47.48083198726177\n",
            "Step=10699 D_loss=0.0016549134254455478, G_loss=43.97771740049124\n",
            "Step=10799 D_loss=0.014164818525314482, G_loss=44.26180844157934\n",
            "Step=10899 D_loss=0.0004578566551209917, G_loss=47.45204474002123\n",
            "Step=10999 D_loss=0.003196210861206028, G_loss=45.754925760328774\n",
            "Step=11099 D_loss=0.004648934602737409, G_loss=45.0300453093648\n",
            "Step=11199 D_loss=0.002035174369812154, G_loss=46.97615761309862\n",
            "Step=11299 D_loss=0.006536514759063694, G_loss=47.60885443001986\n",
            "Step=11399 D_loss=0.003063472509384102, G_loss=42.36073532164097\n",
            "Step=11499 D_loss=0.00322398662567136, G_loss=44.985983563363554\n",
            "Step=11599 D_loss=0.0026061654090880904, G_loss=44.62949540317059\n",
            "Step=11699 D_loss=0.002868509292602628, G_loss=45.60677618205547\n",
            "Step=11799 D_loss=0.003558263778686621, G_loss=49.50543844014406\n",
            "Step=11899 D_loss=0.00212857604026806, G_loss=52.82649761229753\n",
            "Step=11999 D_loss=0.004116851091384932, G_loss=50.44663000226021\n",
            "Step=12099 D_loss=0.0025685966014863393, G_loss=58.0670344042778\n",
            "Step=12199 D_loss=0.006561642885208219, G_loss=53.00163272380829\n",
            "Step=12299 D_loss=0.004712405204772896, G_loss=51.35559044033289\n",
            "Step=12399 D_loss=0.006689550280571055, G_loss=54.08144679814577\n",
            "Step=12499 D_loss=0.005584955215454213, G_loss=56.16943987607956\n",
            "Step=12599 D_loss=0.01381670653820044, G_loss=58.88398895174265\n",
            "Step=12699 D_loss=0.008916932940483124, G_loss=57.058445339500906\n",
            "Step=12799 D_loss=0.01022160589694976, G_loss=56.61638762027025\n",
            "Step=12899 D_loss=0.005255486369132978, G_loss=49.61682926237583\n",
            "Step=12999 D_loss=0.009222534298896745, G_loss=48.875326712131496\n",
            "Step=13099 D_loss=0.005102611780166688, G_loss=48.67842399537563\n",
            "Step=13199 D_loss=0.002169737219810508, G_loss=52.09333392709494\n",
            "Step=13299 D_loss=0.003218753933906582, G_loss=51.221558770239355\n",
            "Step=13399 D_loss=0.001993215680122451, G_loss=51.27590317994356\n",
            "Step=13499 D_loss=0.002336700558662419, G_loss=51.17058130532503\n",
            "Step=13599 D_loss=0.006120736598968501, G_loss=52.093834061622616\n",
            "Step=13699 D_loss=0.0023420190811156427, G_loss=54.492632252275946\n",
            "Step=13799 D_loss=0.0012173545360565363, G_loss=57.577142033278946\n",
            "Step=13899 D_loss=0.0011552786827088157, G_loss=54.968311582505706\n",
            "Step=13999 D_loss=0.007292708158493033, G_loss=52.51427053898573\n",
            "Step=14099 D_loss=0.005888320207595776, G_loss=52.26409152120351\n",
            "Step=14199 D_loss=0.012318046092987034, G_loss=51.5377168622613\n",
            "Step=14299 D_loss=0.00788758456707006, G_loss=49.3435191577673\n",
            "Step=14399 D_loss=0.006694941520690834, G_loss=49.92409689158201\n",
            "Step=14499 D_loss=0.008082182407379124, G_loss=51.7006891900301\n",
            "Step=14599 D_loss=0.0036041188240051314, G_loss=55.19512050718069\n",
            "Step=14699 D_loss=0.011922987699508614, G_loss=52.56683061391115\n",
            "Step=14799 D_loss=0.010802626013755812, G_loss=52.87564666241407\n",
            "Step=14899 D_loss=0.00622356772422783, G_loss=53.10886299729347\n",
            "Step=14999 D_loss=0.005469939112663247, G_loss=50.610016568899155\n",
            "Step=15099 D_loss=0.01295071065425879, G_loss=52.26266687989235\n",
            "Step=15199 D_loss=0.005423591732978905, G_loss=50.300400310754775\n",
            "Step=15299 D_loss=0.004927147626876849, G_loss=54.99263513684273\n",
            "Step=15399 D_loss=0.0038481414318085605, G_loss=52.710344969928265\n",
            "Step=15499 D_loss=0.015698424577712977, G_loss=50.58148018091917\n",
            "Step=15599 D_loss=0.005936629176139907, G_loss=50.28980235695839\n",
            "Step=15699 D_loss=0.007749235033988944, G_loss=51.07309981524944\n",
            "Step=15799 D_loss=0.006170409917831465, G_loss=51.36927479922771\n",
            "Step=15899 D_loss=0.0027418875694275036, G_loss=52.35748892337084\n",
            "Step=15999 D_loss=0.008136807084083597, G_loss=53.06423255503178\n",
            "Step=16099 D_loss=0.010835969448089622, G_loss=52.40152679145336\n",
            "Step=16199 D_loss=0.0224424862861633, G_loss=51.913821521699425\n",
            "Step=16299 D_loss=0.01064541101455685, G_loss=58.095612840652464\n",
            "Step=16399 D_loss=0.014709033370018054, G_loss=54.64870727658272\n",
            "Step=16499 D_loss=0.01282705008983609, G_loss=51.20886054456234\n",
            "Step=16599 D_loss=0.02707134544849399, G_loss=48.65048422962427\n",
            "Step=16699 D_loss=0.0029213637113570945, G_loss=52.4882365000248\n",
            "Step=16799 D_loss=0.009241418242454613, G_loss=51.59945016175509\n",
            "Step=16899 D_loss=0.002658070325851414, G_loss=49.869310685396194\n",
            "Step=16999 D_loss=0.017738724946975704, G_loss=48.16955376267433\n",
            "Step=17099 D_loss=0.014324507713317836, G_loss=46.35820666134357\n",
            "Step=17199 D_loss=0.006556730270385724, G_loss=49.7272969570756\n",
            "Step=17299 D_loss=0.012224344015121513, G_loss=46.900634154081345\n",
            "Step=17399 D_loss=0.003491813540458688, G_loss=47.44059712260962\n",
            "Step=17499 D_loss=0.007855747342109698, G_loss=46.94674344301224\n",
            "Step=17599 D_loss=0.010688743591308558, G_loss=47.82861690849065\n",
            "Step=17699 D_loss=0.013968054652214068, G_loss=46.422756529152394\n",
            "Step=17799 D_loss=0.009192306399345429, G_loss=47.69786895424127\n",
            "Step=17899 D_loss=0.004993176460266091, G_loss=46.69709849029779\n",
            "Step=17999 D_loss=0.01459020078182216, G_loss=43.70189439415932\n",
            "Step=18099 D_loss=0.00835358500480654, G_loss=44.94802535712719\n",
            "Step=18199 D_loss=0.011427218317985544, G_loss=44.265106943845744\n",
            "Step=18299 D_loss=0.013902209401130627, G_loss=42.004621454477316\n",
            "Step=18399 D_loss=0.014417031407356284, G_loss=44.70593490600586\n",
            "Step=18499 D_loss=0.013426036834716881, G_loss=45.361297726631165\n",
            "Step=18599 D_loss=0.005269072651863049, G_loss=44.11460151165724\n",
            "Step=18699 D_loss=0.0075763809680938765, G_loss=46.44786859720945\n",
            "Step=18799 D_loss=0.006320175528526373, G_loss=45.56538430631161\n",
            "Step=18899 D_loss=0.015108918547630301, G_loss=42.669351065456866\n",
            "Step=18999 D_loss=0.005617043972015456, G_loss=40.31344820737839\n",
            "Step=19099 D_loss=0.0175771081447601, G_loss=40.6741670113802\n",
            "Step=19199 D_loss=0.013898127079009948, G_loss=41.72906393021345\n",
            "Step=19299 D_loss=0.018739547729492156, G_loss=44.09600352287292\n",
            "Step=19399 D_loss=0.013023558855056683, G_loss=42.425597890615464\n",
            "Step=19499 D_loss=0.01241889655590056, G_loss=40.171471596360206\n",
            "Step=19599 D_loss=0.010288068652153037, G_loss=40.309540656805034\n",
            "Step=19699 D_loss=0.008479738235473655, G_loss=42.71853475630284\n",
            "Step=19799 D_loss=0.00949381887912748, G_loss=40.57996098339558\n",
            "Step=19899 D_loss=0.016912739872932403, G_loss=38.72657344847918\n",
            "Step=19999 D_loss=0.006160516738891619, G_loss=39.00614632219076\n",
            "Step=20099 D_loss=0.02151648402214046, G_loss=40.921352430284024\n",
            "Step=20199 D_loss=0.00868207037448887, G_loss=38.919460914433\n",
            "Step=20299 D_loss=0.01689424932003014, G_loss=38.0299452060461\n",
            "Step=20399 D_loss=0.010523684620857177, G_loss=38.74695771306753\n",
            "Step=20499 D_loss=0.011178336143493617, G_loss=37.3201825094223\n",
            "Step=20599 D_loss=0.024267885684967072, G_loss=35.4483464050293\n",
            "Step=20699 D_loss=0.016067641973495528, G_loss=36.2382828310132\n",
            "Step=20799 D_loss=0.014034283757209831, G_loss=35.37023947536945\n",
            "Step=20899 D_loss=0.009953231215476976, G_loss=34.93150750309229\n",
            "Step=20999 D_loss=0.010584540367126527, G_loss=34.845227956175805\n",
            "Step=21099 D_loss=0.012696029543876719, G_loss=33.291042016148566\n",
            "Step=21199 D_loss=0.009647866487503043, G_loss=33.71927164822817\n",
            "Step=21299 D_loss=0.01485879361629483, G_loss=33.821060208082194\n",
            "Step=21399 D_loss=0.011019006371498108, G_loss=33.06857138037682\n",
            "Step=21499 D_loss=0.014059142470359776, G_loss=32.434310011565685\n",
            "Step=21599 D_loss=0.011290816068649345, G_loss=33.24461304754019\n",
            "Step=21699 D_loss=0.001929444074630693, G_loss=32.09508318990469\n",
            "Step=21799 D_loss=0.010860260725021331, G_loss=30.731794810295106\n",
            "Step=21899 D_loss=0.014677484035491961, G_loss=33.03600147306919\n",
            "Step=21999 D_loss=0.010095108151435772, G_loss=29.444365468025207\n",
            "Step=22099 D_loss=0.017008461952209508, G_loss=30.46860453516245\n",
            "Step=22199 D_loss=0.013336355090141305, G_loss=29.598371706306935\n",
            "Step=22299 D_loss=0.022710035443305965, G_loss=29.975353399217127\n",
            "Step=22399 D_loss=0.009416354298591667, G_loss=29.84889733403921\n",
            "Step=22499 D_loss=0.019614713191986044, G_loss=28.72736551076174\n",
            "Step=22599 D_loss=0.024115012884139975, G_loss=29.450427421927454\n",
            "Step=22699 D_loss=0.016300806999206552, G_loss=28.0626079466939\n",
            "Step=22799 D_loss=0.022667514085769613, G_loss=26.86081332445145\n",
            "Step=22899 D_loss=0.009912376999855033, G_loss=27.92025580704212\n",
            "Step=22999 D_loss=0.02564668357372285, G_loss=25.910294038951395\n",
            "Step=23099 D_loss=0.014472009241580985, G_loss=25.102209272980687\n",
            "Step=23199 D_loss=0.017053149938583312, G_loss=27.61786691188812\n",
            "Step=23299 D_loss=0.03378829926252369, G_loss=27.14655752927065\n",
            "Step=23399 D_loss=0.02022643148899078, G_loss=25.2817592138052\n",
            "Step=23499 D_loss=0.023543321192264544, G_loss=23.26524154305458\n",
            "Step=23599 D_loss=0.013332107663154569, G_loss=23.62989760667086\n",
            "Step=23699 D_loss=0.01982987999916075, G_loss=23.972131704986097\n",
            "Step=23799 D_loss=0.021274716258049053, G_loss=23.307669976353647\n",
            "Step=23899 D_loss=0.022745550572872142, G_loss=24.352969474196435\n",
            "Step=23999 D_loss=0.02728372931480405, G_loss=24.41510195374489\n",
            "Step=24099 D_loss=0.013407062590122198, G_loss=23.160842022299768\n",
            "Step=24199 D_loss=0.01424576550722123, G_loss=22.514930620789528\n",
            "Step=24299 D_loss=0.0187085756659508, G_loss=21.522176119387147\n",
            "Step=24399 D_loss=0.022186486423015572, G_loss=22.401161875724792\n",
            "Step=24499 D_loss=0.02698250770568844, G_loss=22.776010739207265\n",
            "Step=24599 D_loss=0.020700456202030204, G_loss=20.62545336008072\n",
            "Step=24699 D_loss=0.012496873140335096, G_loss=20.978166756927966\n",
            "Step=24799 D_loss=0.020514875948429157, G_loss=19.908682297170163\n",
            "Step=24899 D_loss=0.022974866032600405, G_loss=19.973481741845607\n",
            "Step=24999 D_loss=0.030105178654193876, G_loss=18.92622697919607\n",
            "Step=25099 D_loss=0.021485908925533315, G_loss=19.99172345787287\n",
            "Step=25199 D_loss=0.016806313395500205, G_loss=18.10233975708485\n",
            "Step=25299 D_loss=0.013025436103344001, G_loss=18.161052489578722\n",
            "Step=25399 D_loss=0.017629898190498317, G_loss=18.117292785942553\n",
            "Step=25499 D_loss=0.02618974983692174, G_loss=18.257374028265474\n",
            "Step=25599 D_loss=0.01736764371395111, G_loss=17.70118085205555\n",
            "Step=25699 D_loss=0.025898182392120384, G_loss=17.663751231133936\n",
            "Step=25799 D_loss=0.030428976416587872, G_loss=16.508527975678444\n",
            "Step=25899 D_loss=0.015496183931827556, G_loss=17.25269666314125\n",
            "Step=25999 D_loss=0.024042647480964685, G_loss=15.58844048857689\n",
            "Step=26099 D_loss=0.024798889160156268, G_loss=15.981775848269464\n",
            "Step=26199 D_loss=0.01791290193796158, G_loss=15.233582716584205\n",
            "Step=26299 D_loss=0.017886953949928286, G_loss=15.141105235815049\n",
            "Step=26399 D_loss=0.021448859274387333, G_loss=14.932418273091317\n",
            "Step=26499 D_loss=0.0184354278445244, G_loss=14.178917556107043\n",
            "Step=26599 D_loss=0.022692278921604192, G_loss=14.888164961338044\n",
            "Step=26699 D_loss=0.023792854845523848, G_loss=13.936572259366512\n",
            "Step=26799 D_loss=0.020367908477783192, G_loss=13.301604259610176\n",
            "Step=26899 D_loss=0.02468264669179915, G_loss=12.834676834642886\n",
            "Step=26999 D_loss=0.01778836876153944, G_loss=12.655047609508037\n",
            "Step=27099 D_loss=0.0202729558944702, G_loss=12.623079511225223\n",
            "Step=27199 D_loss=0.02301565885543827, G_loss=12.37335668116808\n",
            "Step=27299 D_loss=0.02257389307022095, G_loss=12.210100161731242\n",
            "Step=27399 D_loss=0.02891904830932618, G_loss=12.157853270173073\n",
            "Step=27499 D_loss=0.017822767198085787, G_loss=11.4826840159297\n",
            "Step=27599 D_loss=0.02403671175241473, G_loss=11.288180223107338\n",
            "Step=27699 D_loss=0.020590818226337404, G_loss=11.512916503548622\n",
            "Step=27799 D_loss=0.01924738585948943, G_loss=11.612540243268013\n",
            "Step=27899 D_loss=0.02139943838119507, G_loss=10.902496794760227\n",
            "Step=27999 D_loss=0.018052998781204255, G_loss=10.76961982280016\n",
            "Step=28099 D_loss=0.015954875946044944, G_loss=10.791099511682988\n",
            "Step=28199 D_loss=0.01606894254684449, G_loss=10.762767446637154\n",
            "Step=28299 D_loss=0.019967458546161665, G_loss=10.70295087248087\n",
            "Step=28399 D_loss=0.01972509592771532, G_loss=10.585281345844269\n",
            "Step=28499 D_loss=0.01910542160272599, G_loss=10.77777435719967\n",
            "Step=28599 D_loss=0.021944288909435283, G_loss=10.259574390351773\n",
            "Step=28699 D_loss=0.01789584904909136, G_loss=9.524610497951507\n",
            "Step=28799 D_loss=0.01580542922019962, G_loss=9.452309254705906\n",
            "Step=28899 D_loss=0.024562302827835114, G_loss=9.698191080093384\n",
            "Step=28999 D_loss=0.017056847810745235, G_loss=9.559802200198172\n",
            "Step=29099 D_loss=0.019206219613552122, G_loss=9.520542685985564\n",
            "Step=29199 D_loss=0.016006855666637454, G_loss=9.164740710556508\n",
            "Step=29299 D_loss=0.018121521174907673, G_loss=9.045177892148494\n",
            "Step=29399 D_loss=0.018781926929950732, G_loss=9.063865081965924\n",
            "Step=29499 D_loss=0.015494236052036259, G_loss=9.02638886153698\n",
            "Step=29599 D_loss=0.0162419056892395, G_loss=8.778857448399068\n",
            "Step=29699 D_loss=0.01859196543693542, G_loss=8.670038317739962\n",
            "Step=29799 D_loss=0.017229852378368382, G_loss=8.545880849659444\n",
            "Step=29899 D_loss=0.014207857549190517, G_loss=8.525912019908429\n",
            "Step=29999 D_loss=0.017955082356929775, G_loss=8.318547497093677\n",
            "Step=30099 D_loss=0.01857239902019503, G_loss=8.489872536659242\n",
            "Step=30199 D_loss=0.021102490723133083, G_loss=8.133540408313275\n",
            "Step=30299 D_loss=0.017555801868438692, G_loss=8.48079869568348\n",
            "Step=30399 D_loss=0.026962068080902124, G_loss=7.934101797044278\n",
            "Step=30499 D_loss=0.014815315902233117, G_loss=8.091654882729054\n",
            "Step=30599 D_loss=0.012859534621238733, G_loss=8.352570132911204\n",
            "Step=30699 D_loss=0.010087288618087775, G_loss=8.42231243878603\n",
            "Step=30799 D_loss=0.014494346976280215, G_loss=8.489761835634708\n",
            "Step=30899 D_loss=0.01918187886476519, G_loss=8.099732205867767\n",
            "Step=30999 D_loss=0.012708870172500608, G_loss=8.041726480424405\n",
            "Step=31099 D_loss=0.017268162965774547, G_loss=7.851744610071182\n",
            "Step=31199 D_loss=0.01651866257190704, G_loss=7.79052600055933\n",
            "Step=31299 D_loss=0.013446268141269646, G_loss=8.177651962339878\n",
            "Step=31399 D_loss=0.01820613622665407, G_loss=8.003921644985676\n",
            "Step=31499 D_loss=0.02468419939279559, G_loss=7.422919447124005\n",
            "Step=31599 D_loss=0.017536457479000067, G_loss=7.734863512814044\n",
            "Step=31699 D_loss=0.014196691215038304, G_loss=7.952317290604114\n",
            "Step=31799 D_loss=0.017767244577407804, G_loss=7.806571199297906\n",
            "Step=31899 D_loss=0.018519785702228564, G_loss=7.458819968402386\n",
            "Step=31999 D_loss=0.01594844818115232, G_loss=7.566800961494446\n",
            "Step=32099 D_loss=0.011711512804031377, G_loss=7.93610808044672\n",
            "Step=32199 D_loss=0.01153204977512362, G_loss=8.028629485666752\n",
            "Step=32299 D_loss=0.018993383944034548, G_loss=7.8458690482378\n",
            "Step=32399 D_loss=0.02138011664152145, G_loss=7.444626751542091\n",
            "Step=32499 D_loss=0.01811338990926742, G_loss=7.448327645361424\n",
            "Step=32599 D_loss=0.020318521559238467, G_loss=7.134240956902504\n",
            "Step=32699 D_loss=0.017827560305595402, G_loss=7.451444611549377\n",
            "Step=32799 D_loss=0.014241482615470924, G_loss=7.971628300249577\n",
            "Step=32899 D_loss=0.01782020568847653, G_loss=8.05303160816431\n",
            "Step=32999 D_loss=0.01544561892747881, G_loss=7.468288028240204\n",
            "Step=33099 D_loss=0.017209114730358133, G_loss=7.321218191683292\n",
            "Step=33199 D_loss=0.012879889905452746, G_loss=7.54586040109396\n",
            "Step=33299 D_loss=0.01853170126676562, G_loss=7.355096336007119\n",
            "Step=33399 D_loss=0.015861086249351508, G_loss=7.299506057202816\n",
            "Step=33499 D_loss=0.0198653346300125, G_loss=7.342872915267945\n",
            "Step=33599 D_loss=0.016093715131282826, G_loss=7.640483490824699\n",
            "Step=33699 D_loss=0.015021021664142653, G_loss=7.303717768788338\n",
            "Step=33799 D_loss=0.014978252947330517, G_loss=7.14796971321106\n",
            "Step=33899 D_loss=0.019892561733722697, G_loss=7.096399655938149\n",
            "Step=33999 D_loss=0.01684057250618931, G_loss=7.247713135182858\n",
            "Step=34099 D_loss=0.0164837111532688, G_loss=7.004489979445934\n",
            "Step=34199 D_loss=0.013440596312284475, G_loss=7.2317769581079485\n",
            "Step=34299 D_loss=0.019217854887247116, G_loss=6.996317217350006\n",
            "Step=34399 D_loss=0.015390352904796567, G_loss=7.094254433810711\n",
            "Step=34499 D_loss=0.018181682676076893, G_loss=6.91102107077837\n",
            "Step=34599 D_loss=0.018767535388469675, G_loss=7.031683187782765\n",
            "Step=34699 D_loss=0.01637621313333512, G_loss=6.9763768538832664\n",
            "Step=34799 D_loss=0.01389558941125868, G_loss=7.085117882490158\n",
            "Step=34899 D_loss=0.021114922612905512, G_loss=6.74721248537302\n",
            "Step=34999 D_loss=0.019405475407838824, G_loss=6.790938816964626\n",
            "Step=35099 D_loss=0.023046683818101876, G_loss=6.631665202379226\n",
            "Step=35199 D_loss=0.008924088925123225, G_loss=7.005169762670994\n",
            "Step=35299 D_loss=0.019633799493312842, G_loss=6.848501500189305\n",
            "Step=35399 D_loss=0.020712766498327234, G_loss=6.586146169900894\n",
            "Step=35499 D_loss=0.014437082707881937, G_loss=7.060203197300434\n",
            "Step=35599 D_loss=0.01574164003133774, G_loss=6.985348061919212\n",
            "Step=35699 D_loss=0.01544455736875533, G_loss=6.955950379967689\n",
            "Step=35799 D_loss=0.01852712854743005, G_loss=6.5710544231534005\n",
            "Step=35899 D_loss=0.018473030775785437, G_loss=6.59567348062992\n",
            "Step=35999 D_loss=0.015300169587135315, G_loss=6.840809923112393\n",
            "Step=36099 D_loss=0.01306843191385268, G_loss=6.868812111616134\n",
            "Step=36199 D_loss=0.012226602733135222, G_loss=7.085221127271652\n",
            "Step=36299 D_loss=0.01801265612244607, G_loss=6.929929268062114\n",
            "Step=36399 D_loss=0.018242779672145826, G_loss=6.730360157191753\n",
            "Step=36499 D_loss=0.015824028253555278, G_loss=7.198914824426175\n",
            "Step=36599 D_loss=0.015607806891202919, G_loss=7.189682100713252\n",
            "Step=36699 D_loss=0.018489780277013784, G_loss=7.67400651961565\n",
            "Step=36799 D_loss=0.013989184498786916, G_loss=7.176264736950397\n",
            "Step=36899 D_loss=0.012153803557157505, G_loss=7.374726130366326\n",
            "Step=36999 D_loss=0.013193256556987748, G_loss=7.402559720277786\n",
            "Step=37099 D_loss=0.01622003450989723, G_loss=7.298045434653759\n",
            "Step=37199 D_loss=0.019296870529651622, G_loss=7.192651298046112\n",
            "Step=37299 D_loss=0.012403269410133366, G_loss=6.964253259599209\n",
            "Step=37399 D_loss=0.01241863280534744, G_loss=7.164413877427578\n",
            "Step=37499 D_loss=0.013475209623575207, G_loss=7.128646585047245\n",
            "Step=37599 D_loss=0.008757234811782838, G_loss=7.355036916136742\n",
            "Step=37699 D_loss=0.020802604407072073, G_loss=7.143156455159188\n",
            "Step=37799 D_loss=0.009112170189619073, G_loss=6.812693393230439\n",
            "Step=37899 D_loss=0.014167789518833157, G_loss=7.482041392624378\n",
            "Step=37999 D_loss=0.015076686441898357, G_loss=6.8206987783312805\n",
            "Step=38099 D_loss=0.012206717282533641, G_loss=6.894082531034947\n",
            "Step=38199 D_loss=0.015079036355018616, G_loss=6.759418312907219\n",
            "Step=38299 D_loss=0.01571990504860879, G_loss=6.709038005471229\n",
            "Step=38399 D_loss=0.01789464935660362, G_loss=6.638393425345421\n",
            "Step=38499 D_loss=0.01421296834945679, G_loss=6.675013872385025\n",
            "Step=38599 D_loss=0.017540667504072183, G_loss=6.774478488862514\n",
            "Step=38699 D_loss=0.013636101782321941, G_loss=7.81746939778328\n",
            "Step=38799 D_loss=0.014673869609832774, G_loss=7.855416021943092\n",
            "Step=38899 D_loss=0.015521377474069592, G_loss=6.905665534436702\n",
            "Step=38999 D_loss=0.014732108712196351, G_loss=6.996924573183059\n",
            "Step=39099 D_loss=0.01318251132965087, G_loss=6.913927406668663\n",
            "Step=39199 D_loss=0.015195945501327524, G_loss=6.761004414558411\n",
            "Step=39299 D_loss=0.014979118257760993, G_loss=6.827261364459991\n",
            "Step=39399 D_loss=0.015235769599676152, G_loss=6.561978781819344\n",
            "Step=39499 D_loss=0.014101356118917469, G_loss=6.897035359144211\n",
            "Step=39599 D_loss=0.01616902768611908, G_loss=6.648035628497601\n",
            "Step=39699 D_loss=0.016695754528045642, G_loss=6.8411151841282845\n",
            "Step=39799 D_loss=0.020385178476572036, G_loss=6.373487092554569\n",
            "Step=39899 D_loss=0.015306692570447922, G_loss=6.995688470900059\n",
            "Step=39999 D_loss=0.015475145131349566, G_loss=6.8363670575618745\n",
            "Step=40099 D_loss=0.016100181788206075, G_loss=6.535125603377819\n",
            "Step=40199 D_loss=0.015042922347784038, G_loss=6.730128130316734\n",
            "Step=40299 D_loss=0.01776344493031501, G_loss=6.680754927694797\n",
            "Step=40399 D_loss=0.013764792233705525, G_loss=6.918525226414204\n",
            "Step=40499 D_loss=0.016999935358762736, G_loss=6.415306494832039\n",
            "Step=40599 D_loss=0.01996819704771044, G_loss=6.47637653708458\n",
            "Step=40699 D_loss=0.014117068946361527, G_loss=6.997986593842507\n",
            "Step=40799 D_loss=0.016825812608003615, G_loss=6.581196269094944\n",
            "Step=40899 D_loss=0.015498351752758044, G_loss=6.707919962108135\n",
            "Step=40999 D_loss=0.015416970998048785, G_loss=6.614316253662109\n",
            "Step=41099 D_loss=0.01791870072484017, G_loss=6.52671361118555\n",
            "Step=41199 D_loss=0.01392104595899582, G_loss=6.789419866204262\n",
            "Step=41299 D_loss=0.019496459215879447, G_loss=6.687753754258155\n",
            "Step=41399 D_loss=0.016303440034389494, G_loss=6.670328883528709\n",
            "Step=41499 D_loss=0.014757731854915634, G_loss=6.80891720622778\n",
            "Step=41599 D_loss=0.017404401153326032, G_loss=6.471890122890472\n",
            "Step=41699 D_loss=0.013346956893801679, G_loss=6.719805873334408\n",
            "Step=41799 D_loss=0.010010604634881029, G_loss=7.155765672624111\n",
            "Step=41899 D_loss=0.01779974550008774, G_loss=6.911342148780823\n",
            "Step=41999 D_loss=0.015277862548828139, G_loss=6.611052876710891\n",
            "Step=42099 D_loss=0.012812974974513058, G_loss=7.41243761986494\n",
            "Step=42199 D_loss=0.01691712282598018, G_loss=7.313137221336365\n",
            "Step=42299 D_loss=0.019055049493908877, G_loss=6.5657213655114175\n",
            "Step=42399 D_loss=0.013808443322777753, G_loss=6.6221541774272925\n",
            "Step=42499 D_loss=0.016803696602582935, G_loss=6.721048874855041\n",
            "Step=42599 D_loss=0.01779464662075042, G_loss=6.612423675060272\n",
            "Step=42699 D_loss=0.018287438377737997, G_loss=6.702301911711693\n",
            "Step=42799 D_loss=0.012979080379009245, G_loss=7.815688073933125\n",
            "Step=42899 D_loss=0.01373605847358704, G_loss=7.2619582322239875\n",
            "Step=42999 D_loss=0.012775341048836708, G_loss=6.954199881851673\n",
            "Step=43099 D_loss=0.01941371679306031, G_loss=6.758067867159843\n",
            "Step=43199 D_loss=0.02066705137491226, G_loss=6.216271668374539\n",
            "Step=43299 D_loss=0.01387583591043949, G_loss=6.676931942105293\n",
            "Step=43399 D_loss=0.0140789556503296, G_loss=6.618212114870548\n",
            "Step=43499 D_loss=0.016527695059776304, G_loss=6.46828797608614\n",
            "Step=43599 D_loss=0.01605642750859261, G_loss=6.403488684296608\n",
            "Step=43699 D_loss=0.021218299865722656, G_loss=6.301058422029018\n",
            "Step=43799 D_loss=0.02102904476225377, G_loss=6.184504772126674\n",
            "Step=43899 D_loss=0.017463591843843468, G_loss=6.259344207942486\n",
            "Step=43999 D_loss=0.016775769665837284, G_loss=6.436595912873744\n",
            "Step=44099 D_loss=0.012628315016627312, G_loss=6.908899486064911\n",
            "Step=44199 D_loss=0.016089773476123823, G_loss=6.55097588211298\n",
            "Step=44299 D_loss=0.015320808663964278, G_loss=6.429300639331341\n",
            "Step=44399 D_loss=0.017356608957052225, G_loss=6.53065102159977\n",
            "Step=44499 D_loss=0.01097386211156845, G_loss=6.612146449685096\n",
            "Step=44599 D_loss=0.01314336076378822, G_loss=6.648974351584911\n",
            "Step=44699 D_loss=0.013190065175294877, G_loss=6.657571801841259\n",
            "Step=44799 D_loss=0.015615356639027592, G_loss=6.540639698505402\n",
            "Step=44899 D_loss=0.015506910160183907, G_loss=6.571210145652294\n",
            "Step=44999 D_loss=0.015021480321884162, G_loss=6.5293222069740295\n",
            "Step=45099 D_loss=0.012940204069018371, G_loss=6.59960408627987\n",
            "Step=45199 D_loss=0.0131970027089119, G_loss=6.621117533445358\n",
            "Step=45299 D_loss=0.011491174995899203, G_loss=6.648309105038643\n",
            "Step=45399 D_loss=0.016180419102311133, G_loss=6.687531795799732\n",
            "Step=45499 D_loss=0.014249400720000277, G_loss=6.474889174997807\n",
            "Step=45599 D_loss=0.01585312433540821, G_loss=6.691924800276756\n",
            "Step=45699 D_loss=0.015296144038438791, G_loss=6.686019939780236\n",
            "Step=45799 D_loss=0.018518837913870823, G_loss=6.602697435617447\n",
            "Step=45899 D_loss=0.01806157775223255, G_loss=6.560515326261521\n",
            "Step=45999 D_loss=0.019695935845375073, G_loss=6.42187928557396\n",
            "Step=46099 D_loss=0.011180595532059662, G_loss=6.528851596713066\n",
            "Step=46199 D_loss=0.018676316738128654, G_loss=6.572536956965924\n",
            "Step=46299 D_loss=0.013239476159214975, G_loss=6.673541324138641\n",
            "Step=46399 D_loss=0.01501298576593399, G_loss=6.513258330523968\n",
            "Step=46499 D_loss=0.015606720820069322, G_loss=6.561524615287781\n",
            "Step=46599 D_loss=0.012212147563695916, G_loss=6.480910658538341\n",
            "Step=46699 D_loss=0.019785438999533653, G_loss=6.56627346277237\n",
            "Step=46799 D_loss=0.02127428501844407, G_loss=6.072236533761025\n",
            "Step=46899 D_loss=0.014785049930214886, G_loss=6.251399999856949\n",
            "Step=46999 D_loss=0.015616654679179193, G_loss=6.430800629556179\n",
            "Step=47099 D_loss=0.011176684275269508, G_loss=6.717342050075531\n",
            "Step=47199 D_loss=0.014670388400554651, G_loss=6.575935777127743\n",
            "Step=47299 D_loss=0.021758331134915362, G_loss=6.590790480971336\n",
            "Step=47399 D_loss=0.01660547085106373, G_loss=6.476042196750641\n",
            "Step=47499 D_loss=0.013388369828462601, G_loss=6.692047522068024\n",
            "Step=47599 D_loss=0.01734307207167149, G_loss=6.397187061905861\n",
            "Step=47699 D_loss=0.019957075491547585, G_loss=6.328938314914703\n",
            "Step=47799 D_loss=0.017670317813754083, G_loss=6.268137301206589\n",
            "Step=47899 D_loss=0.013145603612065307, G_loss=6.479993821680545\n",
            "Step=47999 D_loss=0.013239344507455827, G_loss=6.634086678922176\n",
            "Step=48099 D_loss=0.011080157235264773, G_loss=6.634706640541554\n",
            "Step=48199 D_loss=0.015244737043976775, G_loss=6.532838268280029\n",
            "Step=48299 D_loss=0.016301703900098802, G_loss=6.40211240708828\n",
            "Step=48399 D_loss=0.014922403991222383, G_loss=6.409661503136158\n",
            "Step=48499 D_loss=0.013407099619507784, G_loss=6.612181637585163\n",
            "Step=48599 D_loss=0.017018423229455945, G_loss=6.440338751971721\n",
            "Step=48699 D_loss=0.015666097775101667, G_loss=6.318803341984749\n",
            "Step=48799 D_loss=0.01478671863675117, G_loss=6.473092263042926\n",
            "Step=48899 D_loss=0.01477685913443566, G_loss=6.429040524959564\n",
            "Step=48999 D_loss=0.009687610194087037, G_loss=6.695107667446136\n",
            "Step=49099 D_loss=0.017392288595437996, G_loss=6.698039769232274\n",
            "Step=49199 D_loss=0.011863410957157608, G_loss=6.716769665777683\n",
            "Step=49299 D_loss=0.01235776156187058, G_loss=6.835939016640187\n",
            "Step=49399 D_loss=0.012280179038643833, G_loss=6.462606040239334\n",
            "Step=49499 D_loss=0.017044347561895844, G_loss=6.530092596411705\n",
            "Step=49599 D_loss=0.012892788685858247, G_loss=6.453404085934162\n",
            "Step=49699 D_loss=0.010710851885378361, G_loss=6.689797120392322\n",
            "Step=49799 D_loss=0.014977850504219542, G_loss=6.575139033794403\n",
            "Step=49899 D_loss=0.01286442905664445, G_loss=6.716567910611629\n",
            "Step=49999 D_loss=0.010989274866878979, G_loss=6.768978539109231\n",
            "Reached 50001 epochs for GAN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNGUJwBhj0pQlAGOMSVOWAIwxJk39P9i8sest1Lh9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3K2Iqevjl1i",
        "colab_type": "text"
      },
      "source": [
        "Above code is pretty self explantory. Steps are: \n",
        "\n",
        "1. create batches\n",
        "2. tcreate an object with the given historical data and batches\n",
        "3. Train this object\n",
        "\n",
        "Code Ref: https://www.kaggle.com/ashishpatel26/gan-beginner-tutorial-for-pytorch-celeba-dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFu0U0AlRMP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#modeling CNN\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "SEED = 42\n",
        "tf.set_random_seed(SEED)\n",
        "class CNN():\n",
        "\n",
        "    def __init__(self, num_features, num_historical_days, is_train=True):\n",
        "      \n",
        "        self.X = tf.placeholder(tf.float32, shape=[None, num_historical_days, num_features])\n",
        "        X = tf.reshape(self.X, [-1, num_historical_days, 1, num_features])\n",
        "        self.Y = tf.placeholder(tf.int32, shape=[None, 2])\n",
        "        self.keep_prob = tf.placeholder(tf.float32, shape=[])\n",
        "\n",
        "        with tf.variable_scope(\"cnn\"):\n",
        "            #[filter_height, filter_width, in_channels, out_channels]\n",
        "            k1 = tf.Variable(tf.truncated_normal([3, 1, num_features, 16],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b1 = tf.Variable(tf.zeros([16], dtype=tf.float32))\n",
        "\n",
        "            conv = tf.nn.conv2d(X,k1,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b1))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob = self.keep_prob)\n",
        "            print(relu)\n",
        "\n",
        "\n",
        "            k2 = tf.Variable(tf.truncated_normal([3, 1, 16, 32],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b2 = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
        "            conv = tf.nn.conv2d(relu, k2,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b2))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob = self.keep_prob)\n",
        "            print(relu)\n",
        "\n",
        "\n",
        "            k3 = tf.Variable(tf.truncated_normal([3, 1, 32, 64],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b3 = tf.Variable(tf.zeros([64], dtype=tf.float32))\n",
        "            conv = tf.nn.conv2d(relu, k3, strides=[1, 1, 1, 1], padding='VALID')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b3))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob=self.keep_prob)\n",
        "            print(relu)\n",
        "\n",
        "\n",
        "            flattened_convolution_size = int(relu.shape[1]) * int(relu.shape[2]) * int(relu.shape[3])\n",
        "            print(flattened_convolution_size)\n",
        "            flattened_convolution = features = tf.reshape(relu, [-1, flattened_convolution_size])\n",
        "\n",
        "            if is_train:\n",
        "                flattened_convolution =  tf.nn.dropout(flattened_convolution, keep_prob=self.keep_prob)\n",
        "\n",
        "            W1 = tf.Variable(tf.truncated_normal([18*1*64, 32]))\n",
        "            b4 = tf.Variable(tf.truncated_normal([32]))\n",
        "            h1 = tf.nn.relu(tf.matmul(flattened_convolution, W1) + b4)\n",
        "\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([32, 2]))\n",
        "            logits = tf.matmul(h1, W2)\n",
        "\n",
        "            #self.accuracy = tf.metrics.accuracy(tf.argmax(self.Y, 1), tf.argmax(logits, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.Y, 1), tf.argmax(logits, 1)), tf.float32))\n",
        "            self.confusion_matrix = tf.confusion_matrix(tf.argmax(self.Y, 1), tf.argmax(logits, 1))\n",
        "            tf.summary.scalar('accuracy', self.accuracy)\n",
        "            theta_D = [k1, b1, k2, b2, k3, b3, W1, b4, W2]           \n",
        "            \n",
        "            # D_prob = tf.nn.sigmoid(D_logit)\n",
        "\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=logits))\n",
        "        tf.summary.scalar('loss', self.loss)\n",
        "        # self.D_l2_loss = (0.0001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_D]) / len(theta_D))\n",
        "        # self.D_loss = D_loss_real + D_loss_fake + self.D_l2_loss\n",
        "        # self.G_l2_loss = (0.00001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_G]) / len(theta_G))\n",
        "        # self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) + self.G_l2_loss\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.loss)\n",
        "        self.summary = tf.summary.merge_all()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2qVIuzFkE5J",
        "colab_type": "text"
      },
      "source": [
        "Make the discriminator. \n",
        "Steps: \n",
        "1. Create a flattened convolution object \n",
        "\n",
        "2. Declare the weights and biases.\n",
        "\n",
        "3. make logits and then calculate accuracy and summary\n",
        "\n",
        "4. Use this as a discriminator model against generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSW0oCUNTZ6Z",
        "colab_type": "code",
        "outputId": "ec406265-f3ae-4957-99c3-19ef161f4483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#training CNN\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "#import xgboost as xgb\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "class TrainCNN:\n",
        "\n",
        "    def __init__(self, num_historical_days, days=10, pct_change=0):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.test_data = []\n",
        "        self.test_labels = []\n",
        "        self.cnn = CNN(num_features=5, num_historical_days=num_historical_days, is_train=False)\n",
        "#         files = [os.path.join('./stock_data', f) for f in os.listdir('./stock_data')]\n",
        "\n",
        "        # Google Drive Method\n",
        "        files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]\n",
        "#         print(files)\n",
        "    \n",
        "    \n",
        "        for file in files:\n",
        "            print(file)\n",
        "            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "            df = df[['open','high','low','close','volume']]\n",
        "            # data for new column labels that will use the pct_change of the closing data.\n",
        "            # pct_change measure change between current and prior element. Map these into a 1x2\n",
        "            # array to show if the pct_change > (our desired threshold) or less than.\n",
        "            labels = df.close.pct_change(days).map(lambda x: [int(x > pct_change/100.0), int(x <= pct_change/100.0)])\n",
        "            \n",
        "            # rolling normalization. (df - df.mean) / (df.max - df.min)\n",
        "            df = ((df -\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "            df['labels'] = labels\n",
        "\n",
        "            # doing pct_change will give some rows (like first row) a NaN value. Drop that.\n",
        "            df = df.dropna()\n",
        "\n",
        "            # Do the testing data split\n",
        "            test_df = df[:365]\n",
        "            df = df[400:]\n",
        "\n",
        "            # get the predictors of the dataframe\n",
        "            data = df[['open','high','low','close','volume']].values\n",
        "\n",
        "            # the response value\n",
        "            labels = df['labels'].values\n",
        "\n",
        "            # start at num_historical_days and iterate the full length of the training\n",
        "            # data at intervals of num_historical_days\n",
        "            for i in range(num_historical_days, len(df), num_historical_days):\n",
        "                # split the df into arrays of length num_historical_days and append\n",
        "                # to data, i.e. array of df[curr - num_days : curr] -> a batch of values\n",
        "                self.data.append(data[i-num_historical_days:i])\n",
        "\n",
        "                # appending if price went up or down in curr day of \"i\" we are looking\n",
        "                # at\n",
        "                self.labels.append(labels[i-1])\n",
        "            \n",
        "            # do same for test data\n",
        "            data = test_df[['open','high','low','close','volume']].values\n",
        "            labels = test_df['labels'].values\n",
        "            for i in range(num_historical_days, len(test_df), 1):\n",
        "                self.test_data.append(data[i-num_historical_days:i])\n",
        "                self.test_labels.append(labels[i-1])\n",
        "\n",
        "    # a function to get a random_batch of data.\n",
        "    def random_batch(self, batch_size=128):\n",
        "        batch = []\n",
        "        labels = []\n",
        "        # zip concatenates each array index of both arrays together\n",
        "        data = list(zip(self.data, self.labels))\n",
        "        i = 0\n",
        "        while True:\n",
        "            i+= 1\n",
        "            while True:\n",
        "                # pick a random array, i.e. range of days, from data\n",
        "                d = random.choice(data)\n",
        "                # balance the data with equal number of positive pct_change\n",
        "                # and negative pct_change\n",
        "                if(d[1][0]== int(i%2)):\n",
        "                    break\n",
        "            batch.append(d[0])  # append the range of days we got to batch\n",
        "            labels.append(d[1])  # append the label of that range of data we got\n",
        "            if (len(batch) == batch_size):\n",
        "                yield batch, labels\n",
        "                batch = []\n",
        "                labels = []\n",
        "\n",
        "    def train(self, print_steps=100, display_steps=100, save_steps=SAVE_STEPS_AMOUNT, batch_size=128, keep_prob=0.6):\n",
        "        if not os.path.exists(f'{googlepath}cnn_models'):\n",
        "            os.makedirs(f'{googlepath}cnn_models')\n",
        "        if not os.path.exists(f'{googlepath}logs'):\n",
        "            os.makedirs(f'{googlepath}logs')\n",
        "        if os.path.exists(f'{googlepath}logs/train'):\n",
        "            for file in [os.path.join(f'{googlepath}logs/train/', f) for f in os.listdir(f'{googlepath}logs/train/')]:\n",
        "                os.remove(file)\n",
        "        if os.path.exists(f'{googlepath}logs/test'):\n",
        "            for file in [os.path.join(f'{googlepath}logs/test/', f) for f in os.listdir(f'{googlepath}logs/test')]:\n",
        "                os.remove(file)\n",
        "\n",
        "        sess = tf.Session()\n",
        "        loss = 0\n",
        "        l2_loss = 0\n",
        "        accuracy = 0\n",
        "        saver = tf.train.Saver()\n",
        "        train_writer = tf.summary.FileWriter(f'{googlepath}/logs/train')\n",
        "        test_writer = tf.summary.FileWriter(f'{googlepath}/logs/test')\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        test_loss_array = []\n",
        "        test_accuracy_array = []\n",
        "        currentStep = \"0\"\n",
        "        \n",
        "        if os.path.exists(f'{googlepath}cnn_models/checkpoint'):\n",
        "                with open(f'{googlepath}cnn_models/checkpoint', 'rb') as f:\n",
        "                    model_name = next(f).split('\"'.encode())[1]\n",
        "                filename = \"{}cnn_models/{}\".format(googlepath, model_name.decode())\n",
        "                currentStep = filename.split(\"-\")[1]\n",
        "                new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\n",
        "                new_saver.restore(sess, \"{}\".format(filename))\n",
        "\n",
        "        for i, [X, y] in enumerate(self.random_batch(batch_size)):\n",
        "\n",
        "          \n",
        "            _, loss_curr, accuracy_curr = sess.run([self.cnn.optimizer, self.cnn.loss, self.cnn.accuracy], feed_dict=\n",
        "                    {self.cnn.X:X, self.cnn.Y:y, self.cnn.keep_prob:keep_prob})\n",
        "            loss += loss_curr\n",
        "            accuracy += accuracy_curr\n",
        "            if (i+1) % print_steps == 0:\n",
        "                print('Step={} loss={}, accuracy={}'.format(i + int(currentStep), loss/print_steps, accuracy/print_steps))\n",
        "                loss = 0\n",
        "                l2_loss = 0\n",
        "                accuracy = 0\n",
        "                test_loss, test_accuracy, confusion_matrix = sess.run([self.cnn.loss, self.cnn.accuracy, self.cnn.confusion_matrix], feed_dict={self.cnn.X:self.test_data, self.cnn.Y:self.test_labels, self.cnn.keep_prob:1})\n",
        "                test_loss_array.append(test_loss)\n",
        "                test_accuracy_array.append(test_accuracy)\n",
        "                print(\"Test loss = {}, Test accuracy = {}\".format(test_loss, test_accuracy))\n",
        "            if (i+1) % save_steps == 0:\n",
        "                saver.save(sess,  f'{googlepath}cnn_models/cnn.ckpt', i)\n",
        "\n",
        "            if (i+1) % display_steps == 0:\n",
        "                summary = sess.run(self.cnn.summary, feed_dict=\n",
        "                    {self.cnn.X:X, self.cnn.Y:y, self.cnn.keep_prob:keep_prob})\n",
        "                train_writer.add_summary(summary, i)\n",
        "                summary = sess.run(self.cnn.summary, feed_dict={\n",
        "                    self.cnn.X:self.test_data, self.cnn.Y:self.test_labels, self.cnn.keep_prob:1})\n",
        "                test_writer.add_summary(summary, i)\n",
        "            \n",
        "            # end training at training_amount epochs\n",
        "            if (i + int(currentStep)) > TRAINING_AMOUNT:\n",
        "                print(\"Reached {} epochs for CNN\".format(i + int(currentStep)))\n",
        "                sess.close()\n",
        "                print(confusion_matrix)\n",
        "                plot_confusion_matrix(confusion_matrix, ['Down', 'Up'], normalize=True, title=\"CNN Confusion Matrix\")\n",
        "                \n",
        "                axisA = np.arange(0,len(test_loss_array),1)\n",
        "                axisB = np.arange(0,len(test_accuracy_array),1)\n",
        "                plt.plot(axisA, test_loss_array, label='test accuracy')\n",
        "                plt.plot(axisB, test_accuracy_array, label='test loss')\n",
        "                plt.legend()\n",
        "                plt.title('test loss and accuracy')\n",
        "                plt.show()\n",
        "\n",
        "                break\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "tf.reset_default_graph()\n",
        "cnn = TrainCNN(num_historical_days=HISTORICAL_DAYS_AMOUNT, days=DAYS_AHEAD, pct_change=PCT_CHANGE_AMOUNT)\n",
        "cnn.train()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"cnn/Relu:0\", shape=(?, 20, 1, 16), dtype=float32)\n",
            "Tensor(\"cnn/Relu_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"cnn/Relu_2:0\", shape=(?, 18, 1, 64), dtype=float32)\n",
            "1152\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AMZN.csv\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AAPL.csv\n",
            "Step=99 loss=0.6455005130171776, accuracy=0.702890625\n",
            "Test loss = 0.5633534789085388, Test accuracy = 0.7318840622901917\n",
            "Step=199 loss=0.38534841775894163, accuracy=0.841796875\n",
            "Test loss = 0.4994240701198578, Test accuracy = 0.7739130258560181\n",
            "Step=299 loss=0.28029976472258566, accuracy=0.919453125\n",
            "Test loss = 0.4840500056743622, Test accuracy = 0.782608687877655\n",
            "Step=399 loss=0.2181575807929039, accuracy=0.942734375\n",
            "Test loss = 0.4898371696472168, Test accuracy = 0.7782608866691589\n",
            "Step=499 loss=0.16817410349845885, accuracy=0.95875\n",
            "Test loss = 0.5119732618331909, Test accuracy = 0.7884057760238647\n",
            "Step=599 loss=0.1318838482350111, accuracy=0.97625\n",
            "Test loss = 0.5282639265060425, Test accuracy = 0.7768115997314453\n",
            "Step=699 loss=0.10476633816957474, accuracy=0.984375\n",
            "Test loss = 0.5554772615432739, Test accuracy = 0.7681159377098083\n",
            "Step=799 loss=0.08198465671390295, accuracy=0.991171875\n",
            "Test loss = 0.5908352732658386, Test accuracy = 0.7681159377098083\n",
            "Step=899 loss=0.06370757568627596, accuracy=0.993984375\n",
            "Test loss = 0.6126183271408081, Test accuracy = 0.7666666507720947\n",
            "Step=999 loss=0.053149219676852225, accuracy=0.9946875\n",
            "Test loss = 0.6560640931129456, Test accuracy = 0.7652173638343811\n",
            "Step=1099 loss=0.04316837003454566, accuracy=0.996953125\n",
            "Test loss = 0.7098760008811951, Test accuracy = 0.7637681365013123\n",
            "Step=1199 loss=0.03471717918291688, accuracy=0.998203125\n",
            "Test loss = 0.7251536846160889, Test accuracy = 0.760869562625885\n",
            "Step=1299 loss=0.029514714051038025, accuracy=0.99890625\n",
            "Test loss = 0.7427104711532593, Test accuracy = 0.7652173638343811\n",
            "Step=1399 loss=0.02395401757210493, accuracy=1.0\n",
            "Test loss = 0.7950646281242371, Test accuracy = 0.7637681365013123\n",
            "Step=1499 loss=0.020126780355349184, accuracy=1.0\n",
            "Test loss = 0.8054652214050293, Test accuracy = 0.7579709887504578\n",
            "Step=1599 loss=0.01689181079156697, accuracy=1.0\n",
            "Test loss = 0.8656027317047119, Test accuracy = 0.7623188495635986\n",
            "Step=1699 loss=0.014489036658778787, accuracy=1.0\n",
            "Test loss = 0.8723316192626953, Test accuracy = 0.7594202756881714\n",
            "Step=1799 loss=0.012030724142678082, accuracy=1.0\n",
            "Test loss = 0.9114437103271484, Test accuracy = 0.7565217614173889\n",
            "Step=1899 loss=0.010295248357579112, accuracy=1.0\n",
            "Test loss = 0.934668242931366, Test accuracy = 0.7579709887504578\n",
            "Step=1999 loss=0.009224811932072044, accuracy=1.0\n",
            "Test loss = 0.995672881603241, Test accuracy = 0.760869562625885\n",
            "Step=2099 loss=0.007623512297868729, accuracy=1.0\n",
            "Test loss = 0.9776466488838196, Test accuracy = 0.7579709887504578\n",
            "Step=2199 loss=0.00687622198369354, accuracy=1.0\n",
            "Test loss = 1.0171468257904053, Test accuracy = 0.7594202756881714\n",
            "Step=2299 loss=0.006238719755783677, accuracy=1.0\n",
            "Test loss = 1.0402721166610718, Test accuracy = 0.7565217614173889\n",
            "Step=2399 loss=0.00533247207524255, accuracy=1.0\n",
            "Test loss = 1.0630614757537842, Test accuracy = 0.7565217614173889\n",
            "Step=2499 loss=0.004672179182525724, accuracy=1.0\n",
            "Test loss = 1.073714017868042, Test accuracy = 0.7536231875419617\n",
            "Step=2599 loss=0.004176417370326817, accuracy=1.0\n",
            "Test loss = 1.0960719585418701, Test accuracy = 0.7550724744796753\n",
            "Step=2699 loss=0.0037637158948928117, accuracy=1.0\n",
            "Test loss = 1.1246168613433838, Test accuracy = 0.7536231875419617\n",
            "Step=2799 loss=0.0033668747358024122, accuracy=1.0\n",
            "Test loss = 1.1570963859558105, Test accuracy = 0.7550724744796753\n",
            "Step=2899 loss=0.003052552400622517, accuracy=1.0\n",
            "Test loss = 1.178293228149414, Test accuracy = 0.7536231875419617\n",
            "Step=2999 loss=0.0027770366007462146, accuracy=1.0\n",
            "Test loss = 1.1862610578536987, Test accuracy = 0.752173900604248\n",
            "Step=3099 loss=0.0023982084076851606, accuracy=1.0\n",
            "Test loss = 1.1974427700042725, Test accuracy = 0.752173900604248\n",
            "Step=3199 loss=0.00230134088662453, accuracy=1.0\n",
            "Test loss = 1.2318204641342163, Test accuracy = 0.752173900604248\n",
            "Step=3299 loss=0.002009047253523022, accuracy=1.0\n",
            "Test loss = 1.2403141260147095, Test accuracy = 0.7507246136665344\n",
            "Step=3399 loss=0.0018324128526728601, accuracy=1.0\n",
            "Test loss = 1.2771998643875122, Test accuracy = 0.7507246136665344\n",
            "Step=3499 loss=0.0016740203532390296, accuracy=1.0\n",
            "Test loss = 1.288344383239746, Test accuracy = 0.7492753863334656\n",
            "Step=3599 loss=0.001488404672127217, accuracy=1.0\n",
            "Test loss = 1.326475739479065, Test accuracy = 0.7507246136665344\n",
            "Step=3699 loss=0.0013668786047492177, accuracy=1.0\n",
            "Test loss = 1.3329163789749146, Test accuracy = 0.752173900604248\n",
            "Step=3799 loss=0.0012860607472248375, accuracy=1.0\n",
            "Test loss = 1.3427609205245972, Test accuracy = 0.7507246136665344\n",
            "Step=3899 loss=0.001173119944287464, accuracy=1.0\n",
            "Test loss = 1.3774237632751465, Test accuracy = 0.752173900604248\n",
            "Step=3999 loss=0.0010717943822965027, accuracy=1.0\n",
            "Test loss = 1.3966646194458008, Test accuracy = 0.7536231875419617\n",
            "Step=4099 loss=0.0009482053417013958, accuracy=1.0\n",
            "Test loss = 1.4059785604476929, Test accuracy = 0.752173900604248\n",
            "Step=4199 loss=0.0009044862870359793, accuracy=1.0\n",
            "Test loss = 1.4172710180282593, Test accuracy = 0.7492753863334656\n",
            "Step=4299 loss=0.0008495339599903673, accuracy=1.0\n",
            "Test loss = 1.4441145658493042, Test accuracy = 0.747826099395752\n",
            "Step=4399 loss=0.0007612658990547061, accuracy=1.0\n",
            "Test loss = 1.454982042312622, Test accuracy = 0.747826099395752\n",
            "Step=4499 loss=0.0006944194735842757, accuracy=1.0\n",
            "Test loss = 1.4686086177825928, Test accuracy = 0.747826099395752\n",
            "Step=4599 loss=0.0006731017568381504, accuracy=1.0\n",
            "Test loss = 1.5000734329223633, Test accuracy = 0.7463768124580383\n",
            "Step=4699 loss=0.0006002894716220908, accuracy=1.0\n",
            "Test loss = 1.494809865951538, Test accuracy = 0.7463768124580383\n",
            "Step=4799 loss=0.0005739481301861815, accuracy=1.0\n",
            "Test loss = 1.5140224695205688, Test accuracy = 0.7463768124580383\n",
            "Step=4899 loss=0.0005197670310735702, accuracy=1.0\n",
            "Test loss = 1.5450067520141602, Test accuracy = 0.7420290112495422\n",
            "Step=4999 loss=0.00047513975732726976, accuracy=1.0\n",
            "Test loss = 1.5551984310150146, Test accuracy = 0.7449275255203247\n",
            "Step=5099 loss=0.0004432367006666027, accuracy=1.0\n",
            "Test loss = 1.5600734949111938, Test accuracy = 0.7449275255203247\n",
            "Step=5199 loss=0.0004202781443018466, accuracy=1.0\n",
            "Test loss = 1.6014375686645508, Test accuracy = 0.7420290112495422\n",
            "Step=5299 loss=0.00038840946392156185, accuracy=1.0\n",
            "Test loss = 1.5892130136489868, Test accuracy = 0.7434782385826111\n",
            "Step=5399 loss=0.0003692286994191818, accuracy=1.0\n",
            "Test loss = 1.6115411520004272, Test accuracy = 0.7434782385826111\n",
            "Step=5499 loss=0.00034028590220259503, accuracy=1.0\n",
            "Test loss = 1.630059003829956, Test accuracy = 0.7434782385826111\n",
            "Step=5599 loss=0.00031685155321611093, accuracy=1.0\n",
            "Test loss = 1.6463714838027954, Test accuracy = 0.7434782385826111\n",
            "Step=5699 loss=0.00028172115577035584, accuracy=1.0\n",
            "Test loss = 1.6595641374588013, Test accuracy = 0.7420290112495422\n",
            "Step=5799 loss=0.00026824295346159486, accuracy=1.0\n",
            "Test loss = 1.67706298828125, Test accuracy = 0.7420290112495422\n",
            "Step=5899 loss=0.00024462924819090403, accuracy=1.0\n",
            "Test loss = 1.6772830486297607, Test accuracy = 0.7449275255203247\n",
            "Step=5999 loss=0.0002374729182338342, accuracy=1.0\n",
            "Test loss = 1.7260812520980835, Test accuracy = 0.7405797243118286\n",
            "Step=6099 loss=0.00021731208078563214, accuracy=1.0\n",
            "Test loss = 1.7356977462768555, Test accuracy = 0.7405797243118286\n",
            "Step=6199 loss=0.00020920861097692978, accuracy=1.0\n",
            "Test loss = 1.7414748668670654, Test accuracy = 0.7405797243118286\n",
            "Step=6299 loss=0.00019230973368394188, accuracy=1.0\n",
            "Test loss = 1.7532926797866821, Test accuracy = 0.7405797243118286\n",
            "Step=6399 loss=0.0001773837226210162, accuracy=1.0\n",
            "Test loss = 1.7794904708862305, Test accuracy = 0.7420290112495422\n",
            "Step=6499 loss=0.0001720641873544082, accuracy=1.0\n",
            "Test loss = 1.7605462074279785, Test accuracy = 0.7434782385826111\n",
            "Step=6599 loss=0.00015833684003155213, accuracy=1.0\n",
            "Test loss = 1.7965834140777588, Test accuracy = 0.7405797243118286\n",
            "Step=6699 loss=0.00014623065995692742, accuracy=1.0\n",
            "Test loss = 1.8130075931549072, Test accuracy = 0.7420290112495422\n",
            "Step=6799 loss=0.00013419187249382957, accuracy=1.0\n",
            "Test loss = 1.83470618724823, Test accuracy = 0.7434782385826111\n",
            "Step=6899 loss=0.0001251046661491273, accuracy=1.0\n",
            "Test loss = 1.8584685325622559, Test accuracy = 0.7434782385826111\n",
            "Step=6999 loss=0.00011675868423481006, accuracy=1.0\n",
            "Test loss = 1.8458561897277832, Test accuracy = 0.7420290112495422\n",
            "Step=7099 loss=0.00011386659425625112, accuracy=1.0\n",
            "Test loss = 1.8799095153808594, Test accuracy = 0.7420290112495422\n",
            "Step=7199 loss=0.00010577966822893359, accuracy=1.0\n",
            "Test loss = 1.9052237272262573, Test accuracy = 0.7420290112495422\n",
            "Step=7299 loss=9.608548352844082e-05, accuracy=1.0\n",
            "Test loss = 1.8881547451019287, Test accuracy = 0.7405797243118286\n",
            "Step=7399 loss=8.809736391413026e-05, accuracy=1.0\n",
            "Test loss = 1.9171779155731201, Test accuracy = 0.7420290112495422\n",
            "Step=7499 loss=8.759146447118838e-05, accuracy=1.0\n",
            "Test loss = 1.9388947486877441, Test accuracy = 0.7420290112495422\n",
            "Step=7599 loss=7.735299830528674e-05, accuracy=1.0\n",
            "Test loss = 1.959372878074646, Test accuracy = 0.7420290112495422\n",
            "Step=7699 loss=7.647127771633678e-05, accuracy=1.0\n",
            "Test loss = 1.9546908140182495, Test accuracy = 0.7420290112495422\n",
            "Step=7799 loss=7.055563841277035e-05, accuracy=1.0\n",
            "Test loss = 1.97428560256958, Test accuracy = 0.7434782385826111\n",
            "Step=7899 loss=6.496139747468987e-05, accuracy=1.0\n",
            "Test loss = 1.9913990497589111, Test accuracy = 0.7434782385826111\n",
            "Step=7999 loss=6.118571942351991e-05, accuracy=1.0\n",
            "Test loss = 2.0152642726898193, Test accuracy = 0.7434782385826111\n",
            "Step=8099 loss=5.955694086878793e-05, accuracy=1.0\n",
            "Test loss = 2.0249290466308594, Test accuracy = 0.7434782385826111\n",
            "Step=8199 loss=5.3291604235710113e-05, accuracy=1.0\n",
            "Test loss = 2.0377357006073, Test accuracy = 0.7434782385826111\n",
            "Step=8299 loss=4.930827893986134e-05, accuracy=1.0\n",
            "Test loss = 2.0533204078674316, Test accuracy = 0.7434782385826111\n",
            "Step=8399 loss=4.814094954781467e-05, accuracy=1.0\n",
            "Test loss = 2.0663301944732666, Test accuracy = 0.7434782385826111\n",
            "Step=8499 loss=4.345009296230273e-05, accuracy=1.0\n",
            "Test loss = 2.082120180130005, Test accuracy = 0.7434782385826111\n",
            "Step=8599 loss=4.393060225993395e-05, accuracy=1.0\n",
            "Test loss = 2.096813201904297, Test accuracy = 0.7420290112495422\n",
            "Step=8699 loss=3.931965993615449e-05, accuracy=1.0\n",
            "Test loss = 2.089421033859253, Test accuracy = 0.7434782385826111\n",
            "Step=8799 loss=3.7439632342284314e-05, accuracy=1.0\n",
            "Test loss = 2.111463785171509, Test accuracy = 0.7434782385826111\n",
            "Step=8899 loss=3.601251932195737e-05, accuracy=1.0\n",
            "Test loss = 2.1339540481567383, Test accuracy = 0.7420290112495422\n",
            "Step=8999 loss=3.244342122343369e-05, accuracy=1.0\n",
            "Test loss = 2.1589953899383545, Test accuracy = 0.7420290112495422\n",
            "Step=9099 loss=3.0682506494486e-05, accuracy=1.0\n",
            "Test loss = 2.1643097400665283, Test accuracy = 0.7420290112495422\n",
            "Step=9199 loss=2.9076252449158346e-05, accuracy=1.0\n",
            "Test loss = 2.162238597869873, Test accuracy = 0.7420290112495422\n",
            "Step=9299 loss=2.7499958359840094e-05, accuracy=1.0\n",
            "Test loss = 2.1879866123199463, Test accuracy = 0.7420290112495422\n",
            "Step=9399 loss=2.587783517810749e-05, accuracy=1.0\n",
            "Test loss = 2.2140636444091797, Test accuracy = 0.7420290112495422\n",
            "Step=9499 loss=2.4264521925942972e-05, accuracy=1.0\n",
            "Test loss = 2.2203757762908936, Test accuracy = 0.7420290112495422\n",
            "Step=9599 loss=2.2637555366600283e-05, accuracy=1.0\n",
            "Test loss = 2.224438190460205, Test accuracy = 0.7420290112495422\n",
            "Step=9699 loss=2.119722728821216e-05, accuracy=1.0\n",
            "Test loss = 2.241741418838501, Test accuracy = 0.7420290112495422\n",
            "Step=9799 loss=2.0683036891568917e-05, accuracy=1.0\n",
            "Test loss = 2.2460215091705322, Test accuracy = 0.7420290112495422\n",
            "Step=9899 loss=1.9144765938108323e-05, accuracy=1.0\n",
            "Test loss = 2.2567896842956543, Test accuracy = 0.7420290112495422\n",
            "Step=9999 loss=1.7768654533938388e-05, accuracy=1.0\n",
            "Test loss = 2.2692184448242188, Test accuracy = 0.7420290112495422\n",
            "Step=10099 loss=1.6431240046586025e-05, accuracy=1.0\n",
            "Test loss = 2.290102005004883, Test accuracy = 0.7420290112495422\n",
            "Step=10199 loss=1.5787431939315864e-05, accuracy=1.0\n",
            "Test loss = 2.2979471683502197, Test accuracy = 0.7420290112495422\n",
            "Step=10299 loss=1.4959151085349732e-05, accuracy=1.0\n",
            "Test loss = 2.313241958618164, Test accuracy = 0.7420290112495422\n",
            "Step=10399 loss=1.3966142842036788e-05, accuracy=1.0\n",
            "Test loss = 2.328423023223877, Test accuracy = 0.7420290112495422\n",
            "Step=10499 loss=1.3122152859068592e-05, accuracy=1.0\n",
            "Test loss = 2.3296751976013184, Test accuracy = 0.7420290112495422\n",
            "Step=10599 loss=1.2521056514742668e-05, accuracy=1.0\n",
            "Test loss = 2.3493690490722656, Test accuracy = 0.7420290112495422\n",
            "Step=10699 loss=1.192994607208675e-05, accuracy=1.0\n",
            "Test loss = 2.3667445182800293, Test accuracy = 0.7420290112495422\n",
            "Step=10799 loss=1.1303631035843864e-05, accuracy=1.0\n",
            "Test loss = 2.3787999153137207, Test accuracy = 0.7420290112495422\n",
            "Step=10899 loss=1.0344936886212963e-05, accuracy=1.0\n",
            "Test loss = 2.3721988201141357, Test accuracy = 0.7420290112495422\n",
            "Step=10999 loss=9.895255216179066e-06, accuracy=1.0\n",
            "Test loss = 2.3966126441955566, Test accuracy = 0.7420290112495422\n",
            "Step=11099 loss=9.304628288191451e-06, accuracy=1.0\n",
            "Test loss = 2.413039207458496, Test accuracy = 0.7420290112495422\n",
            "Step=11199 loss=8.642730613246385e-06, accuracy=1.0\n",
            "Test loss = 2.4201595783233643, Test accuracy = 0.7420290112495422\n",
            "Step=11299 loss=8.270050657301908e-06, accuracy=1.0\n",
            "Test loss = 2.4321718215942383, Test accuracy = 0.7420290112495422\n",
            "Step=11399 loss=7.620891447004396e-06, accuracy=1.0\n",
            "Test loss = 2.464155673980713, Test accuracy = 0.7420290112495422\n",
            "Step=11499 loss=7.406509944303252e-06, accuracy=1.0\n",
            "Test loss = 2.478423595428467, Test accuracy = 0.7420290112495422\n",
            "Step=11599 loss=6.900997068441938e-06, accuracy=1.0\n",
            "Test loss = 2.460021734237671, Test accuracy = 0.7420290112495422\n",
            "Step=11699 loss=6.189346631799708e-06, accuracy=1.0\n",
            "Test loss = 2.483280658721924, Test accuracy = 0.7420290112495422\n",
            "Step=11799 loss=6.03757252520154e-06, accuracy=1.0\n",
            "Test loss = 2.5194950103759766, Test accuracy = 0.7420290112495422\n",
            "Step=11899 loss=5.673383318480774e-06, accuracy=1.0\n",
            "Test loss = 2.5252280235290527, Test accuracy = 0.7405797243118286\n",
            "Step=11999 loss=5.308286247327487e-06, accuracy=1.0\n",
            "Test loss = 2.5435330867767334, Test accuracy = 0.7405797243118286\n",
            "Step=12099 loss=5.073820802863338e-06, accuracy=1.0\n",
            "Test loss = 2.5501794815063477, Test accuracy = 0.7405797243118286\n",
            "Step=12199 loss=4.724118020931201e-06, accuracy=1.0\n",
            "Test loss = 2.5685126781463623, Test accuracy = 0.7405797243118286\n",
            "Step=12299 loss=4.361366156899749e-06, accuracy=1.0\n",
            "Test loss = 2.589716672897339, Test accuracy = 0.7376811504364014\n",
            "Step=12399 loss=4.231990358221083e-06, accuracy=1.0\n",
            "Test loss = 2.591461658477783, Test accuracy = 0.739130437374115\n",
            "Step=12499 loss=4.038501670038386e-06, accuracy=1.0\n",
            "Test loss = 2.6019339561462402, Test accuracy = 0.7420290112495422\n",
            "Step=12599 loss=3.7397016171780707e-06, accuracy=1.0\n",
            "Test loss = 2.6245195865631104, Test accuracy = 0.7376811504364014\n",
            "Step=12699 loss=3.521786493365653e-06, accuracy=1.0\n",
            "Test loss = 2.6247763633728027, Test accuracy = 0.7405797243118286\n",
            "Step=12799 loss=3.3478453360658023e-06, accuracy=1.0\n",
            "Test loss = 2.6407535076141357, Test accuracy = 0.739130437374115\n",
            "Step=12899 loss=3.110938978352351e-06, accuracy=1.0\n",
            "Test loss = 2.6339855194091797, Test accuracy = 0.7405797243118286\n",
            "Step=12999 loss=2.8881868536245748e-06, accuracy=1.0\n",
            "Test loss = 2.6640071868896484, Test accuracy = 0.739130437374115\n",
            "Step=13099 loss=2.7726869939215247e-06, accuracy=1.0\n",
            "Test loss = 2.6739306449890137, Test accuracy = 0.7405797243118286\n",
            "Step=13199 loss=2.613377060924904e-06, accuracy=1.0\n",
            "Test loss = 2.6659095287323, Test accuracy = 0.7405797243118286\n",
            "Step=13299 loss=2.4789791757484636e-06, accuracy=1.0\n",
            "Test loss = 2.690565824508667, Test accuracy = 0.7405797243118286\n",
            "Step=13399 loss=2.3645208921152515e-06, accuracy=1.0\n",
            "Test loss = 2.701817274093628, Test accuracy = 0.739130437374115\n",
            "Step=13499 loss=2.1818719869770576e-06, accuracy=1.0\n",
            "Test loss = 2.7407360076904297, Test accuracy = 0.7362318634986877\n",
            "Step=13599 loss=2.11819851870132e-06, accuracy=1.0\n",
            "Test loss = 2.7602055072784424, Test accuracy = 0.7347826361656189\n",
            "Step=13699 loss=1.9484852316509206e-06, accuracy=1.0\n",
            "Test loss = 2.7516746520996094, Test accuracy = 0.739130437374115\n",
            "Step=13799 loss=1.8983711129294534e-06, accuracy=1.0\n",
            "Test loss = 2.7759323120117188, Test accuracy = 0.7362318634986877\n",
            "Step=13899 loss=1.7548088953844853e-06, accuracy=1.0\n",
            "Test loss = 2.777893304824829, Test accuracy = 0.739130437374115\n",
            "Step=13999 loss=1.6930723995756125e-06, accuracy=1.0\n",
            "Test loss = 2.794534206390381, Test accuracy = 0.7362318634986877\n",
            "Step=14099 loss=1.5701480515417643e-06, accuracy=1.0\n",
            "Test loss = 2.805933952331543, Test accuracy = 0.7376811504364014\n",
            "Step=14199 loss=1.4977382568304164e-06, accuracy=1.0\n",
            "Test loss = 2.819359302520752, Test accuracy = 0.7376811504364014\n",
            "Step=14299 loss=1.3901244926728395e-06, accuracy=1.0\n",
            "Test loss = 2.812837839126587, Test accuracy = 0.739130437374115\n",
            "Step=14399 loss=1.3255937165013166e-06, accuracy=1.0\n",
            "Test loss = 2.846116542816162, Test accuracy = 0.7376811504364014\n",
            "Step=14499 loss=1.2628975440520663e-06, accuracy=1.0\n",
            "Test loss = 2.8489644527435303, Test accuracy = 0.7376811504364014\n",
            "Step=14599 loss=1.1525736630346727e-06, accuracy=1.0\n",
            "Test loss = 2.8597710132598877, Test accuracy = 0.7376811504364014\n",
            "Step=14699 loss=1.121802975490027e-06, accuracy=1.0\n",
            "Test loss = 2.8702805042266846, Test accuracy = 0.739130437374115\n",
            "Step=14799 loss=1.0524476761020196e-06, accuracy=1.0\n",
            "Test loss = 2.888728141784668, Test accuracy = 0.7376811504364014\n",
            "Step=14899 loss=9.871530880900537e-07, accuracy=1.0\n",
            "Test loss = 2.9148027896881104, Test accuracy = 0.7376811504364014\n",
            "Step=14999 loss=9.398234112723003e-07, accuracy=1.0\n",
            "Test loss = 2.925208806991577, Test accuracy = 0.7376811504364014\n",
            "Step=15099 loss=9.31860739115109e-07, accuracy=1.0\n",
            "Test loss = 2.9603214263916016, Test accuracy = 0.7333333492279053\n",
            "Step=15199 loss=8.369965621568554e-07, accuracy=1.0\n",
            "Test loss = 2.9332127571105957, Test accuracy = 0.7376811504364014\n",
            "Step=15299 loss=7.864072961183411e-07, accuracy=1.0\n",
            "Test loss = 2.9520490169525146, Test accuracy = 0.7376811504364014\n",
            "Step=15399 loss=7.304070499003502e-07, accuracy=1.0\n",
            "Test loss = 2.9804773330688477, Test accuracy = 0.7362318634986877\n",
            "Step=15499 loss=7.027003164239431e-07, accuracy=1.0\n",
            "Test loss = 3.000761032104492, Test accuracy = 0.7347826361656189\n",
            "Step=15599 loss=6.827328542158284e-07, accuracy=1.0\n",
            "Test loss = 3.015949010848999, Test accuracy = 0.7347826361656189\n",
            "Step=15699 loss=6.351051760589143e-07, accuracy=1.0\n",
            "Test loss = 3.010493755340576, Test accuracy = 0.7362318634986877\n",
            "Step=15799 loss=5.703970325043883e-07, accuracy=1.0\n",
            "Test loss = 3.0036399364471436, Test accuracy = 0.7376811504364014\n",
            "Step=15899 loss=5.499825056176633e-07, accuracy=1.0\n",
            "Test loss = 3.029167652130127, Test accuracy = 0.7376811504364014\n",
            "Step=15999 loss=5.263549189749028e-07, accuracy=1.0\n",
            "Test loss = 3.0467355251312256, Test accuracy = 0.7347826361656189\n",
            "Step=16099 loss=4.887947434895068e-07, accuracy=1.0\n",
            "Test loss = 3.0690536499023438, Test accuracy = 0.7347826361656189\n",
            "Step=16199 loss=4.5634752893874974e-07, accuracy=1.0\n",
            "Test loss = 3.0762040615081787, Test accuracy = 0.7347826361656189\n",
            "Step=16299 loss=4.5545348285713773e-07, accuracy=1.0\n",
            "Test loss = 3.0857903957366943, Test accuracy = 0.7347826361656189\n",
            "Step=16399 loss=4.083472728666493e-07, accuracy=1.0\n",
            "Test loss = 3.1079599857330322, Test accuracy = 0.7347826361656189\n",
            "Step=16499 loss=4.0378381584105226e-07, accuracy=1.0\n",
            "Test loss = 3.104696035385132, Test accuracy = 0.7362318634986877\n",
            "Step=16599 loss=3.693621917477685e-07, accuracy=1.0\n",
            "Test loss = 3.1263046264648438, Test accuracy = 0.7347826361656189\n",
            "Step=16699 loss=3.4405820102278996e-07, accuracy=1.0\n",
            "Test loss = 3.129976511001587, Test accuracy = 0.7347826361656189\n",
            "Step=16799 loss=3.308706990878818e-07, accuracy=1.0\n",
            "Test loss = 3.1447668075561523, Test accuracy = 0.7347826361656189\n",
            "Step=16899 loss=3.0730827973002304e-07, accuracy=1.0\n",
            "Test loss = 3.152613401412964, Test accuracy = 0.7362318634986877\n",
            "Step=16999 loss=2.952010963497287e-07, accuracy=1.0\n",
            "Test loss = 3.1601803302764893, Test accuracy = 0.7362318634986877\n",
            "Step=17099 loss=2.7580168151075666e-07, accuracy=1.0\n",
            "Test loss = 3.185866117477417, Test accuracy = 0.7362318634986877\n",
            "Step=17199 loss=2.6931037723443295e-07, accuracy=1.0\n",
            "Test loss = 3.206892251968384, Test accuracy = 0.7362318634986877\n",
            "Step=17299 loss=2.544930477199614e-07, accuracy=1.0\n",
            "Test loss = 3.201514482498169, Test accuracy = 0.7362318634986877\n",
            "Step=17399 loss=2.401693190279275e-07, accuracy=1.0\n",
            "Test loss = 3.2133870124816895, Test accuracy = 0.7362318634986877\n",
            "Step=17499 loss=2.184881627442792e-07, accuracy=1.0\n",
            "Test loss = 3.2355451583862305, Test accuracy = 0.7362318634986877\n",
            "Step=17599 loss=2.0951021838300222e-07, accuracy=1.0\n",
            "Test loss = 3.247248649597168, Test accuracy = 0.7362318634986877\n",
            "Step=17699 loss=2.013704693837326e-07, accuracy=1.0\n",
            "Test loss = 3.2599828243255615, Test accuracy = 0.7362318634986877\n",
            "Step=17799 loss=1.8906770776538906e-07, accuracy=1.0\n",
            "Test loss = 3.277385950088501, Test accuracy = 0.7362318634986877\n",
            "Step=17899 loss=1.8555663174879556e-07, accuracy=1.0\n",
            "Test loss = 3.287567138671875, Test accuracy = 0.7362318634986877\n",
            "Step=17999 loss=1.7285339971806478e-07, accuracy=1.0\n",
            "Test loss = 3.286471128463745, Test accuracy = 0.7376811504364014\n",
            "Step=18099 loss=1.607927832480982e-07, accuracy=1.0\n",
            "Test loss = 3.3084635734558105, Test accuracy = 0.7347826361656189\n",
            "Step=18199 loss=1.499894498380172e-07, accuracy=1.0\n",
            "Test loss = 3.3236513137817383, Test accuracy = 0.7362318634986877\n",
            "Step=18299 loss=1.4396379448555764e-07, accuracy=1.0\n",
            "Test loss = 3.331388235092163, Test accuracy = 0.7362318634986877\n",
            "Step=18399 loss=1.3470645754409816e-07, accuracy=1.0\n",
            "Test loss = 3.3393394947052, Test accuracy = 0.7362318634986877\n",
            "Step=18499 loss=1.3086941088147342e-07, accuracy=1.0\n",
            "Test loss = 3.3490355014801025, Test accuracy = 0.7362318634986877\n",
            "Step=18599 loss=1.2390312008392358e-07, accuracy=1.0\n",
            "Test loss = 3.3685874938964844, Test accuracy = 0.7362318634986877\n",
            "Step=18699 loss=1.1320222945698787e-07, accuracy=1.0\n",
            "Test loss = 3.3804023265838623, Test accuracy = 0.7362318634986877\n",
            "Step=18799 loss=1.1135821367247445e-07, accuracy=1.0\n",
            "Test loss = 3.4043421745300293, Test accuracy = 0.7362318634986877\n",
            "Step=18899 loss=1.001450933557635e-07, accuracy=1.0\n",
            "Test loss = 3.3975942134857178, Test accuracy = 0.7376811504364014\n",
            "Step=18999 loss=9.944660053662347e-08, accuracy=1.0\n",
            "Test loss = 3.4208507537841797, Test accuracy = 0.7362318634986877\n",
            "Step=19099 loss=9.42311979912347e-08, accuracy=1.0\n",
            "Test loss = 3.4288294315338135, Test accuracy = 0.7362318634986877\n",
            "Step=19199 loss=8.90809858944408e-08, accuracy=1.0\n",
            "Test loss = 3.4391441345214844, Test accuracy = 0.7362318634986877\n",
            "Step=19299 loss=8.21146945639839e-08, accuracy=1.0\n",
            "Test loss = 3.460885524749756, Test accuracy = 0.7362318634986877\n",
            "Step=19399 loss=7.603316014126449e-08, accuracy=1.0\n",
            "Test loss = 3.4545466899871826, Test accuracy = 0.7362318634986877\n",
            "Step=19499 loss=6.981192711918993e-08, accuracy=1.0\n",
            "Test loss = 3.4694464206695557, Test accuracy = 0.7376811504364014\n",
            "Step=19599 loss=6.4922484988017e-08, accuracy=1.0\n",
            "Test loss = 3.500915050506592, Test accuracy = 0.7376811504364014\n",
            "Step=19699 loss=6.14114000896393e-08, accuracy=1.0\n",
            "Test loss = 3.5072202682495117, Test accuracy = 0.739130437374115\n",
            "Step=19799 loss=5.805863910524067e-08, accuracy=1.0\n",
            "Test loss = 3.551452875137329, Test accuracy = 0.7376811504364014\n",
            "Step=19899 loss=5.455686739708199e-08, accuracy=1.0\n",
            "Test loss = 3.5360794067382812, Test accuracy = 0.739130437374115\n",
            "Step=19999 loss=5.314125736788355e-08, accuracy=1.0\n",
            "Test loss = 3.566746711730957, Test accuracy = 0.739130437374115\n",
            "Step=20099 loss=5.022621911265901e-08, accuracy=1.0\n",
            "Test loss = 3.576843738555908, Test accuracy = 0.739130437374115\n",
            "Step=20199 loss=4.5616172776874464e-08, accuracy=1.0\n",
            "Test loss = 3.5807416439056396, Test accuracy = 0.739130437374115\n",
            "Step=20299 loss=4.2729073541636355e-08, accuracy=1.0\n",
            "Test loss = 3.612828016281128, Test accuracy = 0.7376811504364014\n",
            "Step=20399 loss=4.0875741582624415e-08, accuracy=1.0\n",
            "Test loss = 3.622776508331299, Test accuracy = 0.7376811504364014\n",
            "Step=20499 loss=3.939493947058281e-08, accuracy=1.0\n",
            "Test loss = 3.6470236778259277, Test accuracy = 0.7362318634986877\n",
            "Step=20599 loss=3.9422878632677796e-08, accuracy=1.0\n",
            "Test loss = 3.6657655239105225, Test accuracy = 0.7362318634986877\n",
            "Step=20699 loss=3.752298120218711e-08, accuracy=1.0\n",
            "Test loss = 3.6774699687957764, Test accuracy = 0.7362318634986877\n",
            "Step=20799 loss=3.45054968242664e-08, accuracy=1.0\n",
            "Test loss = 3.693948745727539, Test accuracy = 0.7347826361656189\n",
            "Step=20899 loss=3.506429012034573e-08, accuracy=1.0\n",
            "Test loss = 3.712961435317993, Test accuracy = 0.7347826361656189\n",
            "Step=20999 loss=3.213993805317727e-08, accuracy=1.0\n",
            "Test loss = 3.7235546112060547, Test accuracy = 0.7333333492279053\n",
            "Step=21099 loss=3.062188230984475e-08, accuracy=1.0\n",
            "Test loss = 3.7471704483032227, Test accuracy = 0.7333333492279053\n",
            "Step=21199 loss=3.1115483292154524e-08, accuracy=1.0\n",
            "Test loss = 3.7637217044830322, Test accuracy = 0.7333333492279053\n",
            "Step=21299 loss=2.7995552849269244e-08, accuracy=1.0\n",
            "Test loss = 3.7848877906799316, Test accuracy = 0.7333333492279053\n",
            "Step=21399 loss=2.7008351439761214e-08, accuracy=1.0\n",
            "Test loss = 3.819643497467041, Test accuracy = 0.7318840622901917\n",
            "Step=21499 loss=2.561136756895621e-08, accuracy=1.0\n",
            "Test loss = 3.8363664150238037, Test accuracy = 0.7318840622901917\n",
            "Step=21599 loss=2.4316829372317273e-08, accuracy=1.0\n",
            "Test loss = 3.8578896522521973, Test accuracy = 0.7318840622901917\n",
            "Step=21699 loss=2.3087483818073905e-08, accuracy=1.0\n",
            "Test loss = 3.8628082275390625, Test accuracy = 0.7333333492279053\n",
            "Step=21799 loss=2.1681186903599327e-08, accuracy=1.0\n",
            "Test loss = 3.889359951019287, Test accuracy = 0.7333333492279053\n",
            "Step=21899 loss=2.222135401019898e-08, accuracy=1.0\n",
            "Test loss = 3.917635202407837, Test accuracy = 0.7333333492279053\n",
            "Step=21999 loss=2.0926815862409854e-08, accuracy=1.0\n",
            "Test loss = 3.9446804523468018, Test accuracy = 0.7347826361656189\n",
            "Step=22099 loss=2.0060685903544596e-08, accuracy=1.0\n",
            "Test loss = 3.9577884674072266, Test accuracy = 0.7347826361656189\n",
            "Step=22199 loss=2.1308658051388817e-08, accuracy=1.0\n",
            "Test loss = 4.013298034667969, Test accuracy = 0.7333333492279053\n",
            "Step=22299 loss=1.8775460981501623e-08, accuracy=1.0\n",
            "Test loss = 4.03228759765625, Test accuracy = 0.7333333492279053\n",
            "Step=22399 loss=1.8635762666363575e-08, accuracy=1.0\n",
            "Test loss = 4.06177282333374, Test accuracy = 0.7347826361656189\n",
            "Step=22499 loss=1.8561256953653073e-08, accuracy=1.0\n",
            "Test loss = 4.089627742767334, Test accuracy = 0.7362318634986877\n",
            "Step=22599 loss=1.902691815658386e-08, accuracy=1.0\n",
            "Test loss = 4.132661819458008, Test accuracy = 0.7333333492279053\n",
            "Step=22699 loss=1.7564741918718595e-08, accuracy=1.0\n",
            "Test loss = 4.167755126953125, Test accuracy = 0.7318840622901917\n",
            "Step=22799 loss=1.728534505751611e-08, accuracy=1.0\n",
            "Test loss = 4.179065704345703, Test accuracy = 0.7362318634986877\n",
            "Step=22899 loss=1.73412244341975e-08, accuracy=1.0\n",
            "Test loss = 4.234713077545166, Test accuracy = 0.7347826361656189\n",
            "Step=22999 loss=1.6950069077203978e-08, accuracy=1.0\n",
            "Test loss = 4.2806525230407715, Test accuracy = 0.7347826361656189\n",
            "Step=23099 loss=1.702457482544162e-08, accuracy=1.0\n",
            "Test loss = 4.266643524169922, Test accuracy = 0.739130437374115\n",
            "Step=23199 loss=1.659616648552742e-08, accuracy=1.0\n",
            "Test loss = 4.355361461639404, Test accuracy = 0.7362318634986877\n",
            "Step=23299 loss=1.7127020357499134e-08, accuracy=1.0\n",
            "Test loss = 4.368259906768799, Test accuracy = 0.7347826361656189\n",
            "Step=23399 loss=1.681037061551649e-08, accuracy=1.0\n",
            "Test loss = 4.459596157073975, Test accuracy = 0.7333333492279053\n",
            "Step=23499 loss=1.5525145755646008e-08, accuracy=1.0\n",
            "Test loss = 4.526652812957764, Test accuracy = 0.7333333492279053\n",
            "Step=23599 loss=1.5003605100538665e-08, accuracy=1.0\n",
            "Test loss = 4.557858943939209, Test accuracy = 0.7362318634986877\n",
            "Step=23699 loss=1.627020370875698e-08, accuracy=1.0\n",
            "Test loss = 4.586092948913574, Test accuracy = 0.7362318634986877\n",
            "Step=23799 loss=1.5608964774038724e-08, accuracy=1.0\n",
            "Test loss = 4.672569751739502, Test accuracy = 0.7362318634986877\n",
            "Step=23899 loss=1.5255062149055475e-08, accuracy=1.0\n",
            "Test loss = 4.751363754272461, Test accuracy = 0.7362318634986877\n",
            "Step=23999 loss=1.463107611732184e-08, accuracy=1.0\n",
            "Test loss = 4.795843124389648, Test accuracy = 0.739130437374115\n",
            "Step=24099 loss=1.4957038783069977e-08, accuracy=1.0\n",
            "Test loss = 4.876631259918213, Test accuracy = 0.739130437374115\n",
            "Step=24199 loss=1.4733521598309097e-08, accuracy=1.0\n",
            "Test loss = 5.008546352386475, Test accuracy = 0.739130437374115\n",
            "Step=24299 loss=1.3653187446172056e-08, accuracy=1.0\n",
            "Test loss = 5.091900825500488, Test accuracy = 0.7376811504364014\n",
            "Step=24399 loss=1.4109535393380669e-08, accuracy=1.0\n",
            "Test loss = 5.224099636077881, Test accuracy = 0.7376811504364014\n",
            "Step=24499 loss=1.4994291590575414e-08, accuracy=1.0\n",
            "Test loss = 5.278324127197266, Test accuracy = 0.739130437374115\n",
            "Step=24599 loss=1.5888361197813337e-08, accuracy=1.0\n",
            "Test loss = 5.464596271514893, Test accuracy = 0.7376811504364014\n",
            "Step=24699 loss=1.4221293935801072e-08, accuracy=1.0\n",
            "Test loss = 5.5563459396362305, Test accuracy = 0.7405797243118286\n",
            "Step=24799 loss=1.6177070911727043e-08, accuracy=1.0\n",
            "Test loss = 5.702450752258301, Test accuracy = 0.7420290112495422\n",
            "Step=24899 loss=1.61584440294682e-08, accuracy=1.0\n",
            "Test loss = 5.861598968505859, Test accuracy = 0.7420290112495422\n",
            "Step=24999 loss=1.5925612690814716e-08, accuracy=1.0\n",
            "Test loss = 6.027153968811035, Test accuracy = 0.7420290112495422\n",
            "Step=25099 loss=1.707113967963636e-08, accuracy=1.0\n",
            "Test loss = 6.22492790222168, Test accuracy = 0.7449275255203247\n",
            "Step=25199 loss=0.00019801973545651385, accuracy=1.0\n",
            "Test loss = 6.6022725105285645, Test accuracy = 0.7376811504364014\n",
            "Step=25299 loss=6.929128193462475e-06, accuracy=1.0\n",
            "Test loss = 6.693077564239502, Test accuracy = 0.739130437374115\n",
            "Step=25399 loss=1.4296148776793415e-06, accuracy=1.0\n",
            "Test loss = 6.656167984008789, Test accuracy = 0.739130437374115\n",
            "Step=25499 loss=1.0574533068563597e-06, accuracy=1.0\n",
            "Test loss = 6.648642539978027, Test accuracy = 0.739130437374115\n",
            "Step=25599 loss=7.992447226001787e-07, accuracy=1.0\n",
            "Test loss = 6.617820739746094, Test accuracy = 0.739130437374115\n",
            "Step=25699 loss=6.708280807288247e-07, accuracy=1.0\n",
            "Test loss = 6.606771469116211, Test accuracy = 0.7405797243118286\n",
            "Step=25799 loss=5.972851595004159e-07, accuracy=1.0\n",
            "Test loss = 6.597314357757568, Test accuracy = 0.7405797243118286\n",
            "Step=25899 loss=4.173962913966989e-07, accuracy=1.0\n",
            "Test loss = 6.567183017730713, Test accuracy = 0.7405797243118286\n",
            "Step=25999 loss=3.9201653649456604e-07, accuracy=1.0\n",
            "Test loss = 6.559537887573242, Test accuracy = 0.7420290112495422\n",
            "Step=26099 loss=3.506858526236556e-07, accuracy=1.0\n",
            "Test loss = 6.551013469696045, Test accuracy = 0.7420290112495422\n",
            "Step=26199 loss=3.5317234107878905e-07, accuracy=1.0\n",
            "Test loss = 6.5548996925354, Test accuracy = 0.7420290112495422\n",
            "Step=26299 loss=3.25755194978683e-07, accuracy=1.0\n",
            "Test loss = 6.549324989318848, Test accuracy = 0.7420290112495422\n",
            "Step=26399 loss=2.910731422822721e-07, accuracy=1.0\n",
            "Test loss = 6.544185161590576, Test accuracy = 0.7420290112495422\n",
            "Step=26499 loss=2.477491776176066e-07, accuracy=1.0\n",
            "Test loss = 6.529259204864502, Test accuracy = 0.7420290112495422\n",
            "Step=26599 loss=2.5155762870099354e-07, accuracy=1.0\n",
            "Test loss = 6.5319366455078125, Test accuracy = 0.7420290112495422\n",
            "Step=26699 loss=2.09360261962388e-07, accuracy=1.0\n",
            "Test loss = 6.521713733673096, Test accuracy = 0.7434782385826111\n",
            "Step=26799 loss=2.1663353990675205e-07, accuracy=1.0\n",
            "Test loss = 6.524845600128174, Test accuracy = 0.7449275255203247\n",
            "Step=26899 loss=2.0243128449948246e-07, accuracy=1.0\n",
            "Test loss = 6.5181074142456055, Test accuracy = 0.7449275255203247\n",
            "Step=26999 loss=1.8793993591614822e-07, accuracy=1.0\n",
            "Test loss = 6.519003868103027, Test accuracy = 0.7434782385826111\n",
            "Step=27099 loss=1.6094127857968488e-07, accuracy=1.0\n",
            "Test loss = 6.512140274047852, Test accuracy = 0.7434782385826111\n",
            "Step=27199 loss=1.6327892019774026e-07, accuracy=1.0\n",
            "Test loss = 6.503699779510498, Test accuracy = 0.7434782385826111\n",
            "Step=27299 loss=1.6522524152406958e-07, accuracy=1.0\n",
            "Test loss = 6.511398792266846, Test accuracy = 0.7434782385826111\n",
            "Step=27399 loss=1.2497391285393178e-07, accuracy=1.0\n",
            "Test loss = 6.502496719360352, Test accuracy = 0.7434782385826111\n",
            "Step=27499 loss=1.2030796309403513e-07, accuracy=1.0\n",
            "Test loss = 6.496359825134277, Test accuracy = 0.7434782385826111\n",
            "Step=27599 loss=1.1605186859497963e-07, accuracy=1.0\n",
            "Test loss = 6.483262538909912, Test accuracy = 0.7434782385826111\n",
            "Step=27699 loss=1.1796102445060796e-07, accuracy=1.0\n",
            "Test loss = 6.484827995300293, Test accuracy = 0.7434782385826111\n",
            "Step=27799 loss=1.1111582136180687e-07, accuracy=1.0\n",
            "Test loss = 6.488194942474365, Test accuracy = 0.7434782385826111\n",
            "Step=27899 loss=1.1314615782964666e-07, accuracy=1.0\n",
            "Test loss = 6.483829498291016, Test accuracy = 0.7434782385826111\n",
            "Step=27999 loss=9.905529786635725e-08, accuracy=1.0\n",
            "Test loss = 6.4782023429870605, Test accuracy = 0.7420290112495422\n",
            "Step=28099 loss=9.469672363593417e-08, accuracy=1.0\n",
            "Test loss = 6.473404407501221, Test accuracy = 0.7405797243118286\n",
            "Step=28199 loss=8.458261193666772e-08, accuracy=1.0\n",
            "Test loss = 6.465224266052246, Test accuracy = 0.7405797243118286\n",
            "Step=28299 loss=8.403310419069499e-08, accuracy=1.0\n",
            "Test loss = 6.464104175567627, Test accuracy = 0.7405797243118286\n",
            "Step=28399 loss=8.071762087169531e-08, accuracy=1.0\n",
            "Test loss = 6.465060234069824, Test accuracy = 0.739130437374115\n",
            "Step=28499 loss=7.921819522138663e-08, accuracy=1.0\n",
            "Test loss = 6.4601569175720215, Test accuracy = 0.7405797243118286\n",
            "Step=28599 loss=7.923682483479411e-08, accuracy=1.0\n",
            "Test loss = 6.4604010581970215, Test accuracy = 0.739130437374115\n",
            "Step=28699 loss=7.049172809914239e-08, accuracy=1.0\n",
            "Test loss = 6.454024314880371, Test accuracy = 0.7405797243118286\n",
            "Step=28799 loss=7.209359838000751e-08, accuracy=1.0\n",
            "Test loss = 6.459014892578125, Test accuracy = 0.7405797243118286\n",
            "Step=28899 loss=6.966285900134039e-08, accuracy=1.0\n",
            "Test loss = 6.455502510070801, Test accuracy = 0.7405797243118286\n",
            "Step=28999 loss=6.142998980607217e-08, accuracy=1.0\n",
            "Test loss = 6.448233127593994, Test accuracy = 0.7420290112495422\n",
            "Step=29099 loss=5.6689561898082275e-08, accuracy=1.0\n",
            "Test loss = 6.446737766265869, Test accuracy = 0.7420290112495422\n",
            "Step=29199 loss=5.97908596766672e-08, accuracy=1.0\n",
            "Test loss = 6.449211120605469, Test accuracy = 0.7420290112495422\n",
            "Step=29299 loss=5.669887576331689e-08, accuracy=1.0\n",
            "Test loss = 6.4415364265441895, Test accuracy = 0.7420290112495422\n",
            "Step=29399 loss=5.342062685542714e-08, accuracy=1.0\n",
            "Test loss = 6.436738014221191, Test accuracy = 0.7420290112495422\n",
            "Step=29499 loss=5.214471384462627e-08, accuracy=1.0\n",
            "Test loss = 6.434216499328613, Test accuracy = 0.7420290112495422\n",
            "Step=29599 loss=4.99840490419956e-08, accuracy=1.0\n",
            "Test loss = 6.439688205718994, Test accuracy = 0.7420290112495422\n",
            "Step=29699 loss=4.99654250596393e-08, accuracy=1.0\n",
            "Test loss = 6.440844535827637, Test accuracy = 0.7420290112495422\n",
            "Step=29799 loss=4.59048673562279e-08, accuracy=1.0\n",
            "Test loss = 6.4340691566467285, Test accuracy = 0.7420290112495422\n",
            "Step=29899 loss=4.4014282263304946e-08, accuracy=1.0\n",
            "Test loss = 6.428478240966797, Test accuracy = 0.7420290112495422\n",
            "Step=29999 loss=4.458238315407925e-08, accuracy=1.0\n",
            "Test loss = 6.433304309844971, Test accuracy = 0.7420290112495422\n",
            "Step=30099 loss=4.118306366152069e-08, accuracy=1.0\n",
            "Test loss = 6.433096885681152, Test accuracy = 0.7420290112495422\n",
            "Step=30199 loss=4.629602260664001e-08, accuracy=1.0\n",
            "Test loss = 6.439620018005371, Test accuracy = 0.7420290112495422\n",
            "Step=30299 loss=4.2021258011004645e-08, accuracy=1.0\n",
            "Test loss = 6.435423851013184, Test accuracy = 0.7420290112495422\n",
            "Step=30399 loss=3.863124460323775e-08, accuracy=1.0\n",
            "Test loss = 6.431231498718262, Test accuracy = 0.7420290112495422\n",
            "Step=30499 loss=4.08850453492704e-08, accuracy=1.0\n",
            "Test loss = 6.4324798583984375, Test accuracy = 0.7420290112495422\n",
            "Step=30599 loss=3.750434534488534e-08, accuracy=1.0\n",
            "Test loss = 6.431434631347656, Test accuracy = 0.7420290112495422\n",
            "Step=30699 loss=3.695486609167631e-08, accuracy=1.0\n",
            "Test loss = 6.4341349601745605, Test accuracy = 0.7420290112495422\n",
            "Step=30799 loss=3.5213293640978805e-08, accuracy=1.0\n",
            "Test loss = 6.434019565582275, Test accuracy = 0.7420290112495422\n",
            "Step=30899 loss=3.009102415241216e-08, accuracy=1.0\n",
            "Test loss = 6.432461738586426, Test accuracy = 0.7420290112495422\n",
            "Step=30999 loss=2.756714092111423e-08, accuracy=1.0\n",
            "Test loss = 6.426249027252197, Test accuracy = 0.7420290112495422\n",
            "Step=31099 loss=2.662650476992212e-08, accuracy=1.0\n",
            "Test loss = 6.429245948791504, Test accuracy = 0.7420290112495422\n",
            "Step=31199 loss=2.7818597478912465e-08, accuracy=1.0\n",
            "Test loss = 6.4287428855896, Test accuracy = 0.7420290112495422\n",
            "Step=31299 loss=2.6477493619836424e-08, accuracy=1.0\n",
            "Test loss = 6.428232669830322, Test accuracy = 0.7420290112495422\n",
            "Step=31399 loss=2.5471666589282903e-08, accuracy=1.0\n",
            "Test loss = 6.425050735473633, Test accuracy = 0.7420290112495422\n",
            "Step=31499 loss=2.398155068306096e-08, accuracy=1.0\n",
            "Test loss = 6.423828125, Test accuracy = 0.7420290112495422\n",
            "Step=31599 loss=2.265907257159583e-08, accuracy=1.0\n",
            "Test loss = 6.425915718078613, Test accuracy = 0.7420290112495422\n",
            "Step=31699 loss=2.1196896984321256e-08, accuracy=1.0\n",
            "Test loss = 6.428210735321045, Test accuracy = 0.7420290112495422\n",
            "Step=31799 loss=2.2016460703433438e-08, accuracy=1.0\n",
            "Test loss = 6.428754806518555, Test accuracy = 0.7420290112495422\n",
            "Step=31899 loss=1.974403456506124e-08, accuracy=1.0\n",
            "Test loss = 6.422677993774414, Test accuracy = 0.7420290112495422\n",
            "Step=31999 loss=1.971609475237557e-08, accuracy=1.0\n",
            "Test loss = 6.43113374710083, Test accuracy = 0.7434782385826111\n",
            "Step=32099 loss=1.9175928089865123e-08, accuracy=1.0\n",
            "Test loss = 6.42794942855835, Test accuracy = 0.7434782385826111\n",
            "Step=32199 loss=1.7182898464085384e-08, accuracy=1.0\n",
            "Test loss = 6.425497531890869, Test accuracy = 0.7434782385826111\n",
            "Step=32299 loss=1.6205010038294887e-08, accuracy=1.0\n",
            "Test loss = 6.426691055297852, Test accuracy = 0.7434782385826111\n",
            "Step=32399 loss=1.654959920660559e-08, accuracy=1.0\n",
            "Test loss = 6.428429126739502, Test accuracy = 0.7434782385826111\n",
            "Step=32499 loss=1.766718590090477e-08, accuracy=1.0\n",
            "Test loss = 6.432837963104248, Test accuracy = 0.7434782385826111\n",
            "Step=32599 loss=1.7639245843970032e-08, accuracy=1.0\n",
            "Test loss = 6.4352707862854, Test accuracy = 0.7434782385826111\n",
            "Step=32699 loss=1.584179416092013e-08, accuracy=1.0\n",
            "Test loss = 6.436858654022217, Test accuracy = 0.7434782385826111\n",
            "Step=32799 loss=1.6959381170522647e-08, accuracy=1.0\n",
            "Test loss = 6.438867092132568, Test accuracy = 0.7434782385826111\n",
            "Step=32899 loss=1.4649701934876801e-08, accuracy=1.0\n",
            "Test loss = 6.432536602020264, Test accuracy = 0.7434782385826111\n",
            "Step=32999 loss=1.5385446689997196e-08, accuracy=1.0\n",
            "Test loss = 6.436466693878174, Test accuracy = 0.7434782385826111\n",
            "Step=33099 loss=1.4826653108590548e-08, accuracy=1.0\n",
            "Test loss = 6.438150882720947, Test accuracy = 0.7434782385826111\n",
            "Step=33199 loss=1.4090908577735206e-08, accuracy=1.0\n",
            "Test loss = 6.438005447387695, Test accuracy = 0.7434782385826111\n",
            "Step=33299 loss=1.5357507241464674e-08, accuracy=1.0\n",
            "Test loss = 6.443282127380371, Test accuracy = 0.7434782385826111\n",
            "Step=33399 loss=1.1650844058408438e-08, accuracy=1.0\n",
            "Test loss = 6.443132400512695, Test accuracy = 0.7434782385826111\n",
            "Step=33499 loss=1.2451781310485189e-08, accuracy=1.0\n",
            "Test loss = 6.445905685424805, Test accuracy = 0.7434782385826111\n",
            "Step=33599 loss=1.0915099343256074e-08, accuracy=1.0\n",
            "Test loss = 6.447426795959473, Test accuracy = 0.7434782385826111\n",
            "Step=33699 loss=1.036561906753164e-08, accuracy=1.0\n",
            "Test loss = 6.4439921379089355, Test accuracy = 0.7434782385826111\n",
            "Step=33799 loss=1.099891832501143e-08, accuracy=1.0\n",
            "Test loss = 6.452127933502197, Test accuracy = 0.7434782385826111\n",
            "Step=33899 loss=1.0654329150039388e-08, accuracy=1.0\n",
            "Test loss = 6.451867580413818, Test accuracy = 0.7434782385826111\n",
            "Step=33999 loss=1.1343507688721388e-08, accuracy=1.0\n",
            "Test loss = 6.453790187835693, Test accuracy = 0.7434782385826111\n",
            "Step=34099 loss=1.1129303578716333e-08, accuracy=1.0\n",
            "Test loss = 6.45418643951416, Test accuracy = 0.7434782385826111\n",
            "Step=34199 loss=1.0691582053024008e-08, accuracy=1.0\n",
            "Test loss = 6.459614276885986, Test accuracy = 0.7434782385826111\n",
            "Step=34299 loss=9.76025950449877e-09, accuracy=1.0\n",
            "Test loss = 6.462324619293213, Test accuracy = 0.7434782385826111\n",
            "Step=34399 loss=9.86270502378872e-09, accuracy=1.0\n",
            "Test loss = 6.4676008224487305, Test accuracy = 0.7434782385826111\n",
            "Step=34499 loss=9.462236387758339e-09, accuracy=1.0\n",
            "Test loss = 6.471970558166504, Test accuracy = 0.7434782385826111\n",
            "Step=34599 loss=8.75443131764797e-09, accuracy=1.0\n",
            "Test loss = 6.475099563598633, Test accuracy = 0.7434782385826111\n",
            "Step=34699 loss=8.13044524428097e-09, accuracy=1.0\n",
            "Test loss = 6.477118492126465, Test accuracy = 0.7434782385826111\n",
            "Step=34799 loss=8.102505617557654e-09, accuracy=1.0\n",
            "Test loss = 6.4816575050354, Test accuracy = 0.7434782385826111\n",
            "Step=34899 loss=7.795169194579898e-09, accuracy=1.0\n",
            "Test loss = 6.486965179443359, Test accuracy = 0.7434782385826111\n",
            "Step=34999 loss=7.664784070215979e-09, accuracy=1.0\n",
            "Test loss = 6.4885573387146, Test accuracy = 0.7449275255203247\n",
            "Step=35099 loss=7.264315382560227e-09, accuracy=1.0\n",
            "Test loss = 6.492456436157227, Test accuracy = 0.7434782385826111\n",
            "Step=35199 loss=8.13044530978413e-09, accuracy=1.0\n",
            "Test loss = 6.495466232299805, Test accuracy = 0.7434782385826111\n",
            "Step=35299 loss=7.664784035799067e-09, accuracy=1.0\n",
            "Test loss = 6.499462127685547, Test accuracy = 0.7434782385826111\n",
            "Step=35399 loss=8.130445289800114e-09, accuracy=1.0\n",
            "Test loss = 6.506267070770264, Test accuracy = 0.7434782385826111\n",
            "Step=35499 loss=8.055939497153552e-09, accuracy=1.0\n",
            "Test loss = 6.512568473815918, Test accuracy = 0.7434782385826111\n",
            "Step=35599 loss=6.603076420641507e-09, accuracy=1.0\n",
            "Test loss = 6.511785507202148, Test accuracy = 0.7434782385826111\n",
            "Step=35699 loss=7.972120467103494e-09, accuracy=1.0\n",
            "Test loss = 6.5195441246032715, Test accuracy = 0.7434782385826111\n",
            "Step=35799 loss=8.055939491602438e-09, accuracy=1.0\n",
            "Test loss = 6.525608539581299, Test accuracy = 0.7420290112495422\n",
            "Step=35899 loss=7.795169226221254e-09, accuracy=1.0\n",
            "Test loss = 6.533121585845947, Test accuracy = 0.7434782385826111\n",
            "Step=35999 loss=8.093192435110197e-09, accuracy=1.0\n",
            "Test loss = 6.544404029846191, Test accuracy = 0.7420290112495422\n",
            "Step=36099 loss=7.748603131352282e-09, accuracy=1.0\n",
            "Test loss = 6.548117160797119, Test accuracy = 0.7449275255203247\n",
            "Step=36199 loss=7.227062505665849e-09, accuracy=1.0\n",
            "Test loss = 6.549600124359131, Test accuracy = 0.7449275255203247\n",
            "Step=36299 loss=8.78237104817714e-09, accuracy=1.0\n",
            "Test loss = 6.562654495239258, Test accuracy = 0.7449275255203247\n",
            "Step=36399 loss=8.409842094936338e-09, accuracy=1.0\n",
            "Test loss = 6.575530529022217, Test accuracy = 0.7449275255203247\n",
            "Step=36499 loss=7.70203697708638e-09, accuracy=1.0\n",
            "Test loss = 6.577687740325928, Test accuracy = 0.7449275255203247\n",
            "Step=36599 loss=9.03382812889042e-09, accuracy=1.0\n",
            "Test loss = 6.587345123291016, Test accuracy = 0.7449275255203247\n",
            "Step=36699 loss=9.210779374768663e-09, accuracy=1.0\n",
            "Test loss = 6.602047443389893, Test accuracy = 0.7434782385826111\n",
            "Step=36799 loss=8.689238796266707e-09, accuracy=1.0\n",
            "Test loss = 6.603672504425049, Test accuracy = 0.7434782385826111\n",
            "Step=36899 loss=8.32602305766983e-09, accuracy=1.0\n",
            "Test loss = 6.613795757293701, Test accuracy = 0.7434782385826111\n",
            "Step=36999 loss=9.937210891930449e-09, accuracy=1.0\n",
            "Test loss = 6.640927791595459, Test accuracy = 0.7449275255203247\n",
            "Step=37099 loss=9.024514928679395e-09, accuracy=1.0\n",
            "Test loss = 6.641308784484863, Test accuracy = 0.7449275255203247\n",
            "Step=37199 loss=9.760259654933989e-09, accuracy=1.0\n",
            "Test loss = 6.658082962036133, Test accuracy = 0.7434782385826111\n",
            "Step=37299 loss=9.862705103724778e-09, accuracy=1.0\n",
            "Test loss = 6.658726215362549, Test accuracy = 0.7434782385826111\n",
            "Step=37399 loss=9.629874505034941e-09, accuracy=1.0\n",
            "Test loss = 6.68963623046875, Test accuracy = 0.7449275255203247\n",
            "Step=37499 loss=9.23871900204709e-09, accuracy=1.0\n",
            "Test loss = 6.709546089172363, Test accuracy = 0.7449275255203247\n",
            "Step=37599 loss=9.788199242244389e-09, accuracy=1.0\n",
            "Test loss = 6.720250129699707, Test accuracy = 0.7449275255203247\n",
            "Step=37699 loss=1.0039656372917705e-08, accuracy=1.0\n",
            "Test loss = 6.749277591705322, Test accuracy = 0.7463768124580383\n",
            "Step=37799 loss=1.0784714230549497e-08, accuracy=1.0\n",
            "Test loss = 6.755037784576416, Test accuracy = 0.7463768124580383\n",
            "Step=37899 loss=1.0533257239764283e-08, accuracy=1.0\n",
            "Test loss = 6.778374195098877, Test accuracy = 0.7463768124580383\n",
            "Step=37999 loss=1.1371447383723422e-08, accuracy=1.0\n",
            "Test loss = 6.807152271270752, Test accuracy = 0.7463768124580383\n",
            "Step=38099 loss=1.0980291915707596e-08, accuracy=1.0\n",
            "Test loss = 6.826234340667725, Test accuracy = 0.7463768124580383\n",
            "Step=38199 loss=1.1213122486086746e-08, accuracy=1.0\n",
            "Test loss = 6.84716796875, Test accuracy = 0.7449275255203247\n",
            "Step=38299 loss=1.2032686274032756e-08, accuracy=1.0\n",
            "Test loss = 6.882822513580322, Test accuracy = 0.7449275255203247\n",
            "Step=38399 loss=1.1585651584766764e-08, accuracy=1.0\n",
            "Test loss = 6.906591892242432, Test accuracy = 0.7449275255203247\n",
            "Step=38499 loss=1.1613591178738503e-08, accuracy=1.0\n",
            "Test loss = 6.915470600128174, Test accuracy = 0.7449275255203247\n",
            "Step=38599 loss=1.1557711917520308e-08, accuracy=1.0\n",
            "Test loss = 6.923529624938965, Test accuracy = 0.7449275255203247\n",
            "Step=38699 loss=1.2852250097505902e-08, accuracy=1.0\n",
            "Test loss = 6.9701433181762695, Test accuracy = 0.7434782385826111\n",
            "Step=38799 loss=1.1166556431740914e-08, accuracy=1.0\n",
            "Test loss = 6.97700834274292, Test accuracy = 0.7449275255203247\n",
            "Step=38899 loss=1.0496004282378735e-08, accuracy=1.0\n",
            "Test loss = 6.9973249435424805, Test accuracy = 0.7463768124580383\n",
            "Step=38999 loss=1.1241062175537663e-08, accuracy=1.0\n",
            "Test loss = 7.0531392097473145, Test accuracy = 0.7463768124580383\n",
            "Step=39099 loss=1.0086222462790673e-08, accuracy=1.0\n",
            "Test loss = 7.066290855407715, Test accuracy = 0.7463768124580383\n",
            "Step=39199 loss=1.0719521532087662e-08, accuracy=1.0\n",
            "Test loss = 7.096092224121094, Test accuracy = 0.7463768124580383\n",
            "Step=39299 loss=1.0952352213489113e-08, accuracy=1.0\n",
            "Test loss = 7.129877090454102, Test accuracy = 0.7463768124580383\n",
            "Step=39399 loss=1.1371447318220263e-08, accuracy=1.0\n",
            "Test loss = 7.156691551208496, Test accuracy = 0.7463768124580383\n",
            "Step=39499 loss=1.1408700304471608e-08, accuracy=1.0\n",
            "Test loss = 7.232802391052246, Test accuracy = 0.7449275255203247\n",
            "Step=39599 loss=1.0626389377321744e-08, accuracy=1.0\n",
            "Test loss = 7.26681661605835, Test accuracy = 0.7463768124580383\n",
            "Step=39699 loss=1.1110677073933318e-08, accuracy=1.0\n",
            "Test loss = 7.304901123046875, Test accuracy = 0.747826099395752\n",
            "Step=39799 loss=1.1697409953992377e-08, accuracy=1.0\n",
            "Test loss = 7.451737880706787, Test accuracy = 0.747826099395752\n",
            "Step=39899 loss=1.247040777030417e-08, accuracy=1.0\n",
            "Test loss = 7.434568881988525, Test accuracy = 0.7463768124580383\n",
            "Step=39999 loss=1.358799465944749e-08, accuracy=1.0\n",
            "Test loss = 7.481443405151367, Test accuracy = 0.7463768124580383\n",
            "Step=40099 loss=1.3895330637780923e-08, accuracy=1.0\n",
            "Test loss = 7.556329727172852, Test accuracy = 0.7463768124580383\n",
            "Step=40199 loss=1.3513488283933838e-08, accuracy=1.0\n",
            "Test loss = 7.693355083465576, Test accuracy = 0.7449275255203247\n",
            "Step=40299 loss=1.2954695144951067e-08, accuracy=1.0\n",
            "Test loss = 7.7513628005981445, Test accuracy = 0.7449275255203247\n",
            "Step=40399 loss=1.3457609033817875e-08, accuracy=1.0\n",
            "Test loss = 7.822971820831299, Test accuracy = 0.7463768124580383\n",
            "Step=40499 loss=1.2749804116918284e-08, accuracy=1.0\n",
            "Test loss = 7.9029412269592285, Test accuracy = 0.747826099395752\n",
            "Step=40599 loss=1.1790541977196866e-08, accuracy=1.0\n",
            "Test loss = 8.029817581176758, Test accuracy = 0.747826099395752\n",
            "Step=40699 loss=1.4184040293407918e-08, accuracy=1.0\n",
            "Test loss = 8.165783882141113, Test accuracy = 0.747826099395752\n",
            "Step=40799 loss=1.3373790257453777e-08, accuracy=1.0\n",
            "Test loss = 8.270328521728516, Test accuracy = 0.7507246136665344\n",
            "Step=40899 loss=1.494772454702087e-08, accuracy=1.0\n",
            "Test loss = 8.563406944274902, Test accuracy = 0.7492753863334656\n",
            "Step=40999 loss=1.6344707876037034e-08, accuracy=1.0\n",
            "Test loss = 8.592260360717773, Test accuracy = 0.7507246136665344\n",
            "Step=41099 loss=0.007492369532311918, accuracy=0.998046875\n",
            "Test loss = 8.004533767700195, Test accuracy = 0.7492753863334656\n",
            "Step=41199 loss=0.00024307505169815613, accuracy=0.999921875\n",
            "Test loss = 8.442400932312012, Test accuracy = 0.7536231875419617\n",
            "Step=41299 loss=3.8019948448209106e-06, accuracy=1.0\n",
            "Test loss = 8.463301658630371, Test accuracy = 0.7550724744796753\n",
            "Step=41399 loss=2.4119328760718872e-06, accuracy=1.0\n",
            "Test loss = 8.454305648803711, Test accuracy = 0.7550724744796753\n",
            "Step=41499 loss=2.447367618287899e-06, accuracy=1.0\n",
            "Test loss = 8.454728126525879, Test accuracy = 0.7565217614173889\n",
            "Step=41599 loss=1.904721606464932e-06, accuracy=1.0\n",
            "Test loss = 8.451254844665527, Test accuracy = 0.7565217614173889\n",
            "Step=41699 loss=1.718209762486822e-06, accuracy=1.0\n",
            "Test loss = 8.454851150512695, Test accuracy = 0.7565217614173889\n",
            "Step=41799 loss=1.5121655559369173e-06, accuracy=1.0\n",
            "Test loss = 8.455538749694824, Test accuracy = 0.7565217614173889\n",
            "Step=41899 loss=1.408304884691347e-06, accuracy=1.0\n",
            "Test loss = 8.4592866897583, Test accuracy = 0.7550724744796753\n",
            "Step=41999 loss=1.232696877693229e-06, accuracy=1.0\n",
            "Test loss = 8.461243629455566, Test accuracy = 0.7565217614173889\n",
            "Step=42099 loss=1.1952087747602037e-06, accuracy=1.0\n",
            "Test loss = 8.461732864379883, Test accuracy = 0.7550724744796753\n",
            "Step=42199 loss=1.1102470041635115e-06, accuracy=1.0\n",
            "Test loss = 8.464170455932617, Test accuracy = 0.7550724744796753\n",
            "Step=42299 loss=9.666922931472754e-07, accuracy=1.0\n",
            "Test loss = 8.463323593139648, Test accuracy = 0.7550724744796753\n",
            "Step=42399 loss=8.419630614042717e-07, accuracy=1.0\n",
            "Test loss = 8.46562385559082, Test accuracy = 0.7536231875419617\n",
            "Step=42499 loss=8.06940025981362e-07, accuracy=1.0\n",
            "Test loss = 8.465144157409668, Test accuracy = 0.7536231875419617\n",
            "Step=42599 loss=8.307810020369288e-07, accuracy=1.0\n",
            "Test loss = 8.467039108276367, Test accuracy = 0.7536231875419617\n",
            "Step=42699 loss=7.721367103386001e-07, accuracy=1.0\n",
            "Test loss = 8.470203399658203, Test accuracy = 0.7536231875419617\n",
            "Step=42799 loss=7.235980436703926e-07, accuracy=1.0\n",
            "Test loss = 8.475994110107422, Test accuracy = 0.7536231875419617\n",
            "Step=42899 loss=6.170864842047763e-07, accuracy=1.0\n",
            "Test loss = 8.475194931030273, Test accuracy = 0.7536231875419617\n",
            "Step=42999 loss=6.816533900178002e-07, accuracy=1.0\n",
            "Test loss = 8.482321739196777, Test accuracy = 0.7550724744796753\n",
            "Step=43099 loss=5.844915634156677e-07, accuracy=1.0\n",
            "Test loss = 8.481714248657227, Test accuracy = 0.7550724744796753\n",
            "Step=43199 loss=5.165713307064834e-07, accuracy=1.0\n",
            "Test loss = 8.481770515441895, Test accuracy = 0.7550724744796753\n",
            "Step=43299 loss=4.991561760903096e-07, accuracy=1.0\n",
            "Test loss = 8.480249404907227, Test accuracy = 0.7550724744796753\n",
            "Step=43399 loss=4.771492981348047e-07, accuracy=1.0\n",
            "Test loss = 8.48075008392334, Test accuracy = 0.7550724744796753\n",
            "Step=43499 loss=4.744858874516922e-07, accuracy=1.0\n",
            "Test loss = 8.482579231262207, Test accuracy = 0.7550724744796753\n",
            "Step=43599 loss=4.3687021153537844e-07, accuracy=1.0\n",
            "Test loss = 8.48604965209961, Test accuracy = 0.7550724744796753\n",
            "Step=43699 loss=4.0626762402240504e-07, accuracy=1.0\n",
            "Test loss = 8.486351013183594, Test accuracy = 0.7550724744796753\n",
            "Step=43799 loss=4.477669020275243e-07, accuracy=1.0\n",
            "Test loss = 8.48757553100586, Test accuracy = 0.7550724744796753\n",
            "Step=43899 loss=4.4538310429942384e-07, accuracy=1.0\n",
            "Test loss = 8.488809585571289, Test accuracy = 0.7550724744796753\n",
            "Step=43999 loss=3.8752068970637765e-07, accuracy=1.0\n",
            "Test loss = 8.489336013793945, Test accuracy = 0.7550724744796753\n",
            "Step=44099 loss=3.491228074636865e-07, accuracy=1.0\n",
            "Test loss = 8.488656044006348, Test accuracy = 0.7550724744796753\n",
            "Step=44199 loss=3.4697154262630646e-07, accuracy=1.0\n",
            "Test loss = 8.489947319030762, Test accuracy = 0.7550724744796753\n",
            "Step=44299 loss=3.3004943105652274e-07, accuracy=1.0\n",
            "Test loss = 8.491239547729492, Test accuracy = 0.7550724744796753\n",
            "Step=44399 loss=3.3217297122689616e-07, accuracy=1.0\n",
            "Test loss = 8.489309310913086, Test accuracy = 0.7550724744796753\n",
            "Step=44499 loss=3.4737191988654104e-07, accuracy=1.0\n",
            "Test loss = 8.490498542785645, Test accuracy = 0.7550724744796753\n",
            "Step=44599 loss=2.8785196150238333e-07, accuracy=1.0\n",
            "Test loss = 8.487874984741211, Test accuracy = 0.7550724744796753\n",
            "Step=44699 loss=2.96978573146589e-07, accuracy=1.0\n",
            "Test loss = 8.488306999206543, Test accuracy = 0.7550724744796753\n",
            "Step=44799 loss=2.913256263070707e-07, accuracy=1.0\n",
            "Test loss = 8.490106582641602, Test accuracy = 0.7550724744796753\n",
            "Step=44899 loss=2.6314402614247e-07, accuracy=1.0\n",
            "Test loss = 8.490775108337402, Test accuracy = 0.7550724744796753\n",
            "Step=44999 loss=2.4143508980500883e-07, accuracy=1.0\n",
            "Test loss = 8.48715591430664, Test accuracy = 0.7550724744796753\n",
            "Step=45099 loss=2.431486859677534e-07, accuracy=1.0\n",
            "Test loss = 8.485658645629883, Test accuracy = 0.7550724744796753\n",
            "Step=45199 loss=2.487086904423563e-07, accuracy=1.0\n",
            "Test loss = 8.487300872802734, Test accuracy = 0.7550724744796753\n",
            "Step=45299 loss=2.5166100179774277e-07, accuracy=1.0\n",
            "Test loss = 8.486906051635742, Test accuracy = 0.7550724744796753\n",
            "Step=45399 loss=2.388180608470236e-07, accuracy=1.0\n",
            "Test loss = 8.491495132446289, Test accuracy = 0.7550724744796753\n",
            "Step=45499 loss=2.3363073339055519e-07, accuracy=1.0\n",
            "Test loss = 8.490803718566895, Test accuracy = 0.7550724744796753\n",
            "Step=45599 loss=2.0871801179822568e-07, accuracy=1.0\n",
            "Test loss = 8.488835334777832, Test accuracy = 0.7550724744796753\n",
            "Step=45699 loss=2.1235944743125402e-07, accuracy=1.0\n",
            "Test loss = 8.490151405334473, Test accuracy = 0.7550724744796753\n",
            "Step=45799 loss=2.0793571316346516e-07, accuracy=1.0\n",
            "Test loss = 8.490303993225098, Test accuracy = 0.7550724744796753\n",
            "Step=45899 loss=2.083734551305838e-07, accuracy=1.0\n",
            "Test loss = 8.492161750793457, Test accuracy = 0.7550724744796753\n",
            "Step=45999 loss=1.7830123969275746e-07, accuracy=1.0\n",
            "Test loss = 8.487593650817871, Test accuracy = 0.7550724744796753\n",
            "Step=46099 loss=1.9027791100967306e-07, accuracy=1.0\n",
            "Test loss = 8.488734245300293, Test accuracy = 0.7550724744796753\n",
            "Step=46199 loss=1.8872264842428876e-07, accuracy=1.0\n",
            "Test loss = 8.489635467529297, Test accuracy = 0.7550724744796753\n",
            "Step=46299 loss=2.00140622013123e-07, accuracy=1.0\n",
            "Test loss = 8.493082046508789, Test accuracy = 0.7550724744796753\n",
            "Step=46399 loss=1.8302301558748012e-07, accuracy=1.0\n",
            "Test loss = 8.493247985839844, Test accuracy = 0.7550724744796753\n",
            "Step=46499 loss=1.722942365844915e-07, accuracy=1.0\n",
            "Test loss = 8.492146492004395, Test accuracy = 0.7550724744796753\n",
            "Step=46599 loss=1.6402411027627294e-07, accuracy=1.0\n",
            "Test loss = 8.49303913116455, Test accuracy = 0.7550724744796753\n",
            "Step=46699 loss=1.5069697225200685e-07, accuracy=1.0\n",
            "Test loss = 8.488093376159668, Test accuracy = 0.7550724744796753\n",
            "Step=46799 loss=1.6852241245146614e-07, accuracy=1.0\n",
            "Test loss = 8.492450714111328, Test accuracy = 0.7550724744796753\n",
            "Step=46899 loss=1.4696239176714697e-07, accuracy=1.0\n",
            "Test loss = 8.48793888092041, Test accuracy = 0.7550724744796753\n",
            "Step=46999 loss=1.498588201087614e-07, accuracy=1.0\n",
            "Test loss = 8.487539291381836, Test accuracy = 0.7550724744796753\n",
            "Step=47099 loss=1.3758400651653347e-07, accuracy=1.0\n",
            "Test loss = 8.480515480041504, Test accuracy = 0.7550724744796753\n",
            "Step=47199 loss=1.4415911213205844e-07, accuracy=1.0\n",
            "Test loss = 8.482763290405273, Test accuracy = 0.7550724744796753\n",
            "Step=47299 loss=1.3660611138277546e-07, accuracy=1.0\n",
            "Test loss = 8.485410690307617, Test accuracy = 0.7550724744796753\n",
            "Step=47399 loss=1.3791928989093093e-07, accuracy=1.0\n",
            "Test loss = 8.484467506408691, Test accuracy = 0.7550724744796753\n",
            "Step=47499 loss=1.2727430444670064e-07, accuracy=1.0\n",
            "Test loss = 8.477313041687012, Test accuracy = 0.7550724744796753\n",
            "Step=47599 loss=1.22478003472537e-07, accuracy=1.0\n",
            "Test loss = 8.479338645935059, Test accuracy = 0.7550724744796753\n",
            "Step=47699 loss=1.1586563486432055e-07, accuracy=1.0\n",
            "Test loss = 8.483918190002441, Test accuracy = 0.7550724744796753\n",
            "Step=47799 loss=1.15101971296383e-07, accuracy=1.0\n",
            "Test loss = 8.484722137451172, Test accuracy = 0.7550724744796753\n",
            "Step=47899 loss=1.1285748044542743e-07, accuracy=1.0\n",
            "Test loss = 8.485040664672852, Test accuracy = 0.7550724744796753\n",
            "Step=47999 loss=9.9101881652075e-08, accuracy=1.0\n",
            "Test loss = 8.478204727172852, Test accuracy = 0.7550724744796753\n",
            "Step=48099 loss=1.0661764394015449e-07, accuracy=1.0\n",
            "Test loss = 8.478203773498535, Test accuracy = 0.7550724744796753\n",
            "Step=48199 loss=1.028178553852399e-07, accuracy=1.0\n",
            "Test loss = 8.479150772094727, Test accuracy = 0.7550724744796753\n",
            "Step=48299 loss=9.614961010484535e-08, accuracy=1.0\n",
            "Test loss = 8.477448463439941, Test accuracy = 0.7550724744796753\n",
            "Step=48399 loss=1.0213800582192789e-07, accuracy=1.0\n",
            "Test loss = 8.478102684020996, Test accuracy = 0.7550724744796753\n",
            "Step=48499 loss=9.751865155394057e-08, accuracy=1.0\n",
            "Test loss = 8.481559753417969, Test accuracy = 0.7550724744796753\n",
            "Step=48599 loss=8.58585226382047e-08, accuracy=1.0\n",
            "Test loss = 8.479191780090332, Test accuracy = 0.7536231875419617\n",
            "Step=48699 loss=8.197491926154043e-08, accuracy=1.0\n",
            "Test loss = 8.472281455993652, Test accuracy = 0.7536231875419617\n",
            "Step=48799 loss=7.856627899371915e-08, accuracy=1.0\n",
            "Test loss = 8.473176002502441, Test accuracy = 0.7536231875419617\n",
            "Step=48899 loss=8.354884986516709e-08, accuracy=1.0\n",
            "Test loss = 8.471511840820312, Test accuracy = 0.7536231875419617\n",
            "Step=48999 loss=7.671295374933607e-08, accuracy=1.0\n",
            "Test loss = 8.469552040100098, Test accuracy = 0.7536231875419617\n",
            "Step=49099 loss=7.855697242931114e-08, accuracy=1.0\n",
            "Test loss = 8.472240447998047, Test accuracy = 0.752173900604248\n",
            "Step=49199 loss=7.641493633414598e-08, accuracy=1.0\n",
            "Test loss = 8.475176811218262, Test accuracy = 0.752173900604248\n",
            "Step=49299 loss=7.500864857235001e-08, accuracy=1.0\n",
            "Test loss = 8.471128463745117, Test accuracy = 0.752173900604248\n",
            "Step=49399 loss=7.693648248263684e-08, accuracy=1.0\n",
            "Test loss = 8.469654083251953, Test accuracy = 0.752173900604248\n",
            "Step=49499 loss=6.77070927768142e-08, accuracy=1.0\n",
            "Test loss = 8.466496467590332, Test accuracy = 0.752173900604248\n",
            "Step=49599 loss=5.759294092300138e-08, accuracy=1.0\n",
            "Test loss = 8.465062141418457, Test accuracy = 0.752173900604248\n",
            "Step=49699 loss=6.023789335785068e-08, accuracy=1.0\n",
            "Test loss = 8.460494041442871, Test accuracy = 0.752173900604248\n",
            "Step=49799 loss=6.039621819553531e-08, accuracy=1.0\n",
            "Test loss = 8.4658842086792, Test accuracy = 0.7507246136665344\n",
            "Step=49899 loss=5.6438106277312273e-08, accuracy=1.0\n",
            "Test loss = 8.462470054626465, Test accuracy = 0.7507246136665344\n",
            "Step=49999 loss=5.5124941704676186e-08, accuracy=1.0\n",
            "Test loss = 8.461015701293945, Test accuracy = 0.7507246136665344\n",
            "Reached 50001 epochs for CNN\n",
            "[[103 151]\n",
            " [ 21 415]]\n",
            "Normalized confusion matrix\n",
            "[[0.40551181 0.59448819]\n",
            " [0.04816514 0.95183486]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAEmCAYAAADbUaM7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nssGCqDHGZIMFUWOMyYb/A+Rem2ulvz4PAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGtxKKRVkNLiVUirIaHArpVSQ+X95MM91WwdqgQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6tnNqWFqTqW",
        "colab_type": "code",
        "outputId": "32ddf869-4c6a-49b8-ae45-ba6818e91da2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Training XGBoost\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "class TrainXGBBoost:\n",
        "\n",
        "    def __init__(self, num_historical_days, days=10, pct_change=0):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.test_data = []\n",
        "        self.test_labels = []\n",
        "    #    assert os.path.exists('./models/checkpoint')\n",
        "        gan = GAN(num_features=5, num_historical_days=num_historical_days,\n",
        "                        generator_input_size=200, is_train=False)\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            saver = tf.train.Saver()\n",
        "            with open(f'{googlepath}models/checkpoint', 'rb') as f:\n",
        "              model_name = next(f).split('\"'.encode())[1]\n",
        "    #          saver.restore(sess, \"{}models/{}\".format(googlepath, model_name.decode()))\n",
        "\n",
        "         #   files = [os.path.join('./stock_data', f) for f in os.listdir('./stock_data')]\n",
        "            files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]      \n",
        "            for file in files:\n",
        "                print(file)\n",
        "                #Read in file -- note that parse_dates will be need later\n",
        "                df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "                df = df[['open','high','low','close','volume']]\n",
        "                # #Create new index with missing days\n",
        "                # idx = pd.date_range(df.index[-1], df.index[0])\n",
        "                # #Reindex and fill the missing day with the value from the day before\n",
        "                # df = df.reindex(idx, method='bfill').sort_index(ascending=False)\n",
        "                #Normilize using a of size num_historical_days\n",
        "                labels = df.close.pct_change(days).map(lambda x: int(x > pct_change/100.0))\n",
        "                df = ((df -\n",
        "                df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "                /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "                -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "                df['labels'] = labels\n",
        "                #Drop the last 10 day that we don't have data for\n",
        "                df = df.dropna()\n",
        "                #Hold out the last year of trading for testing\n",
        "                test_df = df[:365]\n",
        "                #Padding to keep labels from bleeding\n",
        "                df = df[400:]\n",
        "                #This may not create good samples if num_historical_days is a\n",
        "                #mutliple of 7\n",
        "                data = df[['open', 'high', 'low', 'close', 'volume']].values\n",
        "                labels = df['labels'].values\n",
        "                for i in range(num_historical_days, len(df), num_historical_days):\n",
        "                    features = sess.run(gan.features, feed_dict={gan.X:[data[i-num_historical_days:i]]})\n",
        "                    self.data.append(features[0])\n",
        "                    print(features[0])\n",
        "                    self.labels.append(labels[i-1])\n",
        "                data = test_df[['open', 'high', 'low', 'close', 'volume']].values\n",
        "                labels = test_df['labels'].values\n",
        "                for i in range(num_historical_days, len(test_df), 1):\n",
        "                    features = sess.run(gan.features, feed_dict={gan.X:[data[i-num_historical_days:i]]})\n",
        "                    self.test_data.append(features[0])\n",
        "                    self.test_labels.append(labels[i-1])\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        params = {}\n",
        "        params['objective'] = 'multi:softprob'\n",
        "        params['eta'] = 0.01\n",
        "        params['num_class'] = 2\n",
        "        params['max_depth'] = 20\n",
        "        params['subsample'] = 0.05\n",
        "        params['colsample_bytree'] = 0.05\n",
        "        params['eval_metric'] = 'mlogloss'\n",
        "        #params['scale_pos_weight'] = 10\n",
        "        #params['silent'] = True\n",
        "        #params['gpu_id'] = 0\n",
        "        #params['max_bin'] = 16\n",
        "        #params['tree_method'] = 'gpu_hist'\n",
        "\n",
        "        train = xgb.DMatrix(self.data, self.labels)\n",
        "        test = xgb.DMatrix(self.test_data, self.test_labels)\n",
        "\n",
        "        watchlist = [(train, 'train'), (test, 'test')]\n",
        "        clf = xgb.train(params, train, 1000, evals=watchlist, early_stopping_rounds=100)\n",
        "        joblib.dump(clf, f'{googlepath}models/clf.pkl')\n",
        "        #cm = confusion_matrix(self.test_labels, map(lambda x: int(x[1] > .5), clf.predict(test)))\n",
        "        #print(cm)\n",
        "        #plot_confusion_matrix(cm, ['Down', 'Up'], normalize=True, title=\"Confusion Matrix\")\n",
        "\n",
        "\n",
        "boost_model = TrainXGBBoost(num_historical_days=20, days=10, pct_change=10)\n",
        "boost_model.train()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Relu:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_2:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"Relu_4:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_5:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_6:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AMZN.csv\n",
            "[0.03233599 0.         0.01741412 ... 0.06604493 0.         0.02162729]\n",
            "[0.         0.         0.04114368 ... 0.         0.         0.09578311]\n",
            "[0.0116357  0.         0.         ... 0.02863285 0.01337457 0.        ]\n",
            "[0.02551394 0.         0.02559086 ... 0.         0.00045254 0.00536522]\n",
            "[0.05187207 0.         0.01009213 ... 0.02371604 0.         0.02022137]\n",
            "[0.00233749 0.         0.         ... 0.         0.         0.03052939]\n",
            "[0.03253358 0.         0.02724711 ... 0.         0.0513097  0.        ]\n",
            "[0.02173332 0.         0.03193788 ... 0.03082949 0.         0.00505348]\n",
            "[0.03203054 0.         0.         ... 0.         0.         0.12238848]\n",
            "[0.02897845 0.         0.00126023 ... 0.00473168 0.         0.02439099]\n",
            "[0.00849279 0.00026431 0.05640072 ... 0.02302236 0.         0.00842175]\n",
            "[0.         0.         0.         ... 0.04616554 0.         0.03461901]\n",
            "[0.01943982 0.         0.         ... 0.         0.01595057 0.01924629]\n",
            "[0.         0.         0.04995041 ... 0.         0.02847041 0.        ]\n",
            "[0.00585243 0.         0.         ... 0.02510902 0.00599416 0.        ]\n",
            "[0.02055603 0.         0.01969388 ... 0.03634372 0.         0.        ]\n",
            "[0.00877874 0.         0.         ... 0.00702079 0.         0.01744582]\n",
            "[0.02979372 0.         0.         ... 0.         0.         0.02275095]\n",
            "[0.         0.         0.03700828 ... 0.         0.         0.00198227]\n",
            "[0.0123596  0.         0.         ... 0.00572683 0.         0.01730301]\n",
            "[0.04116255 0.         0.         ... 0.         0.01695333 0.00865426]\n",
            "[0.05198548 0.         0.08310337 ... 0.00557486 0.         0.01244999]\n",
            "[0.         0.         0.00263595 ... 0.03231228 0.00075452 0.        ]\n",
            "[0.00312032 0.         0.00699464 ... 0.         0.         0.04322532]\n",
            "[0.02029279 0.         0.01502894 ... 0.         0.00342801 0.01007181]\n",
            "[0.01889507 0.         0.         ... 0.01919246 0.00855669 0.        ]\n",
            "[0.         0.         0.         ... 0.         0.01869115 0.        ]\n",
            "[0.05873467 0.         0.02223    ... 0.         0.06248006 0.        ]\n",
            "[0.02840839 0.         0.01115491 ... 0.         0.01330079 0.        ]\n",
            "[0.04508822 0.         0.07666495 ... 0.         0.00750973 0.        ]\n",
            "[0.02185839 0.         0.00807505 ... 0.         0.01719755 0.06242901]\n",
            "[0.01986287 0.         0.         ... 0.         0.01248006 0.01737616]\n",
            "[0.03163689 0.         0.06651136 ... 0.         0.00837331 0.        ]\n",
            "[0.03646839 0.         0.05426225 ... 0.         0.01710617 0.01902495]\n",
            "[0.04141288 0.         0.10640321 ... 0.         0.         0.01304535]\n",
            "[0.04016317 0.         0.         ... 0.02588347 0.00814638 0.        ]\n",
            "[0.00804248 0.         0.         ... 0.02281105 0.         0.        ]\n",
            "[0.         0.         0.         ... 0.         0.00217864 0.01091263]\n",
            "[0.04059281 0.         0.02927712 ... 0.         0.         0.04436439]\n",
            "[0.02160432 0.         0.00238964 ... 0.         0.03270011 0.01829128]\n",
            "[0.04167442 0.         0.00823497 ... 0.         0.01039178 0.02024985]\n",
            "[0.0264501  0.         0.05513189 ... 0.01324194 0.0028067  0.02192087]\n",
            "[0.00592875 0.         0.         ... 0.         0.0036668  0.04101306]\n",
            "[0.0546109  0.         0.04577173 ... 0.00877607 0.         0.00493077]\n",
            "[0.00541042 0.01683906 0.04449337 ... 0.         0.02353319 0.017979  ]\n",
            "[0.08305394 0.         0.07596244 ... 0.         0.00733309 0.02081158]\n",
            "[0.00387132 0.         0.01056988 ... 0.         0.01707689 0.02849261]\n",
            "[0.         0.         0.03971162 ... 0.         0.00842235 0.02031129]\n",
            "[0.         0.         0.00827606 ... 0.         0.         0.01389867]\n",
            "[0.03713074 0.         0.0524836  ... 0.         0.         0.01682168]\n",
            "[0.         0.         0.02043329 ... 0.         0.01887003 0.03311637]\n",
            "[0.0281094  0.         0.01935472 ... 0.         0.01882476 0.01981032]\n",
            "[0.01817363 0.         0.03585138 ... 0.09094594 0.         0.        ]\n",
            "[0.         0.         0.         ... 0.         0.04588864 0.        ]\n",
            "[0.0329507  0.         0.         ... 0.         0.00653354 0.        ]\n",
            "[0.06106513 0.         0.0190639  ... 0.01271575 0.         0.01120603]\n",
            "[0.01180927 0.         0.         ... 0.04371996 0.02648022 0.        ]\n",
            "[0.         0.         0.         ... 0.         0.01571358 0.00824756]\n",
            "[0.00872294 0.         0.01628811 ... 0.02287889 0.02265853 0.        ]\n",
            "[0.00287066 0.         0.         ... 0.         0.00493212 0.03092926]\n",
            "[0.03921797 0.         0.03485425 ... 0.00298851 0.0189442  0.01249392]\n",
            "[0.05034023 0.         0.         ... 0.02145164 0.         0.00035759]\n",
            "[0.        0.        0.        ... 0.        0.0172122 0.       ]\n",
            "[0.05456283 0.         0.06442782 ... 0.         0.03212506 0.02439392]\n",
            "[0.10011574 0.         0.01415259 ... 0.01650868 0.         0.00941179]\n",
            "[0.02534829 0.         0.         ... 0.         0.02586757 0.03667621]\n",
            "[0.01207137 0.         0.05584094 ... 0.00070435 0.00645363 0.0161165 ]\n",
            "[0.0273695  0.         0.02381421 ... 0.         0.         0.04237887]\n",
            "[0.01908408 0.         0.06197788 ... 0.02153911 0.         0.00735407]\n",
            "[0.06096233 0.         0.17646992 ... 0.         0.00501697 0.02417862]\n",
            "[0.03031772 0.         0.         ... 0.         0.         0.03685243]\n",
            "[0.0161506  0.         0.         ... 0.         0.00918541 0.        ]\n",
            "[0.01302022 0.         0.02604075 ... 0.02515278 0.01539197 0.01919353]\n",
            "[0.01986793 0.         0.         ... 0.         0.         0.00508339]\n",
            "[0.01613768 0.         0.0106147  ... 0.         0.00841034 0.01812228]\n",
            "[0.15486881 0.         0.         ... 0.01634412 0.         0.        ]\n",
            "[0.01981451 0.         0.00151529 ... 0.         0.         0.00246712]\n",
            "[0.04494622 0.         0.04075374 ... 0.         0.         0.00284143]\n",
            "[0.         0.         0.         ... 0.         0.01946606 0.02715731]\n",
            "[0.06504534 0.         0.05425583 ... 0.         0.         0.00830844]\n",
            "[0.         0.         0.         ... 0.01159712 0.00623921 0.00561332]\n",
            "[0.00815349 0.         0.         ... 0.         0.         0.02827167]\n",
            "[0.01937865 0.         0.         ... 0.         0.02465645 0.        ]\n",
            "[0.01608262 0.         0.         ... 0.0097806  0.         0.05543784]\n",
            "[0.01088654 0.         0.         ... 0.00766145 0.         0.00832766]\n",
            "[0.0202572  0.         0.03917966 ... 0.         0.02318826 0.00512298]\n",
            "[0.02733448 0.         0.03414948 ... 0.02410158 0.         0.00545148]\n",
            "[0.00406091 0.         0.         ... 0.         0.04991772 0.00387683]\n",
            "[0.02424027 0.         0.01325167 ... 0.         0.         0.05401356]\n",
            "[0.00556263 0.         0.00662452 ... 0.         0.         0.01178954]\n",
            "[0.0420806  0.         0.02059857 ... 0.         0.0630495  0.12683682]\n",
            "[0.02544167 0.         0.04069626 ... 0.0312716  0.         0.        ]\n",
            "[0.         0.         0.         ... 0.01067493 0.         0.00978282]\n",
            "[0.         0.         0.         ... 0.         0.00894721 0.02710042]\n",
            "[0.02824558 0.         0.03980035 ... 0.06548882 0.00475983 0.        ]\n",
            "[0.01878639 0.01731727 0.05641023 ... 0.         0.01662828 0.02094373]\n",
            "[0.01763493 0.         0.02669388 ... 0.         0.0202297  0.03202713]\n",
            "[0.05413127 0.         0.         ... 0.         0.01562841 0.03292376]\n",
            "[0.0494098  0.         0.00269506 ... 0.02201922 0.02212195 0.        ]\n",
            "[0.00561404 0.         0.01578497 ... 0.         0.01285986 0.01775395]\n",
            "[0.09053072 0.         0.07715586 ... 0.         0.01015262 0.        ]\n",
            "[0.03395234 0.         0.01934754 ... 0.         0.00957151 0.03180169]\n",
            "[0.         0.         0.05895447 ... 0.02856    0.         0.        ]\n",
            "[0.01658997 0.         0.         ... 0.02895785 0.05134712 0.        ]\n",
            "[0.00090698 0.         0.03442383 ... 0.00329121 0.         0.00316926]\n",
            "[0.01903404 0.         0.         ... 0.         0.         0.        ]\n",
            "[0.00977595 0.         0.         ... 0.         0.0189244  0.        ]\n",
            "[0.         0.         0.         ... 0.         0.01872745 0.00881012]\n",
            "[0.00130863 0.         0.05268558 ... 0.         0.00031222 0.00064051]\n",
            "[0.01029988 0.         0.01443024 ... 0.0110559  0.00685795 0.02054773]\n",
            "[0.02290939 0.         0.01949588 ... 0.00853302 0.         0.        ]\n",
            "[0.         0.         0.         ... 0.         0.0251752  0.01238646]\n",
            "[0.02631606 0.         0.00111554 ... 0.         0.01291171 0.02462921]\n",
            "[0.04612001 0.         0.0332527  ... 0.04110891 0.         0.        ]\n",
            "[0.02045361 0.         0.         ... 0.02017194 0.01856228 0.        ]\n",
            "[0.02344232 0.         0.         ... 0.01171274 0.         0.        ]\n",
            "[0.01337152 0.         0.         ... 0.         0.03949264 0.01700458]\n",
            "[0.         0.         0.03872992 ... 0.         0.         0.        ]\n",
            "[0.06628618 0.         0.02193442 ... 0.         0.00826694 0.01857967]\n",
            "[0.05009132 0.         0.05020239 ... 0.         0.01122896 0.02017515]\n",
            "[0.02776828 0.         0.02837316 ... 0.         0.         0.06192888]\n",
            "[0.00750249 0.         0.04099512 ... 0.         0.         0.00450692]\n",
            "[0.03380159 0.         0.         ... 0.         0.02924261 0.        ]\n",
            "[0.         0.         0.04632598 ... 0.02668218 0.         0.        ]\n",
            "[0.0069066  0.         0.01337972 ... 0.         0.02993844 0.03924646]\n",
            "[0.02121455 0.         0.01982272 ... 0.         0.         0.03386411]\n",
            "[0.00979767 0.         0.         ... 0.         0.02532334 0.02298265]\n",
            "[0.03676795 0.         0.06763291 ... 0.         0.0187361  0.01429662]\n",
            "[0.04906013 0.         0.04019697 ... 0.         0.01016948 0.03927539]\n",
            "[0.         0.         0.06053547 ... 0.03820995 0.         0.        ]\n",
            "[0.05138538 0.         0.         ... 0.         0.         0.        ]\n",
            "[0.00577836 0.         0.         ... 0.12774548 0.16487207 0.        ]\n",
            "[0.01018727 0.         0.00486046 ... 0.         0.         0.0682854 ]\n",
            "[0.00095951 0.         0.06073928 ... 0.         0.01315786 0.02761148]\n",
            "[0.06476961 0.         0.04546735 ... 0.         0.0249339  0.        ]\n",
            "[0.02075612 0.         0.01433679 ... 0.         0.         0.01044505]\n",
            "[0.01925255 0.         0.00921738 ... 0.01195926 0.01415193 0.        ]\n",
            "[0.02676802 0.         0.02441519 ... 0.11461452 0.04196643 0.05674665]\n",
            "[0.01470936 0.         0.03693815 ... 0.         0.0053405  0.02341842]\n",
            "[0.03712017 0.         0.03912461 ... 0.         0.         0.03063076]\n",
            "[0.02780405 0.         0.0261923  ... 0.         0.         0.04658634]\n",
            "[0.         0.         0.07590067 ... 0.         0.05480747 0.        ]\n",
            "[0.04403766 0.         0.07876477 ... 0.         0.04849903 0.03171191]\n",
            "[0.0156808  0.         0.02198143 ... 0.0158324  0.         0.00158657]\n",
            "[0.         0.         0.         ... 0.         0.         0.03613229]\n",
            "[0.01685303 0.         0.         ... 0.         0.00676605 0.0105032 ]\n",
            "[0.         0.         0.01374955 ... 0.         0.00778754 0.00616567]\n",
            "[0.04398501 0.         0.03042394 ... 0.00640732 0.00044783 0.        ]\n",
            "[0.         0.         0.         ... 0.         0.00374037 0.0064164 ]\n",
            "[0.03686294 0.         0.0312672  ... 0.         0.         0.04684773]\n",
            "[0.01523246 0.         0.00364126 ... 0.00651994 0.04923081 0.        ]\n",
            "[0.01517252 0.         0.04404831 ... 0.         0.         0.03235473]\n",
            "[0.05175441 0.         0.03330164 ... 0.0195087  0.03958657 0.02290933]\n",
            "[0.03542838 0.         0.0101089  ... 0.         0.10646248 0.        ]\n",
            "[0.02425664 0.         0.         ... 0.         0.00076602 0.00182727]\n",
            "[0.02379285 0.         0.         ... 0.00319091 0.         0.0031556 ]\n",
            "[0.00150943 0.         0.03124    ... 0.         0.         0.        ]\n",
            "[0.         0.         0.         ... 0.         0.00953026 0.        ]\n",
            "[0.         0.         0.03863643 ... 0.         0.         0.016269  ]\n",
            "[0.08511925 0.         0.01465238 ... 0.         0.02342273 0.02410582]\n",
            "[0.14366227 0.         0.17556414 ... 0.0528503  0.06021139 0.        ]\n",
            "[0.02461795 0.         0.01376404 ... 0.         0.         0.05740502]\n",
            "[0.01417852 0.         0.0238101  ... 0.         0.01595934 0.01992056]\n",
            "[0.         0.         0.03968648 ... 0.         0.02033293 0.0115725 ]\n",
            "[0.04218094 0.         0.03566211 ... 0.         0.00874712 0.        ]\n",
            "[0.0123555  0.         0.         ... 0.01755642 0.0043269  0.01195756]\n",
            "[0.0017421  0.         0.         ... 0.         0.02359868 0.01116483]\n",
            "[0.01211253 0.         0.04272701 ... 0.         0.00604188 0.        ]\n",
            "[0.0486707  0.         0.02981563 ... 0.         0.         0.0130724 ]\n",
            "[0.02160983 0.         0.00702691 ... 0.         0.01343819 0.        ]\n",
            "[0.06196589 0.         0.         ... 0.00061921 0.         0.00074717]\n",
            "[0.02477437 0.         0.01267113 ... 0.         0.         0.02443136]\n",
            "[0.00643462 0.         0.         ... 0.         0.02751488 0.06563296]\n",
            "[0.0282714  0.         0.04510571 ... 0.         0.01591577 0.02354555]\n",
            "[0.         0.         0.06597841 ... 0.         0.01758391 0.03762879]\n",
            "[0.04056057 0.         0.07530426 ... 0.         0.05045594 0.        ]\n",
            "[0.02599858 0.         0.         ... 0.         0.02274963 0.01267209]\n",
            "[0.02094459 0.         0.06424572 ... 0.         0.00057144 0.00536278]\n",
            "[0.00911812 0.         0.0164877  ... 0.         0.         0.        ]\n",
            "[0.03169618 0.         0.         ... 0.         0.029058   0.03637337]\n",
            "[0.06642033 0.         0.02707847 ... 0.         0.         0.06551056]\n",
            "[0.         0.         0.00787012 ... 0.02815068 0.         0.01852888]\n",
            "[0.03577844 0.         0.         ... 0.         0.03421685 0.02170212]\n",
            "[0.01821276 0.         0.06744662 ... 0.04266193 0.         0.        ]\n",
            "[0.         0.         0.         ... 0.01911861 0.         0.01432394]\n",
            "[0.01083609 0.         0.         ... 0.         0.0183577  0.01299314]\n",
            "[0.05968973 0.         0.05273461 ... 0.         0.03114075 0.0284461 ]\n",
            "[0.01335823 0.         0.01936522 ... 0.0143905  0.         0.01393793]\n",
            "[0.00486357 0.         0.         ... 0.         0.04528583 0.01528834]\n",
            "[0.00998056 0.         0.03636318 ... 0.03203826 0.05954814 0.        ]\n",
            "[0.01084231 0.         0.0181575  ... 0.         0.00977973 0.        ]\n",
            "[0.         0.         0.01908671 ... 0.         0.01183975 0.00625766]\n",
            "[0.07288403 0.         0.04814524 ... 0.00559294 0.04158313 0.03227574]\n",
            "[0.         0.         0.01042706 ... 0.00260166 0.01165236 0.        ]\n",
            "[0.01904767 0.         0.02727132 ... 0.         0.         0.06983504]\n",
            "[0.07051919 0.         0.02775039 ... 0.05374311 0.         0.        ]\n",
            "[0.02981235 0.         0.04751161 ... 0.         0.01573711 0.00875037]\n",
            "[0.03709409 0.         0.0358091  ... 0.         0.05465985 0.00166207]\n",
            "[0.07681216 0.         0.03253013 ... 0.         0.         0.01420362]\n",
            "[0.         0.         0.02813943 ... 0.         0.01736421 0.01334187]\n",
            "[0.02562751 0.         0.03925825 ... 0.         0.10347769 0.13818713]\n",
            "[0.         0.         0.         ... 0.         0.0660085  0.00223077]\n",
            "[0.01242944 0.         0.06368364 ... 0.0296938  0.         0.03426418]\n",
            "[0.00227138 0.         0.02936188 ... 0.         0.0196474  0.02472726]\n",
            "[0.         0.         0.107438   ... 0.         0.         0.00055401]\n",
            "[0.00533192 0.         0.         ... 0.         0.         0.01436153]\n",
            "[0.         0.         0.         ... 0.00016861 0.01384168 0.        ]\n",
            "[0.01528307 0.         0.         ... 0.         0.00633849 0.05351022]\n",
            "[0.09806021 0.         0.00718872 ... 0.         0.         0.02074349]\n",
            "[0.01310702 0.         0.01310125 ... 0.         0.00189174 0.03107855]\n",
            "[0.03430204 0.         0.04137512 ... 0.         0.         0.07490242]\n",
            "[0.01780717 0.         0.03200669 ... 0.         0.         0.        ]\n",
            "[0.03667384 0.         0.         ... 0.         0.         0.02525538]\n",
            "[0.02784122 0.         0.01104652 ... 0.         0.         0.07566206]\n",
            "[0.02220942 0.         0.         ... 0.         0.         0.00411833]\n",
            "[0.04150303 0.         0.02108875 ... 0.         0.         0.01518332]\n",
            "[0.00711759 0.         0.         ... 0.         0.05107029 0.        ]\n",
            "[0.01902612 0.         0.02045238 ... 0.         0.04415692 0.01996882]\n",
            "[0.07686717 0.         0.01873942 ... 0.         0.00609459 0.01739209]\n",
            "[0.04555644 0.         0.05110098 ... 0.         0.         0.        ]\n",
            "[0.         0.         0.02760388 ... 0.         0.01941288 0.01055028]\n",
            "[0.01704332 0.         0.01572453 ... 0.         0.02923068 0.02100705]\n",
            "[0.02409224 0.         0.06933599 ... 0.         0.02618606 0.03266309]\n",
            "[0.         0.         0.04759179 ... 0.         0.         0.04001036]\n",
            "[0.01603167 0.         0.         ... 0.         0.01961826 0.03656138]\n",
            "[0.0087697  0.01798616 0.02666622 ... 0.00380419 0.         0.00419467]\n",
            "[0.         0.         0.         ... 0.01836355 0.02457711 0.        ]\n",
            "[0.00264407 0.         0.01759922 ... 0.00216466 0.03340209 0.01355772]\n",
            "[0.02185441 0.         0.01628545 ... 0.         0.009993   0.01757675]\n",
            "[0.03684964 0.         0.0419652  ... 0.         0.01659225 0.01572197]\n",
            "[0.02669015 0.         0.0312502  ... 0.         0.01366324 0.02342924]\n",
            "[0.00951214 0.         0.0197684  ... 0.         0.02998288 0.03470701]\n",
            "[0.01876515 0.         0.06691442 ... 0.         0.01227602 0.        ]\n",
            "[0.01222536 0.         0.01946588 ... 0.01436892 0.         0.00046705]\n",
            "[0.00081419 0.         0.         ... 0.         0.01694769 0.0078781 ]\n",
            "[0.0517093  0.         0.06561907 ... 0.         0.         0.06920026]\n",
            "[0.03737197 0.         0.00319589 ... 0.         0.         0.0532384 ]\n",
            "[0.08157392 0.         0.01214673 ... 0.         0.02037652 0.02425771]\n",
            "[0.05240062 0.         0.07987957 ... 0.         0.00886864 0.05067249]\n",
            "[0.06111144 0.         0.03384409 ... 0.         0.         0.01541448]\n",
            "[0.         0.         0.02170504 ... 0.02383878 0.         0.00375508]\n",
            "[0.00336487 0.         0.00397463 ... 0.01755755 0.04112507 0.        ]\n",
            "[0.07824761 0.         0.02778552 ... 0.         0.         0.04967343]\n",
            "[0.00991023 0.         0.         ... 0.         0.05743932 0.        ]\n",
            "[0.         0.         0.04097289 ... 0.         0.01381644 0.01717604]\n",
            "[0.02251221 0.         0.         ... 0.         0.02136572 0.02301485]\n",
            "[0.03614128 0.         0.05761099 ... 0.         0.04231169 0.        ]\n",
            "[0.04607297 0.         0.02743782 ... 0.         0.         0.02667458]\n",
            "[0.0128391  0.         0.00339659 ... 0.00175657 0.01514887 0.        ]\n",
            "[0.01335325 0.         0.01367166 ... 0.         0.00489081 0.03978428]\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AAPL.csv\n",
            "[0.01146383 0.         0.06350367 ... 0.         0.         0.02786175]\n",
            "[0.01149759 0.         0.         ... 0.         0.01189811 0.        ]\n",
            "[0.05225091 0.         0.03375757 ... 0.         0.01385402 0.01636475]\n",
            "[0.04691485 0.         0.04242539 ... 0.         0.01433781 0.02696579]\n",
            "[0.01477476 0.         0.02355579 ... 0.         0.         0.00146262]\n",
            "[0.05327479 0.         0.02167445 ... 0.         0.         0.        ]\n",
            "[0.0021928  0.         0.0043436  ... 0.         0.00128685 0.01933449]\n",
            "[0.04801124 0.         0.02967048 ... 0.00280443 0.         0.        ]\n",
            "[0.03034114 0.         0.         ... 0.02012099 0.         0.01142795]\n",
            "[0.0153071  0.         0.         ... 0.03321892 0.         0.        ]\n",
            "[0.02738561 0.         0.03683434 ... 0.02313738 0.         0.00032912]\n",
            "[0.04915681 0.22370513 0.         ... 0.00317115 0.         0.03024298]\n",
            "[0.03646562 0.         0.04527417 ... 0.         0.00392241 0.02940872]\n",
            "[0.07333788 0.         0.05548294 ... 0.01924945 0.         0.00074724]\n",
            "[0.0063535  0.         0.         ... 0.         0.00071618 0.        ]\n",
            "[0.01152978 0.         0.         ... 0.01417664 0.         0.02679821]\n",
            "[0.01000124 0.         0.00408799 ... 0.01928779 0.01113134 0.        ]\n",
            "[0.01741982 0.         0.011543   ... 0.         0.00175243 0.03744774]\n",
            "[0.02540374 0.         0.05826969 ... 0.         0.         0.        ]\n",
            "[0.         0.         0.01647599 ... 0.         0.         0.02194128]\n",
            "[0.03242398 0.         0.         ... 0.         0.00787627 0.01125674]\n",
            "[0.0373374  0.         0.06046296 ... 0.         0.0173111  0.01403163]\n",
            "[0.         0.         0.         ... 0.07822526 0.06166431 0.        ]\n",
            "[0.02557666 0.         0.00124541 ... 0.01334292 0.         0.        ]\n",
            "[0.00638963 0.         0.         ... 0.         0.03292673 0.        ]\n",
            "[0.02660638 0.         0.         ... 0.02360938 0.         0.        ]\n",
            "[0.         0.         0.         ... 0.         0.02188338 0.00969425]\n",
            "[0.03863481 0.         0.03482354 ... 0.         0.         0.02752543]\n",
            "[0.03487697 0.         0.02872977 ... 0.         0.         0.        ]\n",
            "[0.01806534 0.         0.01209724 ... 0.         0.01586152 0.02679521]\n",
            "[0.04685846 0.         0.03810712 ... 0.         0.00749358 0.02859906]\n",
            "[0.04770282 0.         0.03416815 ... 0.         0.00385218 0.01080394]\n",
            "[0.         0.         0.03862585 ... 0.06341102 0.02399666 0.00469243]\n",
            "[0.09280751 0.         0.03974205 ... 0.         0.01738092 0.01929045]\n",
            "[0.00267418 0.         0.0149343  ... 0.         0.         0.00047295]\n",
            "[0.00841252 0.         0.         ... 0.         0.         0.03456545]\n",
            "[0.01843097 0.         0.         ... 0.11238498 0.         0.        ]\n",
            "[0.0476548 0.        0.        ... 0.        0.        0.       ]\n",
            "[0.01787563 0.         0.         ... 0.         0.         0.01727494]\n",
            "[0.00911378 0.         0.         ... 0.         0.02841089 0.        ]\n",
            "[0.02366251 0.         0.00573609 ... 0.03536507 0.         0.        ]\n",
            "[0.02369043 0.         0.03948246 ... 0.         0.03253796 0.05004992]\n",
            "[0.00861293 0.         0.         ... 0.02249015 0.         0.        ]\n",
            "[0.         0.         0.         ... 0.00481036 0.01913797 0.08850263]\n",
            "[0.         0.         0.02991934 ... 0.         0.00226567 0.        ]\n",
            "[0.03728379 0.         0.02402226 ... 0.00268332 0.         0.        ]\n",
            "[0.00180677 0.         0.01891073 ... 0.         0.00302201 0.0218269 ]\n",
            "[0.01179531 0.         0.0330775  ... 0.         0.02064208 0.        ]\n",
            "[0.03410661 0.         0.02318545 ... 0.         0.00433885 0.02374694]\n",
            "[0.02005723 0.         0.0589229  ... 0.02758054 0.         0.01069092]\n",
            "[0.0243612  0.0099522  0.02356779 ... 0.         0.05286396 0.03577047]\n",
            "[0.00057152 0.         0.03862415 ... 0.         0.01467793 0.01493244]\n",
            "[0.03598636 0.         0.02059992 ... 0.         0.01494838 0.05303331]\n",
            "[0.02646581 0.         0.03108174 ... 0.         0.02136335 0.        ]\n",
            "[0.01293094 0.         0.         ... 0.         0.00942195 0.        ]\n",
            "[0.0454023  0.         0.03304471 ... 0.         0.21102059 0.12431385]\n",
            "[0.03173437 0.         0.01977345 ... 0.         0.02417562 0.01043612]\n",
            "[0.05470956 0.         0.03563274 ... 0.         0.03629642 0.01004627]\n",
            "[0.01699783 0.         0.03639168 ... 0.01776386 0.         0.        ]\n",
            "[0.09863628 0.         0.08740339 ... 0.         0.01374682 0.04765174]\n",
            "[0.01829936 0.         0.0332786  ... 0.         0.00259067 0.04827265]\n",
            "[0.         0.         0.00252932 ... 0.         0.01544882 0.01210354]\n",
            "[0.04770474 0.         0.13601501 ... 0.         0.01865133 0.        ]\n",
            "[0.04116196 0.         0.03210953 ... 0.         0.01694346 0.0412706 ]\n",
            "[0.06407974 0.         0.02319946 ... 0.         0.01792786 0.01996067]\n",
            "[0.04849358 0.         0.04715814 ... 0.         0.00944815 0.0076781 ]\n",
            "[0.00752936 0.         0.01957627 ... 0.         0.00248535 0.01753115]\n",
            "[0.         0.00626802 0.04149278 ... 0.         0.01606156 0.03793444]\n",
            "[0.03447483 0.         0.05503355 ... 0.         0.         0.02932414]\n",
            "[0.00919168 0.         0.00305636 ... 0.6253895  0.         0.        ]\n",
            "[0.02436204 0.         0.05545722 ... 0.02704848 0.         0.        ]\n",
            "[0.00667903 0.         0.         ... 0.         0.         0.00522104]\n",
            "[0.01067216 0.         0.00868886 ... 0.         0.         0.04812699]\n",
            "[0.00262926 0.         0.         ... 0.         0.00112598 0.00779396]\n",
            "[0.02327765 0.         0.01766879 ... 0.         0.         0.01935077]\n",
            "[0.02560622 0.         0.00448873 ... 0.         0.01391634 0.01378522]\n",
            "[0.05069305 0.         0.03506082 ... 0.         0.         0.02411814]\n",
            "[0.03087425 0.         0.01647157 ... 0.         0.00945051 0.08396055]\n",
            "[0.02511087 0.         0.         ... 0.         0.01923528 0.03657846]\n",
            "[0.02741435 0.         0.07237168 ... 0.         0.         0.00183527]\n",
            "[0.03359342 0.         0.01896401 ... 0.         0.         0.03916289]\n",
            "[0.       0.       0.       ... 0.       0.009299 0.      ]\n",
            "[0.02442591 0.         0.         ... 0.02067395 0.01457328 0.        ]\n",
            "[0.         0.         0.01232046 ... 0.         0.01693477 0.1452141 ]\n",
            "[0.00986743 0.         0.02083878 ... 0.01022913 0.         0.01187987]\n",
            "[0.01516714 0.         0.         ... 0.         0.01156213 0.        ]\n",
            "[0.00399004 0.         0.00531962 ... 0.00175268 0.00272026 0.00750275]\n",
            "[0.03939056 0.         0.02855126 ... 0.01299652 0.         0.01022448]\n",
            "[0.02857863 0.         0.04351543 ... 0.         0.02229033 0.01190551]\n",
            "[3.9612412e-02 0.0000000e+00 6.8756782e-02 ... 7.6465537e-03 7.5364478e-05\n",
            " 1.5045108e-02]\n",
            "[0.00785612 0.         0.00493806 ... 0.         0.00850222 0.0256843 ]\n",
            "[0.03060693 0.         0.03951436 ... 0.         0.         0.        ]\n",
            "[0.01090962 0.         0.         ... 0.         0.01986085 0.01373829]\n",
            "[0.05106937 0.         0.02110493 ... 0.         0.01641227 0.02948445]\n",
            "[0.01987385 0.         0.         ... 0.         0.         0.02469543]\n",
            "[0.06020167 0.         0.         ... 0.02057048 0.         0.        ]\n",
            "[0.01906173 0.         0.03408056 ... 0.         0.02000717 0.02616411]\n",
            "[0.         0.         0.07692695 ... 0.         0.01984931 0.0452653 ]\n",
            "[0.05214159 0.         0.04596562 ... 0.         0.00101011 0.02094259]\n",
            "[0.03156199 0.         0.02486791 ... 0.0125321  0.         0.        ]\n",
            "[0.0091159  0.         0.         ... 0.         0.01437566 0.00161827]\n",
            "[0.06806869 0.         0.04227637 ... 0.         0.01742897 0.04321893]\n",
            "[0.01737578 0.         0.05361851 ... 0.         0.         0.        ]\n",
            "[0.07127926 0.         0.02268276 ... 0.05124416 0.07170796 0.        ]\n",
            "[0.02633713 0.         0.02047893 ... 0.         0.         0.03876675]\n",
            "[0.01020369 0.         0.         ... 0.01580416 0.00509609 0.        ]\n",
            "[0.00569597 0.         0.         ... 0.0585163  0.06235906 0.        ]\n",
            "[0.0071502  0.         0.         ... 0.         0.         0.02705843]\n",
            "[0.         0.         0.04456166 ... 0.         0.00580411 0.01815948]\n",
            "[0.01647031 0.         0.01860983 ... 0.         0.         0.04733704]\n",
            "[0.00968735 0.         0.01272919 ... 0.02291047 0.         0.        ]\n",
            "[0.       0.       0.       ... 0.       0.000586 0.      ]\n",
            "[0.03573078 0.         0.         ... 0.         0.01099007 0.01543781]\n",
            "[0.04784025 0.         0.05751191 ... 0.03400581 0.         0.        ]\n",
            "[0.01247875 0.         0.01647493 ... 0.02720534 0.01244507 0.        ]\n",
            "[0.02816796 0.         0.07104119 ... 0.         0.         0.01326169]\n",
            "[0.         0.         0.         ... 0.         0.03374625 0.02844546]\n",
            "[0.01587123 0.         0.03645783 ... 0.         0.04265554 0.00355959]\n",
            "[0.04885468 0.00208155 0.03047684 ... 0.         0.03468746 0.00701393]\n",
            "[0.10914848 0.         0.         ... 0.         0.01782827 0.        ]\n",
            "[0.         0.         0.00024502 ... 0.         0.00036104 0.06743185]\n",
            "[0.0031517  0.         0.02778248 ... 0.         0.         0.02438352]\n",
            "[0.04527878 0.         0.         ... 0.         0.02503919 0.01517856]\n",
            "[0.0362579  0.         0.06599494 ... 0.         0.02692348 0.        ]\n",
            "[0.00434544 0.         0.02447897 ... 0.         0.02678899 0.03080748]\n",
            "[0.053745   0.         0.04324755 ... 0.         0.00748554 0.0215166 ]\n",
            "[0.         0.         0.02590831 ... 0.         0.00330695 0.03482275]\n",
            "[0.06270395 0.         0.06320254 ... 0.         0.         0.03216867]\n",
            "[0.07150101 0.         0.03001158 ... 0.         0.05610526 0.00099785]\n",
            "[0.05888148 0.         0.01893453 ... 0.04960471 0.02467493 0.        ]\n",
            "[0.05602319 0.         0.00286573 ... 0.01480941 0.         0.        ]\n",
            "[0.0096476  0.         0.         ... 0.         0.08140045 0.        ]\n",
            "[0.00668899 0.         0.00319753 ... 0.         0.00827938 0.04870152]\n",
            "[0.         0.         0.06860169 ... 0.         0.00292205 0.02946443]\n",
            "[0.02014155 0.         0.03582316 ... 0.02197981 0.02048041 0.        ]\n",
            "[0.         0.         0.         ... 0.         0.         0.01270804]\n",
            "[0.0174198  0.         0.00895441 ... 0.01201276 0.         0.01197836]\n",
            "[0.01087749 0.         0.08048919 ... 0.         0.         0.03085423]\n",
            "[0.01787171 0.         0.         ... 0.         0.01420576 0.01765955]\n",
            "[0.01744836 0.         0.02933604 ... 0.         0.         0.03270859]\n",
            "[0.0196903  0.         0.02969815 ... 0.         0.         0.01118036]\n",
            "[0.01639894 0.         0.01401161 ... 0.         0.0547975  0.        ]\n",
            "[0.01103859 0.         0.07432772 ... 0.         0.02416245 0.        ]\n",
            "[0.03284923 0.         0.02426719 ... 0.         0.         0.00857584]\n",
            "[0.02043529 0.         0.07250071 ... 0.05238705 0.01149624 0.01130369]\n",
            "[0.01918135 0.         0.         ... 0.         0.02463805 0.        ]\n",
            "[0.00703376 0.         0.04289199 ... 0.         0.00658736 0.00251815]\n",
            "[0.02166874 0.         0.05880818 ... 0.00467409 0.         0.00442148]\n",
            "[0.         0.         0.         ... 0.05341747 0.04756927 0.00336739]\n",
            "[0.01275044 0.         0.033689   ... 0.         0.0243108  0.03853539]\n",
            "[0.08300696 0.         0.06751304 ... 0.00347858 0.0224728  0.        ]\n",
            "[0.06434632 0.         0.02357264 ... 0.01599712 0.00312751 0.0193977 ]\n",
            "[0.07659694 0.         0.02776517 ... 0.02115697 0.         0.        ]\n",
            "[0.         0.         0.01023917 ... 0.01230787 0.         0.0189568 ]\n",
            "[0.02795636 0.         0.01209179 ... 0.         0.02412885 0.01897517]\n",
            "[0.         0.         0.02405839 ... 0.         0.00174272 0.04307966]\n",
            "[0.04823538 0.         0.04262261 ... 0.         0.         0.02341812]\n",
            "[0.05149346 0.         0.05297683 ... 0.         0.0228755  0.02503202]\n",
            "[0.0250263  0.         0.03732691 ... 0.         0.00386369 0.04676916]\n",
            "[0.02545305 0.         0.01219013 ... 0.0078487  0.04130351 0.        ]\n",
            "[0.         0.         0.         ... 0.         0.03248493 0.        ]\n",
            "[0.0559209  0.         0.0204931  ... 0.         0.00945896 0.02943683]\n",
            "[0.01963069 0.         0.02091757 ... 0.0020931  0.         0.00531833]\n",
            "[0.         0.00050224 0.01597012 ... 0.         0.01798789 0.04444467]\n",
            "[0.02542624 0.         0.04480236 ... 0.00870773 0.         0.        ]\n",
            "[0.00162547 0.         0.         ... 0.         0.01676946 0.02565628]\n",
            "[0.01026241 0.         0.         ... 0.         0.0404442  0.00522849]\n",
            "[0.01318894 0.         0.01135347 ... 0.         0.03774828 0.01039342]\n",
            "[0.01729298 0.         0.06789724 ... 0.01803958 0.         0.00251514]\n",
            "[0.01337534 0.         0.03141376 ... 0.01420568 0.         0.00271673]\n",
            "[0.0371738  0.         0.         ... 0.         0.         0.02241135]\n",
            "[0.04805255 0.         0.02375326 ... 0.         0.00594712 0.        ]\n",
            "[0.01691422 0.         0.00364515 ... 0.00076125 0.         0.0099933 ]\n",
            "[0.00995327 0.         0.         ... 0.00244759 0.         0.00927475]\n",
            "[0.02873012 0.         0.         ... 0.         0.         0.08374023]\n",
            "[0.01273118 0.         0.01875207 ... 0.         0.01794355 0.02407452]\n",
            "[0.02704236 0.         0.05616629 ... 0.00863645 0.03405377 0.        ]\n",
            "[0.01205669 0.         0.         ... 0.         0.04150751 0.07079556]\n",
            "[0.01035079 0.         0.0491557  ... 0.         0.         0.04184373]\n",
            "[0.05011509 0.         0.10571219 ... 0.         0.03336497 0.03375823]\n",
            "[0.08159982 0.         0.05920709 ... 0.         0.         0.        ]\n",
            "[0.         0.         0.01312564 ... 0.00905734 0.         0.        ]\n",
            "[0.01170027 0.         0.05923089 ... 0.00934517 0.07819778 0.00135499]\n",
            "[0.03049691 0.         0.02325227 ... 0.         0.         0.        ]\n",
            "[0.00926892 0.         0.         ... 0.         0.02938912 0.02920052]\n",
            "[0.09641589 0.         0.03426075 ... 0.0186465  0.         0.        ]\n",
            "[0.00147428 0.         0.         ... 0.         0.02240575 0.00664225]\n",
            "[0.01833867 0.         0.01275131 ... 0.00633827 0.         0.        ]\n",
            "[0.05438004 0.         0.03719785 ... 0.         0.01600906 0.04593163]\n",
            "[0.07860286 0.         0.02528056 ... 0.         0.0325969  0.07185465]\n",
            "[0.01387924 0.         0.03142842 ... 0.         0.         0.        ]\n",
            "[0.         0.         0.00525584 ... 0.         0.00935434 0.02042551]\n",
            "[0.08258209 0.         0.05735731 ... 0.         0.         0.02582543]\n",
            "[0.         0.         0.01327145 ... 0.         0.01863198 0.        ]\n",
            "[0.007384   0.         0.04519038 ... 0.         0.00326097 0.02856142]\n",
            "[0.06035341 0.         0.05211063 ... 0.         0.         0.        ]\n",
            "[0.00510138 0.00207019 0.06608419 ... 0.         0.00581754 0.00823286]\n",
            "[0.01464228 0.         0.03065132 ... 0.         0.02002661 0.        ]\n",
            "[0.05101571 0.         0.00123483 ... 0.02026843 0.02092637 0.00212351]\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "[0.00830235 0.         0.         ... 0.01211706 0.03405437 0.        ]\n",
            "[0.         0.         0.         ... 0.         0.09346437 0.0289403 ]\n",
            "[0.00819724 0.         0.04983412 ... 0.01215971 0.         0.00195487]\n",
            "[0.         0.         0.02166284 ... 0.         0.01335913 0.03533376]\n",
            "[0.         0.         0.02874352 ... 0.         0.00128435 0.024255  ]\n",
            "[0.0019868  0.         0.         ... 0.00302347 0.0005124  0.00171291]\n",
            "[0.01530833 0.         0.         ... 0.00313256 0.01760116 0.00185664]\n",
            "[0.0466646  0.         0.         ... 0.         0.         0.04229993]\n",
            "[0.03314197 0.         0.02096726 ... 0.         0.         0.02614068]\n",
            "[0.         0.         0.02708267 ... 0.02615436 0.         0.00601204]\n",
            "[0.02122114 0.         0.         ... 0.         0.00838374 0.00477657]\n",
            "[0.         0.         0.05459138 ... 0.02631597 0.00817283 0.00175002]\n",
            "[0.0157997  0.         0.         ... 0.         0.01556693 0.02895399]\n",
            "[0.03918988 0.         0.03278473 ... 0.         0.         0.04825875]\n",
            "[0.0195004  0.         0.         ... 0.         0.         0.02693213]\n",
            "[0.01224664 0.         0.02801099 ... 0.         0.02987409 0.02996885]\n",
            "[0.02666644 0.         0.03710134 ... 0.         0.         0.00627531]\n",
            "[0.03106626 0.         0.05161072 ... 0.         0.00966027 0.        ]\n",
            "[0.05358423 0.         0.04517249 ... 0.         0.         0.04138222]\n",
            "[0.02625293 0.         0.04520035 ... 0.         0.01921367 0.02402524]\n",
            "[0.06292044 0.         0.1796135  ... 0.         0.01321171 0.00409255]\n",
            "[0.00347004 0.         0.02845057 ... 0.         0.         0.        ]\n",
            "[0.00923871 0.         0.         ... 0.         0.02341306 0.03978307]\n",
            "[0.06030478 0.         0.05542228 ... 0.00449563 0.         0.00870448]\n",
            "[0.00330244 0.         0.         ... 0.         0.02068589 0.04951934]\n",
            "[0.0102163  0.         0.02446584 ... 0.         0.00639712 0.02299238]\n",
            "[0.00462641 0.         0.03110694 ... 0.00671375 0.02110481 0.00280274]\n",
            "[0.04446025 0.         0.05368697 ... 0.         0.04704791 0.00421992]\n",
            "[0.0215892  0.00056734 0.07578114 ... 0.         0.01608511 0.02135985]\n",
            "[0.0507982  0.         0.03537752 ... 0.01713884 0.         0.01001193]\n",
            "[0.05902748 0.         0.01835041 ... 0.         0.00918256 0.04272408]\n",
            "[0.01570076 0.         0.         ... 0.02098102 0.         0.        ]\n",
            "[0.00182104 0.         0.         ... 0.         0.03987264 0.0248328 ]\n",
            "[0.06544055 0.         0.0495452  ... 0.03916381 0.         0.00124169]\n",
            "[0.00963566 0.         0.         ... 0.         0.00977695 0.        ]\n",
            "[0.022529   0.         0.00859899 ... 0.         0.         0.15288909]\n",
            "[0.         0.         0.05044074 ... 0.0118391  0.         0.01654438]\n",
            "[0.02029813 0.         0.00434164 ... 0.         0.00106244 0.02607713]\n",
            "[0.00821449 0.         0.04486218 ... 0.         0.         0.        ]\n",
            "[0.02618473 0.         0.02367129 ... 0.         0.         0.01744074]\n",
            "[0.         0.         0.         ... 0.         0.         0.00615102]\n",
            "[0.01639017 0.         0.         ... 0.01666133 0.         0.        ]\n",
            "[0.02011167 0.         0.         ... 0.         0.         0.00580388]\n",
            "[0.00090657 0.         0.         ... 0.         0.0544578  0.        ]\n",
            "[0.01918503 0.         0.         ... 0.         0.02280155 0.01810979]\n",
            "[0.07113001 0.         0.         ... 0.         0.01916966 0.02901005]\n",
            "[0.02765809 0.         0.04548378 ... 0.         0.00947601 0.03146039]\n",
            "[0.0829112  0.         0.02744785 ... 0.         0.         0.        ]\n",
            "[0.01617967 0.         0.00675321 ... 0.03937702 0.01321123 0.        ]\n",
            "[0.00466256 0.         0.01759823 ... 0.         0.         0.05023537]\n",
            "[0]\ttrain-mlogloss:0.687517\ttest-mlogloss:0.689974\n",
            "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until test-mlogloss hasn't improved in 100 rounds.\n",
            "[1]\ttrain-mlogloss:0.681814\ttest-mlogloss:0.687445\n",
            "[2]\ttrain-mlogloss:0.677472\ttest-mlogloss:0.685085\n",
            "[3]\ttrain-mlogloss:0.671069\ttest-mlogloss:0.682362\n",
            "[4]\ttrain-mlogloss:0.665495\ttest-mlogloss:0.680186\n",
            "[5]\ttrain-mlogloss:0.659572\ttest-mlogloss:0.67729\n",
            "[6]\ttrain-mlogloss:0.654022\ttest-mlogloss:0.674863\n",
            "[7]\ttrain-mlogloss:0.648595\ttest-mlogloss:0.672531\n",
            "[8]\ttrain-mlogloss:0.64482\ttest-mlogloss:0.669327\n",
            "[9]\ttrain-mlogloss:0.640558\ttest-mlogloss:0.667331\n",
            "[10]\ttrain-mlogloss:0.635307\ttest-mlogloss:0.66512\n",
            "[11]\ttrain-mlogloss:0.630093\ttest-mlogloss:0.662087\n",
            "[12]\ttrain-mlogloss:0.625782\ttest-mlogloss:0.660339\n",
            "[13]\ttrain-mlogloss:0.621976\ttest-mlogloss:0.658615\n",
            "[14]\ttrain-mlogloss:0.617293\ttest-mlogloss:0.656818\n",
            "[15]\ttrain-mlogloss:0.612705\ttest-mlogloss:0.654095\n",
            "[16]\ttrain-mlogloss:0.608533\ttest-mlogloss:0.652222\n",
            "[17]\ttrain-mlogloss:0.603944\ttest-mlogloss:0.649995\n",
            "[18]\ttrain-mlogloss:0.599779\ttest-mlogloss:0.648558\n",
            "[19]\ttrain-mlogloss:0.595175\ttest-mlogloss:0.6457\n",
            "[20]\ttrain-mlogloss:0.590959\ttest-mlogloss:0.6442\n",
            "[21]\ttrain-mlogloss:0.587186\ttest-mlogloss:0.640602\n",
            "[22]\ttrain-mlogloss:0.583855\ttest-mlogloss:0.639288\n",
            "[23]\ttrain-mlogloss:0.580124\ttest-mlogloss:0.637363\n",
            "[24]\ttrain-mlogloss:0.576458\ttest-mlogloss:0.636318\n",
            "[25]\ttrain-mlogloss:0.573096\ttest-mlogloss:0.634717\n",
            "[26]\ttrain-mlogloss:0.569265\ttest-mlogloss:0.632635\n",
            "[27]\ttrain-mlogloss:0.565733\ttest-mlogloss:0.631646\n",
            "[28]\ttrain-mlogloss:0.562024\ttest-mlogloss:0.629991\n",
            "[29]\ttrain-mlogloss:0.558685\ttest-mlogloss:0.628522\n",
            "[30]\ttrain-mlogloss:0.555519\ttest-mlogloss:0.627056\n",
            "[31]\ttrain-mlogloss:0.552172\ttest-mlogloss:0.625079\n",
            "[32]\ttrain-mlogloss:0.548496\ttest-mlogloss:0.623811\n",
            "[33]\ttrain-mlogloss:0.544746\ttest-mlogloss:0.622898\n",
            "[34]\ttrain-mlogloss:0.541049\ttest-mlogloss:0.62137\n",
            "[35]\ttrain-mlogloss:0.537321\ttest-mlogloss:0.620472\n",
            "[36]\ttrain-mlogloss:0.53391\ttest-mlogloss:0.618546\n",
            "[37]\ttrain-mlogloss:0.530813\ttest-mlogloss:0.61776\n",
            "[38]\ttrain-mlogloss:0.529123\ttest-mlogloss:0.615222\n",
            "[39]\ttrain-mlogloss:0.525353\ttest-mlogloss:0.613757\n",
            "[40]\ttrain-mlogloss:0.522448\ttest-mlogloss:0.612878\n",
            "[41]\ttrain-mlogloss:0.518942\ttest-mlogloss:0.612178\n",
            "[42]\ttrain-mlogloss:0.51549\ttest-mlogloss:0.610133\n",
            "[43]\ttrain-mlogloss:0.51261\ttest-mlogloss:0.608794\n",
            "[44]\ttrain-mlogloss:0.509647\ttest-mlogloss:0.607865\n",
            "[45]\ttrain-mlogloss:0.50711\ttest-mlogloss:0.607825\n",
            "[46]\ttrain-mlogloss:0.504175\ttest-mlogloss:0.606322\n",
            "[47]\ttrain-mlogloss:0.502119\ttest-mlogloss:0.605236\n",
            "[48]\ttrain-mlogloss:0.499838\ttest-mlogloss:0.604853\n",
            "[49]\ttrain-mlogloss:0.497555\ttest-mlogloss:0.604917\n",
            "[50]\ttrain-mlogloss:0.494387\ttest-mlogloss:0.603427\n",
            "[51]\ttrain-mlogloss:0.491682\ttest-mlogloss:0.601356\n",
            "[52]\ttrain-mlogloss:0.489209\ttest-mlogloss:0.600982\n",
            "[53]\ttrain-mlogloss:0.486433\ttest-mlogloss:0.599506\n",
            "[54]\ttrain-mlogloss:0.483734\ttest-mlogloss:0.598225\n",
            "[55]\ttrain-mlogloss:0.481044\ttest-mlogloss:0.597952\n",
            "[56]\ttrain-mlogloss:0.47881\ttest-mlogloss:0.596135\n",
            "[57]\ttrain-mlogloss:0.476266\ttest-mlogloss:0.594494\n",
            "[58]\ttrain-mlogloss:0.473649\ttest-mlogloss:0.593189\n",
            "[59]\ttrain-mlogloss:0.473013\ttest-mlogloss:0.592285\n",
            "[60]\ttrain-mlogloss:0.470573\ttest-mlogloss:0.592188\n",
            "[61]\ttrain-mlogloss:0.470156\ttest-mlogloss:0.592245\n",
            "[62]\ttrain-mlogloss:0.46773\ttest-mlogloss:0.590534\n",
            "[63]\ttrain-mlogloss:0.466063\ttest-mlogloss:0.59049\n",
            "[64]\ttrain-mlogloss:0.4643\ttest-mlogloss:0.591061\n",
            "[65]\ttrain-mlogloss:0.461951\ttest-mlogloss:0.589826\n",
            "[66]\ttrain-mlogloss:0.459516\ttest-mlogloss:0.589832\n",
            "[67]\ttrain-mlogloss:0.457743\ttest-mlogloss:0.589708\n",
            "[68]\ttrain-mlogloss:0.45569\ttest-mlogloss:0.589322\n",
            "[69]\ttrain-mlogloss:0.453345\ttest-mlogloss:0.58896\n",
            "[70]\ttrain-mlogloss:0.451076\ttest-mlogloss:0.588519\n",
            "[71]\ttrain-mlogloss:0.448964\ttest-mlogloss:0.589044\n",
            "[72]\ttrain-mlogloss:0.446987\ttest-mlogloss:0.587524\n",
            "[73]\ttrain-mlogloss:0.444687\ttest-mlogloss:0.586989\n",
            "[74]\ttrain-mlogloss:0.443089\ttest-mlogloss:0.586331\n",
            "[75]\ttrain-mlogloss:0.44137\ttest-mlogloss:0.586144\n",
            "[76]\ttrain-mlogloss:0.439455\ttest-mlogloss:0.585225\n",
            "[77]\ttrain-mlogloss:0.437034\ttest-mlogloss:0.584944\n",
            "[78]\ttrain-mlogloss:0.43467\ttest-mlogloss:0.58435\n",
            "[79]\ttrain-mlogloss:0.432572\ttest-mlogloss:0.584388\n",
            "[80]\ttrain-mlogloss:0.430557\ttest-mlogloss:0.584283\n",
            "[81]\ttrain-mlogloss:0.429036\ttest-mlogloss:0.584835\n",
            "[82]\ttrain-mlogloss:0.427366\ttest-mlogloss:0.583846\n",
            "[83]\ttrain-mlogloss:0.425565\ttest-mlogloss:0.583617\n",
            "[84]\ttrain-mlogloss:0.424056\ttest-mlogloss:0.583516\n",
            "[85]\ttrain-mlogloss:0.422286\ttest-mlogloss:0.583416\n",
            "[86]\ttrain-mlogloss:0.420392\ttest-mlogloss:0.583698\n",
            "[87]\ttrain-mlogloss:0.41831\ttest-mlogloss:0.58408\n",
            "[88]\ttrain-mlogloss:0.416327\ttest-mlogloss:0.583206\n",
            "[89]\ttrain-mlogloss:0.414486\ttest-mlogloss:0.583409\n",
            "[90]\ttrain-mlogloss:0.412738\ttest-mlogloss:0.583644\n",
            "[91]\ttrain-mlogloss:0.411136\ttest-mlogloss:0.583859\n",
            "[92]\ttrain-mlogloss:0.409247\ttest-mlogloss:0.58268\n",
            "[93]\ttrain-mlogloss:0.407705\ttest-mlogloss:0.582849\n",
            "[94]\ttrain-mlogloss:0.406323\ttest-mlogloss:0.583264\n",
            "[95]\ttrain-mlogloss:0.404725\ttest-mlogloss:0.583366\n",
            "[96]\ttrain-mlogloss:0.403387\ttest-mlogloss:0.583228\n",
            "[97]\ttrain-mlogloss:0.401886\ttest-mlogloss:0.583112\n",
            "[98]\ttrain-mlogloss:0.400515\ttest-mlogloss:0.583303\n",
            "[99]\ttrain-mlogloss:0.399354\ttest-mlogloss:0.581364\n",
            "[100]\ttrain-mlogloss:0.397973\ttest-mlogloss:0.580072\n",
            "[101]\ttrain-mlogloss:0.396661\ttest-mlogloss:0.5807\n",
            "[102]\ttrain-mlogloss:0.395044\ttest-mlogloss:0.580103\n",
            "[103]\ttrain-mlogloss:0.393396\ttest-mlogloss:0.580154\n",
            "[104]\ttrain-mlogloss:0.39176\ttest-mlogloss:0.580507\n",
            "[105]\ttrain-mlogloss:0.39001\ttest-mlogloss:0.580282\n",
            "[106]\ttrain-mlogloss:0.388917\ttest-mlogloss:0.580581\n",
            "[107]\ttrain-mlogloss:0.387674\ttest-mlogloss:0.580912\n",
            "[108]\ttrain-mlogloss:0.386158\ttest-mlogloss:0.580778\n",
            "[109]\ttrain-mlogloss:0.384719\ttest-mlogloss:0.581081\n",
            "[110]\ttrain-mlogloss:0.383261\ttest-mlogloss:0.580013\n",
            "[111]\ttrain-mlogloss:0.381837\ttest-mlogloss:0.579931\n",
            "[112]\ttrain-mlogloss:0.380915\ttest-mlogloss:0.578286\n",
            "[113]\ttrain-mlogloss:0.379721\ttest-mlogloss:0.578845\n",
            "[114]\ttrain-mlogloss:0.378376\ttest-mlogloss:0.579106\n",
            "[115]\ttrain-mlogloss:0.37768\ttest-mlogloss:0.580287\n",
            "[116]\ttrain-mlogloss:0.376493\ttest-mlogloss:0.580713\n",
            "[117]\ttrain-mlogloss:0.37564\ttest-mlogloss:0.580846\n",
            "[118]\ttrain-mlogloss:0.374602\ttest-mlogloss:0.581067\n",
            "[119]\ttrain-mlogloss:0.373636\ttest-mlogloss:0.580341\n",
            "[120]\ttrain-mlogloss:0.373058\ttest-mlogloss:0.580798\n",
            "[121]\ttrain-mlogloss:0.371681\ttest-mlogloss:0.580787\n",
            "[122]\ttrain-mlogloss:0.370135\ttest-mlogloss:0.580821\n",
            "[123]\ttrain-mlogloss:0.369664\ttest-mlogloss:0.581163\n",
            "[124]\ttrain-mlogloss:0.368823\ttest-mlogloss:0.580768\n",
            "[125]\ttrain-mlogloss:0.367751\ttest-mlogloss:0.581635\n",
            "[126]\ttrain-mlogloss:0.366426\ttest-mlogloss:0.581148\n",
            "[127]\ttrain-mlogloss:0.365027\ttest-mlogloss:0.579514\n",
            "[128]\ttrain-mlogloss:0.363921\ttest-mlogloss:0.579957\n",
            "[129]\ttrain-mlogloss:0.362661\ttest-mlogloss:0.580355\n",
            "[130]\ttrain-mlogloss:0.361724\ttest-mlogloss:0.581296\n",
            "[131]\ttrain-mlogloss:0.360626\ttest-mlogloss:0.581069\n",
            "[132]\ttrain-mlogloss:0.359662\ttest-mlogloss:0.580107\n",
            "[133]\ttrain-mlogloss:0.359174\ttest-mlogloss:0.580152\n",
            "[134]\ttrain-mlogloss:0.358121\ttest-mlogloss:0.579846\n",
            "[135]\ttrain-mlogloss:0.35699\ttest-mlogloss:0.579798\n",
            "[136]\ttrain-mlogloss:0.356376\ttest-mlogloss:0.579566\n",
            "[137]\ttrain-mlogloss:0.356029\ttest-mlogloss:0.579345\n",
            "[138]\ttrain-mlogloss:0.355259\ttest-mlogloss:0.577636\n",
            "[139]\ttrain-mlogloss:0.354392\ttest-mlogloss:0.577267\n",
            "[140]\ttrain-mlogloss:0.353577\ttest-mlogloss:0.576955\n",
            "[141]\ttrain-mlogloss:0.352497\ttest-mlogloss:0.576805\n",
            "[142]\ttrain-mlogloss:0.351499\ttest-mlogloss:0.577308\n",
            "[143]\ttrain-mlogloss:0.350836\ttest-mlogloss:0.57789\n",
            "[144]\ttrain-mlogloss:0.349874\ttest-mlogloss:0.578004\n",
            "[145]\ttrain-mlogloss:0.349157\ttest-mlogloss:0.578492\n",
            "[146]\ttrain-mlogloss:0.348145\ttest-mlogloss:0.578742\n",
            "[147]\ttrain-mlogloss:0.347254\ttest-mlogloss:0.579268\n",
            "[148]\ttrain-mlogloss:0.346401\ttest-mlogloss:0.579616\n",
            "[149]\ttrain-mlogloss:0.34551\ttest-mlogloss:0.578578\n",
            "[150]\ttrain-mlogloss:0.344569\ttest-mlogloss:0.578473\n",
            "[151]\ttrain-mlogloss:0.343826\ttest-mlogloss:0.579521\n",
            "[152]\ttrain-mlogloss:0.342914\ttest-mlogloss:0.57815\n",
            "[153]\ttrain-mlogloss:0.342033\ttest-mlogloss:0.579138\n",
            "[154]\ttrain-mlogloss:0.341307\ttest-mlogloss:0.579155\n",
            "[155]\ttrain-mlogloss:0.340542\ttest-mlogloss:0.58034\n",
            "[156]\ttrain-mlogloss:0.339631\ttest-mlogloss:0.580318\n",
            "[157]\ttrain-mlogloss:0.338982\ttest-mlogloss:0.579647\n",
            "[158]\ttrain-mlogloss:0.338155\ttest-mlogloss:0.579422\n",
            "[159]\ttrain-mlogloss:0.33757\ttest-mlogloss:0.578781\n",
            "[160]\ttrain-mlogloss:0.336522\ttest-mlogloss:0.579114\n",
            "[161]\ttrain-mlogloss:0.335724\ttest-mlogloss:0.579957\n",
            "[162]\ttrain-mlogloss:0.334995\ttest-mlogloss:0.580649\n",
            "[163]\ttrain-mlogloss:0.333935\ttest-mlogloss:0.581099\n",
            "[164]\ttrain-mlogloss:0.332949\ttest-mlogloss:0.582114\n",
            "[165]\ttrain-mlogloss:0.332182\ttest-mlogloss:0.583074\n",
            "[166]\ttrain-mlogloss:0.331406\ttest-mlogloss:0.583725\n",
            "[167]\ttrain-mlogloss:0.330842\ttest-mlogloss:0.584537\n",
            "[168]\ttrain-mlogloss:0.329983\ttest-mlogloss:0.584462\n",
            "[169]\ttrain-mlogloss:0.329168\ttest-mlogloss:0.584267\n",
            "[170]\ttrain-mlogloss:0.328666\ttest-mlogloss:0.585027\n",
            "[171]\ttrain-mlogloss:0.327928\ttest-mlogloss:0.586079\n",
            "[172]\ttrain-mlogloss:0.327461\ttest-mlogloss:0.586073\n",
            "[173]\ttrain-mlogloss:0.326964\ttest-mlogloss:0.586642\n",
            "[174]\ttrain-mlogloss:0.325954\ttest-mlogloss:0.586453\n",
            "[175]\ttrain-mlogloss:0.325601\ttest-mlogloss:0.586335\n",
            "[176]\ttrain-mlogloss:0.324982\ttest-mlogloss:0.586799\n",
            "[177]\ttrain-mlogloss:0.324389\ttest-mlogloss:0.586391\n",
            "[178]\ttrain-mlogloss:0.323714\ttest-mlogloss:0.586295\n",
            "[179]\ttrain-mlogloss:0.323218\ttest-mlogloss:0.586714\n",
            "[180]\ttrain-mlogloss:0.322499\ttest-mlogloss:0.587024\n",
            "[181]\ttrain-mlogloss:0.322169\ttest-mlogloss:0.587038\n",
            "[182]\ttrain-mlogloss:0.321548\ttest-mlogloss:0.585783\n",
            "[183]\ttrain-mlogloss:0.321129\ttest-mlogloss:0.585193\n",
            "[184]\ttrain-mlogloss:0.320458\ttest-mlogloss:0.586418\n",
            "[185]\ttrain-mlogloss:0.319872\ttest-mlogloss:0.587244\n",
            "[186]\ttrain-mlogloss:0.319459\ttest-mlogloss:0.587296\n",
            "[187]\ttrain-mlogloss:0.318772\ttest-mlogloss:0.585649\n",
            "[188]\ttrain-mlogloss:0.317999\ttest-mlogloss:0.585452\n",
            "[189]\ttrain-mlogloss:0.317534\ttest-mlogloss:0.586336\n",
            "[190]\ttrain-mlogloss:0.316739\ttest-mlogloss:0.585653\n",
            "[191]\ttrain-mlogloss:0.316227\ttest-mlogloss:0.586639\n",
            "[192]\ttrain-mlogloss:0.315907\ttest-mlogloss:0.587472\n",
            "[193]\ttrain-mlogloss:0.315107\ttest-mlogloss:0.58626\n",
            "[194]\ttrain-mlogloss:0.314577\ttest-mlogloss:0.585901\n",
            "[195]\ttrain-mlogloss:0.313882\ttest-mlogloss:0.586493\n",
            "[196]\ttrain-mlogloss:0.313415\ttest-mlogloss:0.58654\n",
            "[197]\ttrain-mlogloss:0.312971\ttest-mlogloss:0.586423\n",
            "[198]\ttrain-mlogloss:0.312403\ttest-mlogloss:0.58621\n",
            "[199]\ttrain-mlogloss:0.311859\ttest-mlogloss:0.58676\n",
            "[200]\ttrain-mlogloss:0.311508\ttest-mlogloss:0.586786\n",
            "[201]\ttrain-mlogloss:0.310973\ttest-mlogloss:0.588329\n",
            "[202]\ttrain-mlogloss:0.310337\ttest-mlogloss:0.589154\n",
            "[203]\ttrain-mlogloss:0.309935\ttest-mlogloss:0.5903\n",
            "[204]\ttrain-mlogloss:0.309328\ttest-mlogloss:0.590963\n",
            "[205]\ttrain-mlogloss:0.30888\ttest-mlogloss:0.592179\n",
            "[206]\ttrain-mlogloss:0.308463\ttest-mlogloss:0.592622\n",
            "[207]\ttrain-mlogloss:0.307989\ttest-mlogloss:0.593507\n",
            "[208]\ttrain-mlogloss:0.307438\ttest-mlogloss:0.593375\n",
            "[209]\ttrain-mlogloss:0.306977\ttest-mlogloss:0.594405\n",
            "[210]\ttrain-mlogloss:0.306511\ttest-mlogloss:0.594281\n",
            "[211]\ttrain-mlogloss:0.306019\ttest-mlogloss:0.594893\n",
            "[212]\ttrain-mlogloss:0.305327\ttest-mlogloss:0.59557\n",
            "[213]\ttrain-mlogloss:0.305052\ttest-mlogloss:0.596236\n",
            "[214]\ttrain-mlogloss:0.30495\ttest-mlogloss:0.597678\n",
            "[215]\ttrain-mlogloss:0.304868\ttest-mlogloss:0.598577\n",
            "[216]\ttrain-mlogloss:0.304291\ttest-mlogloss:0.599714\n",
            "[217]\ttrain-mlogloss:0.304021\ttest-mlogloss:0.600175\n",
            "[218]\ttrain-mlogloss:0.303299\ttest-mlogloss:0.600813\n",
            "[219]\ttrain-mlogloss:0.303\ttest-mlogloss:0.601388\n",
            "[220]\ttrain-mlogloss:0.302367\ttest-mlogloss:0.599711\n",
            "[221]\ttrain-mlogloss:0.301968\ttest-mlogloss:0.600585\n",
            "[222]\ttrain-mlogloss:0.301372\ttest-mlogloss:0.600841\n",
            "[223]\ttrain-mlogloss:0.301003\ttest-mlogloss:0.600031\n",
            "[224]\ttrain-mlogloss:0.300295\ttest-mlogloss:0.600347\n",
            "[225]\ttrain-mlogloss:0.299861\ttest-mlogloss:0.600565\n",
            "[226]\ttrain-mlogloss:0.299644\ttest-mlogloss:0.600994\n",
            "[227]\ttrain-mlogloss:0.299222\ttest-mlogloss:0.60206\n",
            "[228]\ttrain-mlogloss:0.298971\ttest-mlogloss:0.603504\n",
            "[229]\ttrain-mlogloss:0.29873\ttest-mlogloss:0.603848\n",
            "[230]\ttrain-mlogloss:0.298493\ttest-mlogloss:0.60468\n",
            "[231]\ttrain-mlogloss:0.297914\ttest-mlogloss:0.604818\n",
            "[232]\ttrain-mlogloss:0.297451\ttest-mlogloss:0.605683\n",
            "[233]\ttrain-mlogloss:0.297385\ttest-mlogloss:0.606851\n",
            "[234]\ttrain-mlogloss:0.297198\ttest-mlogloss:0.607988\n",
            "[235]\ttrain-mlogloss:0.297119\ttest-mlogloss:0.608724\n",
            "[236]\ttrain-mlogloss:0.296818\ttest-mlogloss:0.609851\n",
            "[237]\ttrain-mlogloss:0.296314\ttest-mlogloss:0.610454\n",
            "[238]\ttrain-mlogloss:0.296126\ttest-mlogloss:0.61174\n",
            "[239]\ttrain-mlogloss:0.295583\ttest-mlogloss:0.611321\n",
            "[240]\ttrain-mlogloss:0.295213\ttest-mlogloss:0.611155\n",
            "[241]\ttrain-mlogloss:0.29496\ttest-mlogloss:0.610752\n",
            "Stopping. Best iteration:\n",
            "[141]\ttrain-mlogloss:0.352497\ttest-mlogloss:0.576805\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OnAYG3DETzi",
        "colab_type": "code",
        "outputId": "875549eb-51f6-4c0f-c3c0-6a306c0fc92e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "# Create GAN estimator.\n",
        "\n",
        "import tensorflow as tf\n",
        "tfgan = tf.contrib.gan\n",
        "\n",
        "gan_estimator = tfgan.estimator.GANEstimator(\n",
        "         model_dir = '{googlepath}/checkpoint',\n",
        "         generator_fn=GAN(num_features=5, num_historical_days=20,generator_input_size=200),\n",
        "         discriminator_fn=TrainCNN(num_historical_days=20, days=5, pct_change=5),\n",
        "         generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\n",
        "         discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\n",
        "         generator_optimizer=tf.compat.v1.train.AdamOptimizer(0.1, 0.5),\n",
        "         discriminator_optimizer=tf.compat.v1.train.AdamOptimizer(0.1, 0.5))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"dropout/mul_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"dropout_1/mul_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"dropout_2/mul_1:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"dropout_4/mul_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"dropout_5/mul_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"dropout_6/mul_1:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"cnn_1/Relu:0\", shape=(?, 20, 1, 16), dtype=float32)\n",
            "Tensor(\"cnn_1/Relu_1:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"cnn_1/Relu_2:0\", shape=(?, 18, 1, 64), dtype=float32)\n",
            "1152\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AMZN.csv\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AAPL.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQj9QRwNxiDX",
        "colab_type": "code",
        "outputId": "8ed9fb7b-d340-4cf8-e954-3bc9266b9550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "##Predict mode\n",
        "#PREDICTING THE MODEL\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "class Predict:\n",
        "  def __init__(self, num_historical_days=20, days=10, pct_change=0, gan_model=f'{googlepath}/deployed_models/gan', cnn_modle=f'{googlepath}/deployed_models/cnn', xgb_model=f'{googlepath}/deployed_models/xgb'):\n",
        "    self.data = []\n",
        "    self.num_historical_days = num_historical_days\n",
        "    self.gan_model = gan_model\n",
        "    self.cnn_modle = cnn_modle\n",
        "    self.xgb_model = xgb_model\n",
        "    \n",
        "    files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")] \n",
        "    for file in files:\n",
        "      \n",
        "      print(file)\n",
        "      df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "      df = df[['open','high','low','close','volume']]\n",
        "            # data for new column labels that will use the pct_change of the closing data.\n",
        "            # pct_change measure change between current and prior element. Map these into a 1x2\n",
        "            # array to show if the pct_change > (our desired threshold) or less than.\n",
        "            \n",
        "      df = ((df -\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "      df = df.dropna()\n",
        "      self.data.append((file.split('/')[-1], df.iloc[0], df[200:200+num_historical_days].values))\n",
        "      #split the df into arrays of length num_historical_days and append\n",
        "      # to data, i.e. array of df[curr - num_days : curr] -> a batch of values\n",
        "      # appending if price went up or down in curr day of \"i\" we are lookin\n",
        "      # at\n",
        "      \n",
        "      \n",
        "  def gan_predict(self):\n",
        "    tf.reset_default_graph()\n",
        "    gan = GAN(num_features=5, num_historical_days=self.num_historical_days, generator_input_size=200, is_train=False)\n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      saver = tf.train.Saver()\n",
        "      saver.restore(sess, self.gan_model)\n",
        "      clf = joblib.load(self.xgb_model)\n",
        "      for sym, date, data in self.data:\n",
        "        features = sess.run(gan.features, feed_dict={gan.X:[data]})\n",
        "        features = xgb.DMatrix(features)\n",
        "        print('{} {} {}'.format(str(date).split(' ')[0], sym, clf.predict(features)[0][1] > 0.5))\n",
        "        #predictions = np.array([x for x in gan_estimator.predict(p.gan_predict())])\n",
        "        #print(predictions)\n",
        "\n",
        "p = Predict()\n",
        "p.gan_predict()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AMZN.csv\n",
            "/content/drive/My Drive/Colab Notebooks/SeniorDesignGAN/MiloGAN/stock_data/AAPL.csv\n",
            "Tensor(\"Relu:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_2:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"Relu_4:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_5:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_6:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "open AMZN.csv True\n",
            "open AAPL.csv True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqr-JgUUT7R8",
        "colab_type": "text"
      },
      "source": [
        "**Confusion Matrix basically gets stored into deplyed models. Open it up to have a good look. Here is the a model.** \n",
        "\n",
        "**The up and down are indicating the movement of the stock price. So if the stock is going up we predict that it is going up 93% of the time and if it is going down we predict it 87% of the time which is great!**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PvhVGi89lMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
